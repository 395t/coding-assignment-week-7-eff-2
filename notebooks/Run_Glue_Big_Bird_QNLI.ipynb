{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Run Glue Big Bird QNLI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqe1m_-RX07v",
        "outputId": "c7071d2f-2ccf-4caa-b683-dbabfddeea1b"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 85569, done.\u001b[K\n",
            "remote: Total 85569 (delta 0), reused 0 (delta 0), pack-reused 85569\u001b[K\n",
            "Receiving objects: 100% (85569/85569), 68.41 MiB | 16.72 MiB/s, done.\n",
            "Resolving deltas: 100% (61505/61505), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kbBD__AX6vg",
        "outputId": "184a5579-cc06-4585-c8b4-06c668c43f9f"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install accelerate\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.0.dev0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.10.0)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.17)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.7.4.post0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.6.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-ju86lscd\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-ju86lscd\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (5.4.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (4.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (0.0.46)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (0.0.17)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers==4.12.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.12.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.12.0.dev0) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from accelerate) (5.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (3.7.4.3)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 13.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypmvvTLGYSL7",
        "outputId": "495f3b53-b6fc-4a43-c864-08bd6b09ef1b"
      },
      "source": [
        "!python transformers/examples/pytorch/text-classification/run_glue.py \\\n",
        "  --model_name_or_path google/bigbird-roberta-base \\\n",
        "  --task_name qnli \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 64 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir /tmp/qnli-big-bird/ \\\n",
        "  --overwrite_output_dir \\\n",
        "  --gradient_checkpointing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/03/2021 19:24:44 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "10/03/2021 19:24:44 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/qnli-big-bird/runs/Oct03_19-24-44_13ba4c2a79a0,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=/tmp/qnli-big-bird/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/qnli-big-bird/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "10/03/2021 19:24:45 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "10/03/2021 19:24:45 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "10/03/2021 19:24:45 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "10/03/2021 19:24:45 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "10/03/2021 19:24:45 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "10/03/2021 19:24:45 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "10/03/2021 19:24:45 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "10/03/2021 19:24:45 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "10/03/2021 19:24:45 - WARNING - datasets.builder - Reusing dataset glue (/root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "10/03/2021 19:24:45 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 779.47it/s]\n",
            "[INFO|configuration_utils.py:583] 2021-10-03 19:24:45,946 >> loading configuration file https://huggingface.co/google/bigbird-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/d7643b757353be56f05bdd19496d6e3fb5bb9edfdf5f9e5eca88d6f479e32324.dc98375bb3e19a644a5cadd5c305949ec470186fcc20bd8c8b959a43dcc3ff21\n",
            "[INFO|configuration_utils.py:620] 2021-10-03 19:24:45,948 >> Model config BigBirdConfig {\n",
            "  \"architectures\": [\n",
            "    \"BigBirdForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_type\": \"block_sparse\",\n",
            "  \"block_size\": 64,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"qnli\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"big_bird\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_random_blocks\": 3,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"rescale_embeddings\": false,\n",
            "  \"sep_token_id\": 66,\n",
            "  \"transformers_version\": \"4.12.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bias\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50358\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:583] 2021-10-03 19:24:46,657 >> loading configuration file https://huggingface.co/google/bigbird-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/d7643b757353be56f05bdd19496d6e3fb5bb9edfdf5f9e5eca88d6f479e32324.dc98375bb3e19a644a5cadd5c305949ec470186fcc20bd8c8b959a43dcc3ff21\n",
            "[INFO|configuration_utils.py:620] 2021-10-03 19:24:46,657 >> Model config BigBirdConfig {\n",
            "  \"architectures\": [\n",
            "    \"BigBirdForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_type\": \"block_sparse\",\n",
            "  \"block_size\": 64,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"big_bird\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_random_blocks\": 3,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"rescale_embeddings\": false,\n",
            "  \"sep_token_id\": 66,\n",
            "  \"transformers_version\": \"4.12.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bias\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50358\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-03 19:24:48,792 >> loading file https://huggingface.co/google/bigbird-roberta-base/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/d318d7bb69cafb1d8964fc87515592ac3092a2c8fdb305068f9ba4020df3ee3b.271d467a9adc15fb44348481bc75c48b63cba0fd4934bc5377d63a63de052c45\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-03 19:24:48,792 >> loading file https://huggingface.co/google/bigbird-roberta-base/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-03 19:24:48,793 >> loading file https://huggingface.co/google/bigbird-roberta-base/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-03 19:24:48,793 >> loading file https://huggingface.co/google/bigbird-roberta-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/400be7e354ea6eb77319bcc7fa34899ec9fa2e3aff0fa677f6eb7e45a01b1548.75b358ecb30fa6b001d9d87bfde336c02d9123e7a8f5b90cc890d0f6efc3d4a3\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-03 19:24:48,793 >> loading file https://huggingface.co/google/bigbird-roberta-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/d20a688e918d227ce5dbcd5f2b570a093cee6b095952d74b9c245b245e6510de.c8f14f85d9ff88cdd1fe7094cde11f85b74fcb7eb03616822964895bc6626c3b\n",
            "[INFO|configuration_utils.py:583] 2021-10-03 19:24:49,149 >> loading configuration file https://huggingface.co/google/bigbird-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/d7643b757353be56f05bdd19496d6e3fb5bb9edfdf5f9e5eca88d6f479e32324.dc98375bb3e19a644a5cadd5c305949ec470186fcc20bd8c8b959a43dcc3ff21\n",
            "[INFO|configuration_utils.py:620] 2021-10-03 19:24:49,150 >> Model config BigBirdConfig {\n",
            "  \"architectures\": [\n",
            "    \"BigBirdForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_type\": \"block_sparse\",\n",
            "  \"block_size\": 64,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"big_bird\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_random_blocks\": 3,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"rescale_embeddings\": false,\n",
            "  \"sep_token_id\": 66,\n",
            "  \"transformers_version\": \"4.12.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bias\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50358\n",
            "}\n",
            "\n",
            "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
            "[INFO|configuration_utils.py:583] 2021-10-03 19:24:49,580 >> loading configuration file https://huggingface.co/google/bigbird-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/d7643b757353be56f05bdd19496d6e3fb5bb9edfdf5f9e5eca88d6f479e32324.dc98375bb3e19a644a5cadd5c305949ec470186fcc20bd8c8b959a43dcc3ff21\n",
            "[INFO|configuration_utils.py:620] 2021-10-03 19:24:49,581 >> Model config BigBirdConfig {\n",
            "  \"architectures\": [\n",
            "    \"BigBirdForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_type\": \"block_sparse\",\n",
            "  \"block_size\": 64,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"big_bird\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_random_blocks\": 3,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"rescale_embeddings\": false,\n",
            "  \"sep_token_id\": 66,\n",
            "  \"transformers_version\": \"4.12.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bias\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50358\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1664] 2021-10-03 19:24:50,075 >> https://huggingface.co/google/bigbird-roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmparpu8qiy\n",
            "Downloading: 100% 489M/489M [00:10<00:00, 48.0MB/s]\n",
            "[INFO|file_utils.py:1668] 2021-10-03 19:25:01,339 >> storing https://huggingface.co/google/bigbird-roberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/c523b12608662dbff39b2c24a608a6ff30857bc7967a5c9b00cb76d1147e223b.06e7996caf35449212f17d31a2129bb55c59c19054fcf8552a847e4bcb475688\n",
            "[INFO|file_utils.py:1676] 2021-10-03 19:25:01,340 >> creating metadata file for /root/.cache/huggingface/transformers/c523b12608662dbff39b2c24a608a6ff30857bc7967a5c9b00cb76d1147e223b.06e7996caf35449212f17d31a2129bb55c59c19054fcf8552a847e4bcb475688\n",
            "[INFO|modeling_utils.py:1323] 2021-10-03 19:25:01,340 >> loading weights file https://huggingface.co/google/bigbird-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/c523b12608662dbff39b2c24a608a6ff30857bc7967a5c9b00cb76d1147e223b.06e7996caf35449212f17d31a2129bb55c59c19054fcf8552a847e4bcb475688\n",
            "[WARNING|modeling_utils.py:1580] 2021-10-03 19:25:02,923 >> Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1591] 2021-10-03 19:25:02,924 >> Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/105 [00:00<?, ?ba/s]10/03/2021 19:25:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-56ac52130f976f40.arrow\n",
            "Running tokenizer on dataset: 100% 105/105 [00:13<00:00,  7.93ba/s]\n",
            "Running tokenizer on dataset:   0% 0/6 [00:00<?, ?ba/s]10/03/2021 19:25:16 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-5d04dd6536d8f5cb.arrow\n",
            "Running tokenizer on dataset: 100% 6/6 [00:00<00:00,  8.41ba/s]\n",
            "Running tokenizer on dataset:   0% 0/6 [00:00<?, ?ba/s]10/03/2021 19:25:17 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-82b1c13ae216b968.arrow\n",
            "Running tokenizer on dataset: 100% 6/6 [00:00<00:00,  8.40ba/s]\n",
            "10/03/2021 19:25:17 - INFO - __main__ - Sample 83810 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 83810, 'input_ids': [65, 1968, 10916, 387, 363, 451, 11832, 648, 1395, 391, 472, 11481, 6654, 508, 5698, 385, 5513, 131, 66, 1651, 2782, 3490, 480, 363, 7084, 10284, 2267, 112, 358, 572, 114, 151, 114, 868, 501, 27850, 7559, 3411, 9235, 14588, 4917, 458, 119, 112, 931, 5896, 109, 391, 363, 572, 114, 151, 114, 453, 402, 11101, 7559, 458, 1166, 112, 931, 100, 1415, 112, 931, 45434, 109, 648, 45203, 430, 363, 451, 11832, 961, 501, 5508, 5013, 439, 1216, 113, 31287, 2105, 652, 21271, 1833, 10916, 480, 363, 5939, 387, 10132, 31470, 40526, 10941, 112, 576, 585, 5358, 385, 6755, 840, 3802, 5322, 391, 1496, 12879, 1205, 2147, 100, 45882, 452, 718, 1416, 112, 931, 10199, 18600, 114, 66, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'question': 'What tactics of the PVA were US and ROK troops not prepared to handle?', 'sentence': \"On 27 November at the Korean eastern front, a U.S. 7th Infantry Division Regimental Combat Team (3,000 soldiers) and the U.S. 1st Marine Division (12,000–15,000 marines) were unprepared for the PVA 9th Army Group's three-pronged encirclement tactics at the Battle of Chosin Reservoir, but they managed to escape under Air Force and X Corps support fire—albeit with some 15,000 collective casualties.\"}.\n",
            "10/03/2021 19:25:17 - INFO - __main__ - Sample 14592 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 14592, 'input_ids': [65, 1475, 851, 363, 14176, 762, 1355, 647, 500, 25030, 439, 2983, 385, 363, 1469, 131, 66, 500, 25030, 851, 508, 1355, 427, 363, 14176, 5508, 474, 3593, 430, 358, 19308, 391, 851, 508, 50182, 2137, 3194, 114, 66, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'question': \"How did the Egyptian people feel about Nasser's response to the attack?\", 'sentence': 'Nasser did not feel that the Egyptian Army was ready for a confrontation and did not retaliate militarily.'}.\n",
            "10/03/2021 19:25:17 - INFO - __main__ - Sample 3278 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 3278, 'input_ids': [65, 1968, 1712, 387, 6316, 490, 6108, 387, 2364, 673, 31778, 131, 66, 5001, 412, 10569, 47569, 6316, 490, 6108, 387, 2364, 673, 31778, 523, 358, 4237, 420, 363, 6304, 112, 1363, 363, 10555, 391, 358, 2592, 1024, 419, 1391, 618, 5353, 7043, 391, 13993, 358, 20241, 4320, 3564, 114, 66, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'question': 'What kind of aircraft are capable of taking off vertically?', 'sentence': 'Although STOVL aircraft are capable of taking off vertically from a spot on the deck, using the ramp and a running start is far more fuel efficient and permits a heavier launch weight.'}.\n",
            "10/03/2021 19:25:18 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpc2s4x1q3\n",
            "Downloading: 5.78kB [00:00, 6.78MB/s]       \n",
            "10/03/2021 19:25:18 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py in cache at /root/.cache/huggingface/datasets/downloads/3d00b264fb94fc8d9977c2149d05fc9eb1f8a5e485aadb2eae351dfc9c3a6db4.d4934ed99ee0ef4a5d0d757367c52f72d6ae30a68db09205c01563cab11c5a0d.py\n",
            "10/03/2021 19:25:18 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/3d00b264fb94fc8d9977c2149d05fc9eb1f8a5e485aadb2eae351dfc9c3a6db4.d4934ed99ee0ef4a5d0d757367c52f72d6ae30a68db09205c01563cab11c5a0d.py\n",
            "10/03/2021 19:25:18 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "10/03/2021 19:25:18 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "10/03/2021 19:25:18 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "10/03/2021 19:25:18 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "10/03/2021 19:25:18 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "[INFO|trainer.py:541] 2021-10-03 19:25:24,516 >> The following columns in the training set  don't have a corresponding argument in `BigBirdForSequenceClassification.forward` and have been ignored: idx, question, sentence.\n",
            "[INFO|trainer.py:1196] 2021-10-03 19:25:24,532 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-10-03 19:25:24,532 >>   Num examples = 104743\n",
            "[INFO|trainer.py:1198] 2021-10-03 19:25:24,532 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1199] 2021-10-03 19:25:24,532 >>   Instantaneous batch size per device = 64\n",
            "[INFO|trainer.py:1200] 2021-10-03 19:25:24,533 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1201] 2021-10-03 19:25:24,533 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-10-03 19:25:24,533 >>   Total optimization steps = 4911\n",
            "  0% 0/4911 [00:00<?, ?it/s][WARNING|modeling_big_bird.py:2062] 2021-10-03 19:25:24,567 >> Attention type 'block_sparse' is not possible if sequence_length: 128 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3.Changing attention type to 'original_full'...\n",
            "{'loss': 0.4173, 'learning_rate': 1.7963754836082265e-05, 'epoch': 0.31}\n",
            " 10% 500/4911 [09:06<1:20:20,  1.09s/it][INFO|trainer.py:1987] 2021-10-03 19:34:31,186 >> Saving model checkpoint to /tmp/qnli-big-bird/checkpoint-500\n",
            "[INFO|configuration_utils.py:413] 2021-10-03 19:34:31,187 >> Configuration saved in /tmp/qnli-big-bird/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-03 19:34:32,479 >> Model weights saved in /tmp/qnli-big-bird/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-03 19:34:32,480 >> tokenizer config file saved in /tmp/qnli-big-bird/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-03 19:34:32,480 >> Special tokens file saved in /tmp/qnli-big-bird/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.3483, 'learning_rate': 1.592750967216453e-05, 'epoch': 0.61}\n",
            " 20% 1000/4911 [18:19<1:11:19,  1.09s/it][INFO|trainer.py:1987] 2021-10-03 19:43:43,602 >> Saving model checkpoint to /tmp/qnli-big-bird/checkpoint-1000\n",
            "[INFO|configuration_utils.py:413] 2021-10-03 19:43:43,602 >> Configuration saved in /tmp/qnli-big-bird/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-03 19:43:44,882 >> Model weights saved in /tmp/qnli-big-bird/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-03 19:43:44,883 >> tokenizer config file saved in /tmp/qnli-big-bird/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-03 19:43:44,883 >> Special tokens file saved in /tmp/qnli-big-bird/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.3283, 'learning_rate': 1.3891264508246794e-05, 'epoch': 0.92}\n",
            " 31% 1500/4911 [27:31<1:02:17,  1.10s/it][INFO|trainer.py:1987] 2021-10-03 19:52:56,331 >> Saving model checkpoint to /tmp/qnli-big-bird/checkpoint-1500\n",
            "[INFO|configuration_utils.py:413] 2021-10-03 19:52:56,332 >> Configuration saved in /tmp/qnli-big-bird/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-03 19:52:57,771 >> Model weights saved in /tmp/qnli-big-bird/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-03 19:52:57,772 >> tokenizer config file saved in /tmp/qnli-big-bird/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-03 19:52:57,773 >> Special tokens file saved in /tmp/qnli-big-bird/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 0.2749, 'learning_rate': 1.1855019344329057e-05, 'epoch': 1.22}\n",
            " 41% 2000/4911 [36:44<53:07,  1.09s/it][INFO|trainer.py:1987] 2021-10-03 20:02:09,375 >> Saving model checkpoint to /tmp/qnli-big-bird/checkpoint-2000\n",
            "[INFO|configuration_utils.py:413] 2021-10-03 20:02:09,376 >> Configuration saved in /tmp/qnli-big-bird/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-03 20:02:10,704 >> Model weights saved in /tmp/qnli-big-bird/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-03 20:02:10,705 >> tokenizer config file saved in /tmp/qnli-big-bird/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-03 20:02:10,705 >> Special tokens file saved in /tmp/qnli-big-bird/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 0.2493, 'learning_rate': 9.818774180411322e-06, 'epoch': 1.53}\n",
            " 51% 2500/4911 [45:58<44:00,  1.10s/it][INFO|trainer.py:1987] 2021-10-03 20:11:22,712 >> Saving model checkpoint to /tmp/qnli-big-bird/checkpoint-2500\n",
            "[INFO|configuration_utils.py:413] 2021-10-03 20:11:22,713 >> Configuration saved in /tmp/qnli-big-bird/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-03 20:11:23,980 >> Model weights saved in /tmp/qnli-big-bird/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-03 20:11:23,981 >> tokenizer config file saved in /tmp/qnli-big-bird/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-03 20:11:23,981 >> Special tokens file saved in /tmp/qnli-big-bird/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 0.2393, 'learning_rate': 7.782529016493586e-06, 'epoch': 1.83}\n",
            " 61% 3000/4911 [55:11<34:58,  1.10s/it][INFO|trainer.py:1987] 2021-10-03 20:20:36,166 >> Saving model checkpoint to /tmp/qnli-big-bird/checkpoint-3000\n",
            "[INFO|configuration_utils.py:413] 2021-10-03 20:20:36,167 >> Configuration saved in /tmp/qnli-big-bird/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-03 20:20:37,440 >> Model weights saved in /tmp/qnli-big-bird/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-03 20:20:37,441 >> tokenizer config file saved in /tmp/qnli-big-bird/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-03 20:20:37,441 >> Special tokens file saved in /tmp/qnli-big-bird/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 0.2166, 'learning_rate': 5.74628385257585e-06, 'epoch': 2.14}\n",
            " 71% 3500/4911 [1:04:25<25:45,  1.10s/it][INFO|trainer.py:1987] 2021-10-03 20:29:50,338 >> Saving model checkpoint to /tmp/qnli-big-bird/checkpoint-3500\n",
            "[INFO|configuration_utils.py:413] 2021-10-03 20:29:50,339 >> Configuration saved in /tmp/qnli-big-bird/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-03 20:29:51,697 >> Model weights saved in /tmp/qnli-big-bird/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-03 20:29:51,698 >> tokenizer config file saved in /tmp/qnli-big-bird/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-03 20:29:51,698 >> Special tokens file saved in /tmp/qnli-big-bird/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 0.1849, 'learning_rate': 3.7100386886581147e-06, 'epoch': 2.44}\n",
            " 81% 4000/4911 [1:13:39<16:40,  1.10s/it][INFO|trainer.py:1987] 2021-10-03 20:39:04,287 >> Saving model checkpoint to /tmp/qnli-big-bird/checkpoint-4000\n",
            "[INFO|configuration_utils.py:413] 2021-10-03 20:39:04,288 >> Configuration saved in /tmp/qnli-big-bird/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-03 20:39:05,648 >> Model weights saved in /tmp/qnli-big-bird/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-03 20:39:05,650 >> tokenizer config file saved in /tmp/qnli-big-bird/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-03 20:39:05,650 >> Special tokens file saved in /tmp/qnli-big-bird/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 0.1883, 'learning_rate': 1.6737935247403788e-06, 'epoch': 2.75}\n",
            " 92% 4500/4911 [1:22:53<07:30,  1.10s/it][INFO|trainer.py:1987] 2021-10-03 20:48:18,299 >> Saving model checkpoint to /tmp/qnli-big-bird/checkpoint-4500\n",
            "[INFO|configuration_utils.py:413] 2021-10-03 20:48:18,300 >> Configuration saved in /tmp/qnli-big-bird/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-03 20:48:19,607 >> Model weights saved in /tmp/qnli-big-bird/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-03 20:48:19,608 >> tokenizer config file saved in /tmp/qnli-big-bird/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-03 20:48:19,608 >> Special tokens file saved in /tmp/qnli-big-bird/checkpoint-4500/special_tokens_map.json\n",
            "100% 4911/4911 [1:30:29<00:00,  1.02it/s][INFO|trainer.py:1401] 2021-10-03 20:55:54,329 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 5429.799, 'train_samples_per_second': 57.871, 'train_steps_per_second': 0.904, 'train_loss': 0.2644744543698139, 'epoch': 3.0}\n",
            "100% 4911/4911 [1:30:29<00:00,  1.11s/it]\n",
            "[INFO|trainer.py:1987] 2021-10-03 20:55:54,346 >> Saving model checkpoint to /tmp/qnli-big-bird/\n",
            "[INFO|configuration_utils.py:413] 2021-10-03 20:55:54,347 >> Configuration saved in /tmp/qnli-big-bird/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-03 20:55:55,630 >> Model weights saved in /tmp/qnli-big-bird/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-03 20:55:55,631 >> tokenizer config file saved in /tmp/qnli-big-bird/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-03 20:55:55,631 >> Special tokens file saved in /tmp/qnli-big-bird/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.2645\n",
            "  train_runtime            = 1:30:29.79\n",
            "  train_samples            =     104743\n",
            "  train_samples_per_second =     57.871\n",
            "  train_steps_per_second   =      0.904\n",
            "10/03/2021 20:55:55 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:541] 2021-10-03 20:55:55,959 >> The following columns in the evaluation set  don't have a corresponding argument in `BigBirdForSequenceClassification.forward` and have been ignored: idx, question, sentence.\n",
            "[INFO|trainer.py:2235] 2021-10-03 20:55:56,011 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2237] 2021-10-03 20:55:56,011 >>   Num examples = 5463\n",
            "[INFO|trainer.py:2240] 2021-10-03 20:55:56,011 >>   Batch size = 8\n",
            "100% 683/683 [00:27<00:00, 25.00it/s]10/03/2021 20:56:23 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow\n",
            "100% 683/683 [00:27<00:00, 24.69it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.9041\n",
            "  eval_loss               =     0.2706\n",
            "  eval_runtime            = 0:00:27.74\n",
            "  eval_samples            =       5463\n",
            "  eval_samples_per_second =     196.88\n",
            "  eval_steps_per_second   =     24.615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZXKdqxeHrQE"
      },
      "source": [
        "!mkdir /tmp/tmp\n",
        "!cp -r /tmp/qnli-big-bird/ /tmp/tmp/qnli-big-bird/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzED3a43Isuf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "80b6985d-f73d-426b-a599-d04fbf8901e1"
      },
      "source": [
        "!rm -r /tmp/tmp/qnli-big-bird/checkpoint*\n",
        "!rm -r /tmp/tmp/qnli-big-bird/runs\n",
        "!rm /tmp/tmp/qnli-big-bird/pytorch_model.bin\n",
        "!zip -r /tmp/qnli-big-bird.zip /tmp/tmp/qnli-big-bird\n",
        "from google.colab import files; files.download('/tmp/qnli-big-bird.zip')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/tmp/tmp/qnli-big-bird/checkpoint*': No such file or directory\n",
            "rm: cannot remove '/tmp/tmp/qnli-big-bird/runs': No such file or directory\n",
            "rm: cannot remove '/tmp/tmp/qnli-big-bird/pytorch_model.bin': No such file or directory\n",
            "  adding: tmp/tmp/qnli-big-bird/ (stored 0%)\n",
            "  adding: tmp/tmp/qnli-big-bird/spiece.model (deflated 69%)\n",
            "  adding: tmp/tmp/qnli-big-bird/train_results.json (deflated 41%)\n",
            "  adding: tmp/tmp/qnli-big-bird/README.md (deflated 49%)\n",
            "  adding: tmp/tmp/qnli-big-bird/special_tokens_map.json (deflated 80%)\n",
            "  adding: tmp/tmp/qnli-big-bird/config.json (deflated 53%)\n",
            "  adding: tmp/tmp/qnli-big-bird/trainer_state.json (deflated 69%)\n",
            "  adding: tmp/tmp/qnli-big-bird/eval_results.json (deflated 41%)\n",
            "  adding: tmp/tmp/qnli-big-bird/tokenizer.json (deflated 74%)\n",
            "  adding: tmp/tmp/qnli-big-bird/tokenizer_config.json (deflated 70%)\n",
            "  adding: tmp/tmp/qnli-big-bird/all_results.json (deflated 56%)\n",
            "  adding: tmp/tmp/qnli-big-bird/training_args.bin (deflated 49%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_83a01270-06a6-4976-ba1b-c7f287ac7cc6\", \"qnli-big-bird.zip\", 491879)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T9n9UTWT1rw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}