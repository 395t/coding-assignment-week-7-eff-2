{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Run Glue Longformer QNLI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqe1m_-RX07v",
        "outputId": "f74d1a99-d858-440b-c8ba-2d2faf43bd4b"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 85569, done.\u001b[K\n",
            "remote: Total 85569 (delta 0), reused 0 (delta 0), pack-reused 85569\u001b[K\n",
            "Receiving objects: 100% (85569/85569), 68.41 MiB | 23.19 MiB/s, done.\n",
            "Resolving deltas: 100% (61505/61505), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kbBD__AX6vg",
        "outputId": "3145a4f7-86e9-4eae-d379-626c8f5e57db"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install accelerate"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.11.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 29.2 MB/s eta 0:00:01\r\u001b[K     |▎                               | 20 kB 34.1 MB/s eta 0:00:01\r\u001b[K     |▍                               | 30 kB 38.2 MB/s eta 0:00:01\r\u001b[K     |▌                               | 40 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |▋                               | 51 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |▊                               | 61 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |▉                               | 71 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█                               | 81 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |█                               | 92 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 102 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 112 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 122 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 133 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 143 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 153 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 163 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 174 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 184 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 194 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 204 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 215 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 225 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 235 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 245 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 256 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 266 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 276 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 286 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 296 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 307 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 317 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 327 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 337 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 348 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 358 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 368 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 378 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 389 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 399 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 409 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 419 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 430 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 440 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████                           | 450 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████                           | 460 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 471 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 481 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 491 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 501 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 512 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 522 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 532 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 542 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 552 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 563 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 573 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 583 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 593 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 604 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 614 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 624 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 634 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 645 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 655 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 665 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 675 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 686 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 696 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 706 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 716 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 727 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 737 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 747 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 757 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 768 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 778 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 788 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 798 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 808 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 819 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 829 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 839 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 849 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 860 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 870 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 880 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 890 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 901 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 911 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 921 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 931 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 942 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 952 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 962 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 972 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 983 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 993 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 1.0 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 1.0 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 1.0 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 1.0 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 1.0 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 1.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 1.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 1.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 1.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 1.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 1.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 1.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 1.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 1.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 1.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 1.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 1.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 1.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 1.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 1.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 1.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 1.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.9 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.9 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.9 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.9 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.9 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.9 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.9 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.9 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.9 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.9 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 2.0 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 2.0 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 2.0 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 2.0 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 2.0 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 2.0 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 2.0 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 2.0 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 2.0 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 2.0 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 2.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 2.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 2.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 2.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 2.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 2.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 2.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 2.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 2.1 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 2.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 2.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 2.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 2.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 2.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 2.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 2.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 2.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 2.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 2.2 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 2.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 2.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 2.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 2.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 2.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 2.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 2.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 2.3 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 2.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 2.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 2.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 2.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 2.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 2.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.4 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 2.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 2.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 2.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 2.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 2.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 2.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 2.5 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 2.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 2.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 2.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 2.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 2.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 2.6 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 2.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 2.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 2.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.7 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 2.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.8 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.9 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.9 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.9 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.9 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.9 MB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.9 MB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 56.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 63.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 37.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.17 pyyaml-5.4.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.2\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 15.5 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 70.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 61.9 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.0-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 70.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.14 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.17)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 54.4 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 53.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, xxhash, datasets\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.12.1 fsspec-2021.10.0 multidict-5.1.0 xxhash-2.0.2 yarl-1.6.3\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-hrbh5_zj\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-hrbh5_zj\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (0.0.17)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (5.4.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (4.8.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.0.dev0) (0.10.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers==4.12.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.12.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.12.0.dev0) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.0.dev0) (7.1.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.12.0.dev0-py3-none-any.whl size=2880358 sha256=e7c1e6e85018101903420b999ed8a81e3d4a29184896d1b5854e9c46ce3e3b0d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4h9cqn03/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.11.2\n",
            "    Uninstalling transformers-4.11.2:\n",
            "      Successfully uninstalled transformers-4.11.2\n",
            "Successfully installed transformers-4.12.0.dev0\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.5.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.9.0+cu102)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from accelerate) (5.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (3.7.4.3)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypmvvTLGYSL7",
        "outputId": "2451e845-b68e-4a33-f0d9-1a85d7e9ed6f"
      },
      "source": [
        "!python transformers/examples/pytorch/text-classification/run_glue.py \\\n",
        "  --model_name_or_path allenai/longformer-base-4096 \\\n",
        "  --task_name qnli \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 64 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --output_dir /tmp/qnli/ \\\n",
        "  --overwrite_output_dir \\\n",
        "  --gradient_checkpointing"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/03/2021 19:18:42 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "10/03/2021 19:18:42 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/qnli/runs/Oct03_19-18-42_c83be99776bc,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "output_dir=/tmp/qnli/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/qnli/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "10/03/2021 19:18:43 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue\n",
            "10/03/2021 19:18:43 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "10/03/2021 19:18:43 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.py\n",
            "10/03/2021 19:18:43 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/dataset_infos.json\n",
            "10/03/2021 19:18:43 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/glue.json\n",
            "10/03/2021 19:18:43 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "10/03/2021 19:18:43 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "10/03/2021 19:18:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "10/03/2021 19:18:43 - WARNING - datasets.builder - Reusing dataset glue (/root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "10/03/2021 19:18:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
            "100% 3/3 [00:00<00:00, 754.42it/s]\n",
            "[INFO|configuration_utils.py:583] 2021-10-03 19:18:43,551 >> loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
            "[INFO|configuration_utils.py:620] 2021-10-03 19:18:43,553 >> Model config LongformerConfig {\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"qnli\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 2,\n",
            "  \"transformers_version\": \"4.12.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:334] 2021-10-03 19:18:43,908 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:583] 2021-10-03 19:18:44,262 >> loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
            "[INFO|configuration_utils.py:620] 2021-10-03 19:18:44,263 >> Model config LongformerConfig {\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 2,\n",
            "  \"transformers_version\": \"4.12.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-03 19:18:46,758 >> loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/859f4633944e1b7e7fa301e72161388cd5903e36385d0ef2917256506bff64c3.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-03 19:18:46,758 >> loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/af6fcabe2bf8cab6f77b20d94ba46a3dbf441ca0549e1f3c852c437b612f5224.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-03 19:18:46,758 >> loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/93ab433997eab2709f7adf8fa46f21d4699497bf20768f3ffd25e2e73b9b93c2.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-03 19:18:46,758 >> loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-03 19:18:46,758 >> loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1741] 2021-10-03 19:18:46,758 >> loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:583] 2021-10-03 19:18:47,114 >> loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
            "[INFO|configuration_utils.py:620] 2021-10-03 19:18:47,116 >> Model config LongformerConfig {\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 2,\n",
            "  \"transformers_version\": \"4.12.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1323] 2021-10-03 19:18:47,553 >> loading weights file https://huggingface.co/allenai/longformer-base-4096/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a7a586602e625bd012d75abdfcc615f5bb1fe133273845f7381332c634273bd9.dc3a4f03d4ab11f972b126d0e6b67f43e5d9003b3aec54f8e549cc7e2d42398d\n",
            "[WARNING|modeling_utils.py:1580] 2021-10-03 19:18:49,105 >> Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1591] 2021-10-03 19:18:49,105 >> Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "10/03/2021 19:18:49 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d1316b44ba8ffc98.arrow\n",
            "10/03/2021 19:18:49 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3d87c8debd2ae85b.arrow\n",
            "10/03/2021 19:18:49 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-6a46ac2c585bf414.arrow\n",
            "10/03/2021 19:18:49 - INFO - __main__ - Sample 83810 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'idx': 83810, 'input_ids': [0, 2264, 8893, 9, 5, 221, 9788, 58, 382, 8, 248, 9335, 3517, 45, 2460, 7, 3679, 116, 2, 2, 4148, 974, 759, 23, 5, 2238, 4580, 760, 6, 10, 121, 4, 104, 4, 262, 212, 35614, 2925, 6304, 40955, 30456, 2711, 36, 246, 6, 151, 3878, 43, 8, 5, 121, 4, 104, 4, 112, 620, 6144, 2925, 36, 1092, 6, 151, 2383, 996, 6, 151, 42669, 43, 58, 35578, 13, 5, 221, 9788, 361, 212, 2938, 826, 18, 130, 12, 4862, 1657, 196, 9689, 21163, 13767, 8893, 23, 5, 9846, 9, 732, 366, 179, 23895, 13878, 6, 53, 51, 2312, 7, 5111, 223, 1754, 3177, 8, 1577, 8848, 323, 668, 578, 41324, 19, 103, 379, 6, 151, 6981, 12675, 4, 2, 1, 1, 1, 1, 1, 1], 'label': 0, 'question': 'What tactics of the PVA were US and ROK troops not prepared to handle?', 'sentence': \"On 27 November at the Korean eastern front, a U.S. 7th Infantry Division Regimental Combat Team (3,000 soldiers) and the U.S. 1st Marine Division (12,000–15,000 marines) were unprepared for the PVA 9th Army Group's three-pronged encirclement tactics at the Battle of Chosin Reservoir, but they managed to escape under Air Force and X Corps support fire—albeit with some 15,000 collective casualties.\"}.\n",
            "10/03/2021 19:18:49 - INFO - __main__ - Sample 14592 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 14592, 'input_ids': [0, 6179, 222, 5, 10377, 82, 619, 59, 234, 16151, 18, 1263, 7, 5, 908, 116, 2, 2, 487, 16151, 222, 45, 619, 14, 5, 10377, 2938, 21, 1227, 13, 10, 12065, 8, 222, 45, 28528, 16959, 19048, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'question': \"How did the Egyptian people feel about Nasser's response to the attack?\", 'sentence': 'Nasser did not feel that the Egyptian Army was ready for a confrontation and did not retaliate militarily.'}.\n",
            "10/03/2021 19:18:49 - INFO - __main__ - Sample 3278 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 3278, 'input_ids': [0, 2264, 761, 9, 3054, 32, 4453, 9, 602, 160, 32039, 116, 2, 2, 13863, 4062, 13565, 574, 3054, 32, 4453, 9, 602, 160, 32039, 31, 10, 1514, 15, 5, 9124, 6, 634, 5, 6854, 8, 10, 878, 386, 16, 444, 55, 2423, 5693, 8, 8621, 10, 19351, 1709, 2408, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'question': 'What kind of aircraft are capable of taking off vertically?', 'sentence': 'Although STOVL aircraft are capable of taking off vertically from a spot on the deck, using the ramp and a running start is far more fuel efficient and permits a heavier launch weight.'}.\n",
            "10/03/2021 19:18:49 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "10/03/2021 19:18:49 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981\n",
            "10/03/2021 19:18:49 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.py\n",
            "10/03/2021 19:18:49 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/dataset_infos.json\n",
            "10/03/2021 19:18:49 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.12.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981/glue.json\n",
            "[INFO|trainer.py:541] 2021-10-03 19:18:53,584 >> The following columns in the training set  don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: question, sentence, idx.\n",
            "[INFO|trainer.py:1196] 2021-10-03 19:18:53,597 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-10-03 19:18:53,597 >>   Num examples = 104743\n",
            "[INFO|trainer.py:1198] 2021-10-03 19:18:53,597 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1199] 2021-10-03 19:18:53,597 >>   Instantaneous batch size per device = 64\n",
            "[INFO|trainer.py:1200] 2021-10-03 19:18:53,597 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1201] 2021-10-03 19:18:53,597 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-10-03 19:18:53,597 >>   Total optimization steps = 1637\n",
            "  0% 0/1637 [00:00<?, ?it/s][INFO|modeling_longformer.py:1853] 2021-10-03 19:18:53,633 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:18:53,634 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  0% 1/1637 [00:05<2:24:51,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:18:58,931 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:18:58,932 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  0% 2/1637 [00:10<2:24:11,  5.29s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:19:04,208 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:19:04,208 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  0% 3/1637 [00:15<2:24:20,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:19:09,519 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:19:09,520 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  0% 4/1637 [00:21<2:24:04,  5.29s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:19:14,802 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:19:14,802 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  0% 5/1637 [00:26<2:24:03,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:19:20,103 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:19:20,104 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  0% 6/1637 [00:31<2:24:00,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:19:25,403 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:19:25,403 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  0% 7/1637 [00:37<2:23:59,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:19:30,709 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:19:30,709 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  0% 8/1637 [00:42<2:23:57,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:19:36,016 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:19:36,016 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 9/1637 [00:47<2:23:48,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:19:41,311 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:19:41,311 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 10/1637 [00:52<2:23:39,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:19:46,602 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:19:46,602 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 11/1637 [00:58<2:23:33,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:19:51,899 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:19:51,900 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 12/1637 [01:03<2:23:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:19:57,225 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:19:57,226 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 13/1637 [01:08<2:23:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:20:02,539 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:20:02,540 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 14/1637 [01:14<2:23:29,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:20:07,834 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:20:07,834 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 15/1637 [01:19<2:23:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:20:13,145 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:20:13,145 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 16/1637 [01:24<2:23:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:20:18,465 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:20:18,465 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 17/1637 [01:30<2:23:24,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:20:23,782 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:20:23,782 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 18/1637 [01:35<2:23:27,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:20:29,108 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:20:29,108 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 19/1637 [01:40<2:23:21,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:20:34,424 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:20:34,424 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 20/1637 [01:46<2:23:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:20:39,730 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:20:39,730 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 21/1637 [01:51<2:23:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:20:45,033 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:20:45,033 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 22/1637 [01:56<2:22:53,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:20:50,337 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:20:50,338 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 23/1637 [02:02<2:22:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:20:55,644 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:20:55,644 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 24/1637 [02:07<2:22:51,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:21:00,972 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:21:00,972 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 25/1637 [02:12<2:22:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:21:06,275 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:21:06,276 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 26/1637 [02:17<2:22:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:21:11,590 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:21:11,591 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 27/1637 [02:23<2:22:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:21:16,890 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:21:16,891 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 28/1637 [02:28<2:22:15,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:21:22,186 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:21:22,187 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 29/1637 [02:33<2:22:06,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:21:27,484 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:21:27,484 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 30/1637 [02:39<2:22:03,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:21:32,792 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:21:32,793 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 31/1637 [02:44<2:21:53,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:21:38,086 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:21:38,086 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 32/1637 [02:49<2:21:51,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:21:43,393 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:21:43,394 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 33/1637 [02:55<2:21:48,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:21:48,702 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:21:48,702 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 34/1637 [03:00<2:21:51,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:21:54,023 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:21:54,023 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 35/1637 [03:05<2:21:37,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:21:59,313 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:21:59,314 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 36/1637 [03:11<2:21:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:22:04,624 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:22:04,624 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 37/1637 [03:16<2:21:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:22:09,943 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:22:09,943 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 38/1637 [03:21<2:21:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:22:15,261 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:22:15,261 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 39/1637 [03:26<2:21:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:22:20,555 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:22:20,555 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 40/1637 [03:32<2:21:18,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:22:25,869 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:22:25,869 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 41/1637 [03:37<2:21:24,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:22:31,203 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:22:31,203 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 42/1637 [03:42<2:21:26,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:22:36,534 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:22:36,534 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 43/1637 [03:48<2:21:23,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:22:41,858 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:22:41,858 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 44/1637 [03:53<2:21:11,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:22:47,165 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:22:47,166 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 45/1637 [03:58<2:21:09,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:22:52,492 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:22:52,492 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 46/1637 [04:04<2:21:08,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:22:57,821 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:22:57,821 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 47/1637 [04:09<2:20:59,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:23:03,136 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:23:03,136 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 48/1637 [04:14<2:20:42,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:23:08,430 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:23:08,430 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 49/1637 [04:20<2:20:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:23:13,722 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:23:13,723 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 50/1637 [04:25<2:20:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:23:19,040 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:23:19,041 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 51/1637 [04:30<2:20:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:23:24,342 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:23:24,342 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 52/1637 [04:36<2:20:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:23:29,666 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:23:29,666 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 53/1637 [04:41<2:20:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:23:34,964 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:23:34,965 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 54/1637 [04:46<2:20:12,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:23:40,294 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:23:40,295 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 55/1637 [04:52<2:20:13,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:23:45,620 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:23:45,621 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 56/1637 [04:57<2:20:05,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:23:50,933 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:23:50,933 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 57/1637 [05:02<2:19:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:23:56,239 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:23:56,240 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 58/1637 [05:07<2:19:46,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:24:01,545 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:24:01,545 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 59/1637 [05:13<2:19:36,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:24:06,850 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:24:06,850 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 60/1637 [05:18<2:19:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:24:12,170 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:24:12,170 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 61/1637 [05:23<2:19:32,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:24:17,483 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:24:17,483 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 62/1637 [05:29<2:19:28,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:24:22,804 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:24:22,805 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 63/1637 [05:34<2:19:28,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:24:28,125 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:24:28,125 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 64/1637 [05:39<2:19:27,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:24:33,451 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:24:33,451 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 65/1637 [05:45<2:19:20,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:24:38,765 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:24:38,765 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 66/1637 [05:50<2:19:10,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:24:44,071 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:24:44,072 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 67/1637 [05:55<2:19:06,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:24:49,391 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:24:49,392 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 68/1637 [06:01<2:19:01,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:24:54,707 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:24:54,707 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 69/1637 [06:06<2:18:51,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:25:00,015 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:25:00,016 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 70/1637 [06:11<2:18:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:25:05,325 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:25:05,326 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 71/1637 [06:17<2:18:38,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:25:10,635 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:25:10,636 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 72/1637 [06:22<2:18:32,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:25:15,946 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:25:15,946 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 73/1637 [06:27<2:18:23,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:25:21,249 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:25:21,250 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 74/1637 [06:32<2:18:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:25:26,545 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:25:26,545 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 75/1637 [06:38<2:18:07,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:25:31,852 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:25:31,852 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 76/1637 [06:43<2:18:09,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:25:37,174 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:25:37,174 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 77/1637 [06:48<2:18:07,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:25:42,491 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:25:42,491 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 78/1637 [06:54<2:18:06,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:25:47,813 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:25:47,814 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 79/1637 [06:59<2:18:06,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:25:53,142 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:25:53,142 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 80/1637 [07:04<2:17:58,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:25:58,452 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:25:58,452 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 81/1637 [07:10<2:17:50,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:26:03,764 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:26:03,764 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 82/1637 [07:15<2:17:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:26:09,075 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:26:09,076 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 83/1637 [07:20<2:17:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:26:14,381 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:26:14,382 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 84/1637 [07:26<2:17:29,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:26:19,694 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:26:19,694 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 85/1637 [07:31<2:17:21,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:26:25,000 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:26:25,001 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 86/1637 [07:36<2:17:25,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:26:30,331 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:26:30,332 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 87/1637 [07:42<2:17:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:26:35,639 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:26:35,640 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 88/1637 [07:47<2:17:07,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:26:40,944 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:26:40,944 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 89/1637 [07:52<2:17:00,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:26:46,253 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:26:46,254 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 90/1637 [07:57<2:16:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:26:51,563 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:26:51,563 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 91/1637 [08:03<2:16:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:26:56,871 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:26:56,871 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 92/1637 [08:08<2:16:42,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:27:02,177 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:27:02,178 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 93/1637 [08:13<2:16:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:27:07,480 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:27:07,481 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 94/1637 [08:19<2:16:38,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:27:12,809 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:27:12,810 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 95/1637 [08:24<2:16:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:27:18,122 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:27:18,122 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 96/1637 [08:29<2:16:24,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:27:23,427 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:27:23,427 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 97/1637 [08:35<2:16:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:27:28,733 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:27:28,734 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 98/1637 [08:40<2:16:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:27:34,043 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:27:34,043 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 99/1637 [08:45<2:16:07,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:27:39,355 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:27:39,355 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 100/1637 [08:51<2:15:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:27:44,659 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:27:44,659 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 101/1637 [08:56<2:15:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:27:49,980 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:27:49,980 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 102/1637 [09:01<2:16:02,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:27:55,311 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:27:55,311 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 103/1637 [09:06<2:15:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:28:00,613 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:28:00,613 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 104/1637 [09:12<2:15:42,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:28:05,920 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:28:05,920 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 105/1637 [09:17<2:15:30,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:28:11,216 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:28:11,216 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 106/1637 [09:22<2:15:20,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:28:16,514 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:28:16,515 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 107/1637 [09:28<2:15:12,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:28:21,814 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:28:21,814 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 108/1637 [09:33<2:15:03,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:28:27,107 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:28:27,107 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 109/1637 [09:38<2:15:03,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:28:32,418 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:28:32,418 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 110/1637 [09:44<2:15:03,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:28:37,735 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:28:37,735 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 111/1637 [09:49<2:15:00,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:28:43,046 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:28:43,047 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 112/1637 [09:54<2:14:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:28:48,352 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:28:48,352 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 113/1637 [10:00<2:14:51,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:28:53,665 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:28:53,666 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 114/1637 [10:05<2:14:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:28:58,990 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:28:58,991 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 115/1637 [10:10<2:14:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:29:04,303 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:29:04,303 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 116/1637 [10:15<2:14:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:29:09,616 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:29:09,616 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 117/1637 [10:21<2:14:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:29:14,926 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:29:14,926 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 118/1637 [10:26<2:14:28,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:29:20,236 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:29:20,237 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 119/1637 [10:31<2:14:21,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:29:25,544 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:29:25,544 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 120/1637 [10:37<2:14:25,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:29:30,876 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:29:30,877 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 121/1637 [10:42<2:14:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:29:36,184 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:29:36,185 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 122/1637 [10:47<2:14:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:29:41,491 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:29:41,492 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 123/1637 [10:53<2:13:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:29:46,795 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:29:46,795 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 124/1637 [10:58<2:13:53,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:29:52,104 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:29:52,105 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 125/1637 [11:03<2:13:58,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:29:57,436 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:29:57,437 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 126/1637 [11:09<2:13:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:30:02,732 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:30:02,732 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 127/1637 [11:14<2:13:38,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:30:08,043 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:30:08,043 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 128/1637 [11:19<2:13:41,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:30:13,372 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:30:13,372 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 129/1637 [11:25<2:13:39,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:30:18,696 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:30:18,696 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 130/1637 [11:30<2:13:32,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:30:24,008 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:30:24,008 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 131/1637 [11:35<2:13:31,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:30:29,336 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:30:29,336 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 132/1637 [11:41<2:13:25,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:30:34,654 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:30:34,655 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 133/1637 [11:46<2:13:16,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:30:39,964 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:30:39,964 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 134/1637 [11:51<2:13:13,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:30:45,286 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:30:45,287 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 135/1637 [11:56<2:13:04,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:30:50,597 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:30:50,598 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 136/1637 [12:02<2:12:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:30:55,892 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:30:55,892 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 137/1637 [12:07<2:12:51,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:31:01,217 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:31:01,217 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 138/1637 [12:12<2:12:39,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:31:06,515 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:31:06,516 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 139/1637 [12:18<2:12:32,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:31:11,822 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:31:11,823 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 140/1637 [12:23<2:12:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:31:17,133 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:31:17,133 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 141/1637 [12:28<2:12:23,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:31:22,444 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:31:22,444 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 142/1637 [12:34<2:12:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:31:27,747 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:31:27,748 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 143/1637 [12:39<2:12:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:31:33,067 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:31:33,067 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 144/1637 [12:44<2:12:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:31:38,375 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:31:38,376 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 145/1637 [12:50<2:12:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:31:43,697 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:31:43,698 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 146/1637 [12:55<2:12:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:31:49,007 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:31:49,007 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 147/1637 [13:00<2:11:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:31:54,303 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:31:54,304 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 148/1637 [13:05<2:11:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:31:59,613 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:31:59,613 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 149/1637 [13:11<2:11:31,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:32:04,905 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:32:04,906 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 150/1637 [13:16<2:11:29,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:32:10,215 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:32:10,216 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 151/1637 [13:21<2:11:23,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:32:15,520 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:32:15,520 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 152/1637 [13:27<2:11:21,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:32:20,833 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:32:20,833 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 153/1637 [13:32<2:11:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:32:26,140 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:32:26,141 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 154/1637 [13:37<2:11:12,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:32:31,453 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:32:31,453 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 155/1637 [13:43<2:11:13,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:32:36,774 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:32:36,774 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 156/1637 [13:48<2:11:05,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:32:42,079 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:32:42,079 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 157/1637 [13:53<2:11:03,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:32:47,405 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:32:47,405 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 158/1637 [13:59<2:11:00,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:32:52,717 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:32:52,718 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 159/1637 [14:04<2:10:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:32:58,029 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:32:58,030 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 160/1637 [14:09<2:10:51,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:33:03,349 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:33:03,350 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 161/1637 [14:15<2:10:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:33:08,645 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:33:08,645 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 162/1637 [14:20<2:10:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:33:13,957 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:33:13,958 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 163/1637 [14:25<2:10:29,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:33:19,269 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:33:19,270 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 164/1637 [14:30<2:10:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:33:24,579 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:33:24,579 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 165/1637 [14:36<2:10:13,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:33:29,881 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:33:29,881 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 166/1637 [14:41<2:10:09,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:33:35,193 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:33:35,193 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 167/1637 [14:46<2:10:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:33:40,510 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:33:40,511 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 168/1637 [14:52<2:10:09,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:33:45,839 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:33:45,839 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 169/1637 [14:57<2:09:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:33:51,142 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:33:51,142 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 170/1637 [15:02<2:09:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:33:56,457 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:33:56,457 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 171/1637 [15:08<2:09:55,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:34:01,784 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:34:01,785 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 172/1637 [15:13<2:09:46,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:34:07,092 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:34:07,093 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 173/1637 [15:18<2:09:48,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:34:12,427 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:34:12,427 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 174/1637 [15:24<2:09:51,  5.33s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:34:17,764 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:34:17,765 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 175/1637 [15:29<2:09:40,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:34:23,077 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:34:23,077 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 176/1637 [15:34<2:09:31,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:34:28,389 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:34:28,390 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 177/1637 [15:40<2:09:31,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:34:33,721 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:34:33,721 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 178/1637 [15:45<2:09:29,  5.33s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:34:39,052 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:34:39,053 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 179/1637 [15:50<2:09:20,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:34:44,368 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:34:44,369 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 180/1637 [15:56<2:09:14,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:34:49,690 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:34:49,690 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 181/1637 [16:01<2:09:10,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:34:55,014 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:34:55,014 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 182/1637 [16:06<2:09:05,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:35:00,338 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:35:00,339 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 183/1637 [16:12<2:08:52,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:35:05,644 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:35:05,644 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 184/1637 [16:17<2:08:48,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:35:10,966 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:35:10,966 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 185/1637 [16:22<2:08:42,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:35:16,282 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:35:16,282 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 186/1637 [16:27<2:08:37,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:35:21,603 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:35:21,604 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 187/1637 [16:33<2:08:37,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:35:26,933 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:35:26,934 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 188/1637 [16:38<2:08:39,  5.33s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:35:32,271 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:35:32,272 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 189/1637 [16:43<2:08:28,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:35:37,587 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:35:37,587 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 190/1637 [16:49<2:08:18,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:35:42,900 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:35:42,901 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 191/1637 [16:54<2:08:17,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:35:48,230 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:35:48,230 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 192/1637 [16:59<2:08:11,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:35:53,551 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:35:53,551 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 193/1637 [17:05<2:07:59,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:35:58,858 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:35:58,858 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 194/1637 [17:10<2:07:52,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:36:04,172 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:36:04,173 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 195/1637 [17:15<2:07:50,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:36:09,499 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:36:09,499 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 196/1637 [17:21<2:07:44,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:36:14,815 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:36:14,815 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 197/1637 [17:26<2:07:37,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:36:20,131 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:36:20,131 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 198/1637 [17:31<2:07:38,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:36:25,464 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:36:25,464 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 199/1637 [17:37<2:07:33,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:36:30,786 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:36:30,787 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 200/1637 [17:42<2:07:25,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:36:36,103 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:36:36,103 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 201/1637 [17:47<2:07:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:36:41,401 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:36:41,401 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 202/1637 [17:53<2:07:07,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:36:46,719 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:36:46,719 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 203/1637 [17:58<2:07:09,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:36:52,053 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:36:52,054 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 204/1637 [18:03<2:07:05,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:36:57,376 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:36:57,376 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 205/1637 [18:09<2:07:06,  5.33s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:37:02,713 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:37:02,714 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 206/1637 [18:14<2:06:58,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:37:08,031 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:37:08,031 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 207/1637 [18:19<2:06:51,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:37:13,351 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:37:13,351 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 208/1637 [18:25<2:06:40,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:37:18,661 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:37:18,662 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 209/1637 [18:30<2:06:31,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:37:23,971 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:37:23,971 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 210/1637 [18:35<2:06:35,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:37:29,308 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:37:29,309 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 211/1637 [18:41<2:06:34,  5.33s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:37:34,641 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:37:34,642 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 212/1637 [18:46<2:06:27,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:37:39,963 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:37:39,964 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 213/1637 [18:51<2:06:21,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:37:45,286 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:37:45,287 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 214/1637 [18:56<2:06:08,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:37:50,593 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:37:50,593 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 215/1637 [19:02<2:06:12,  5.33s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:37:55,933 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:37:55,933 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 216/1637 [19:07<2:06:05,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:38:01,254 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:38:01,254 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 217/1637 [19:12<2:05:56,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:38:06,568 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:38:06,568 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 218/1637 [19:18<2:05:49,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:38:11,887 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:38:11,887 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 219/1637 [19:23<2:05:39,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:38:17,197 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:38:17,197 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 220/1637 [19:28<2:05:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:38:22,496 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:38:22,496 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 221/1637 [19:34<2:05:27,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:38:27,821 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:38:27,822 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 222/1637 [19:39<2:05:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:38:33,132 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:38:33,133 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 223/1637 [19:44<2:05:18,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:38:38,457 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:38:38,458 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 224/1637 [19:50<2:05:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:38:43,757 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:38:43,758 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 225/1637 [19:55<2:05:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:38:49,073 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:38:49,074 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 226/1637 [20:00<2:05:04,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:38:54,405 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:38:54,406 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 227/1637 [20:06<2:05:01,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:38:59,727 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:38:59,727 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 228/1637 [20:11<2:04:53,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:39:05,042 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:39:05,042 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 229/1637 [20:16<2:04:39,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:39:10,338 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:39:10,338 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 230/1637 [20:22<2:04:41,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:39:15,670 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:39:15,670 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 231/1637 [20:27<2:04:42,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:39:21,001 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:39:21,001 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 232/1637 [20:32<2:04:35,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:39:26,318 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:39:26,318 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 233/1637 [20:38<2:04:29,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:39:31,637 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:39:31,637 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 234/1637 [20:43<2:04:26,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:39:36,963 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:39:36,964 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 235/1637 [20:48<2:04:16,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:39:42,273 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:39:42,273 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 236/1637 [20:53<2:04:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:39:47,571 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:39:47,572 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 237/1637 [20:59<2:04:01,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:39:52,896 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:39:52,896 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 238/1637 [21:04<2:04:02,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:39:58,226 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:39:58,226 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 239/1637 [21:09<2:04:02,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:40:03,558 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:40:03,558 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 240/1637 [21:15<2:03:52,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:40:08,870 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:40:08,871 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 241/1637 [21:20<2:03:49,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:40:14,195 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:40:14,196 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 242/1637 [21:25<2:03:39,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:40:19,507 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:40:19,508 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 243/1637 [21:31<2:03:34,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:40:24,827 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:40:24,827 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 244/1637 [21:36<2:03:30,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:40:30,149 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:40:30,149 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 245/1637 [21:41<2:03:22,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:40:35,462 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:40:35,462 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 246/1637 [21:47<2:03:18,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:40:40,785 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:40:40,785 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 247/1637 [21:52<2:03:11,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:40:46,099 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:40:46,099 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 248/1637 [21:57<2:03:02,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:40:51,408 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:40:51,408 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 249/1637 [22:03<2:02:49,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:40:56,704 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:40:56,705 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 250/1637 [22:08<2:02:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:41:02,014 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:41:02,014 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 251/1637 [22:13<2:02:47,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:41:07,344 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:41:07,344 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 252/1637 [22:19<2:02:45,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:41:12,668 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:41:12,668 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 253/1637 [22:24<2:02:46,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:41:18,001 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:41:18,002 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 254/1637 [22:29<2:02:31,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:41:23,301 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:41:23,301 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 255/1637 [22:35<2:02:27,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:41:28,620 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:41:28,621 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 256/1637 [22:40<2:02:29,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:41:33,954 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:41:33,955 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 257/1637 [22:45<2:02:19,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:41:39,265 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:41:39,265 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 258/1637 [22:50<2:02:12,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:41:44,581 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:41:44,581 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 259/1637 [22:56<2:02:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:41:49,885 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:41:49,885 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 260/1637 [23:01<2:02:02,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:41:55,212 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:41:55,213 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 261/1637 [23:06<2:01:57,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:42:00,531 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:42:00,531 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 262/1637 [23:12<2:01:46,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:42:05,836 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:42:05,837 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 263/1637 [23:17<2:01:45,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:42:11,159 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:42:11,160 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 264/1637 [23:22<2:01:41,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:42:16,480 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:42:16,480 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 265/1637 [23:28<2:01:39,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:42:21,806 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:42:21,807 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 266/1637 [23:33<2:01:33,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:42:27,125 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:42:27,125 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 267/1637 [23:38<2:01:32,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:42:32,454 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:42:32,454 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 268/1637 [23:44<2:01:20,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:42:37,761 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:42:37,761 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 269/1637 [23:49<2:01:17,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:42:43,085 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:42:43,086 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 270/1637 [23:54<2:01:17,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:42:48,418 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:42:48,418 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 271/1637 [24:00<2:01:11,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:42:53,741 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:42:53,741 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 272/1637 [24:05<2:01:02,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:42:59,054 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:42:59,054 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 273/1637 [24:10<2:00:55,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:43:04,372 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:43:04,372 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 274/1637 [24:16<2:00:54,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:43:09,700 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:43:09,700 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 275/1637 [24:21<2:00:52,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:43:15,031 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:43:15,031 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 276/1637 [24:26<2:00:43,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:43:20,347 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:43:20,347 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 277/1637 [24:32<2:00:36,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:43:25,663 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:43:25,664 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 278/1637 [24:37<2:00:31,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:43:30,988 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:43:30,988 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 279/1637 [24:42<2:00:20,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:43:36,294 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:43:36,295 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 280/1637 [24:47<2:00:14,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:43:41,608 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:43:41,609 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 281/1637 [24:53<2:00:12,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:43:46,934 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:43:46,934 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 282/1637 [24:58<2:00:01,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:43:52,239 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:43:52,240 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 283/1637 [25:03<1:59:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:43:57,551 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:43:57,551 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 284/1637 [25:09<1:59:45,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:44:02,855 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:44:02,855 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 285/1637 [25:14<1:59:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:44:08,177 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:44:08,177 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 286/1637 [25:19<1:59:41,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:44:13,496 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:44:13,496 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 287/1637 [25:25<1:59:32,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:44:18,803 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:44:18,803 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 288/1637 [25:30<1:59:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:44:24,098 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:44:24,098 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 289/1637 [25:35<1:59:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:44:29,410 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:44:29,411 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 290/1637 [25:41<1:59:13,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:44:34,726 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:44:34,727 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 291/1637 [25:46<1:59:07,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:44:40,035 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:44:40,035 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 292/1637 [25:51<1:59:03,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:44:45,348 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:44:45,348 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 293/1637 [25:57<1:59:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:44:50,668 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:44:50,668 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 294/1637 [26:02<1:58:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:44:55,971 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:44:55,972 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 295/1637 [26:07<1:58:56,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:45:01,306 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:45:01,307 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 296/1637 [26:12<1:58:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:45:06,604 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:45:06,604 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 297/1637 [26:18<1:58:43,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:45:11,932 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:45:11,932 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 298/1637 [26:23<1:58:39,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:45:17,248 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:45:17,249 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 299/1637 [26:28<1:58:39,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:45:22,579 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:45:22,579 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 300/1637 [26:34<1:58:36,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:45:27,907 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:45:27,907 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 301/1637 [26:39<1:58:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:45:33,202 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:45:33,202 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 302/1637 [26:44<1:58:16,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:45:38,520 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:45:38,521 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 303/1637 [26:50<1:58:14,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:45:43,844 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:45:43,845 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 304/1637 [26:55<1:58:11,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:45:49,169 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:45:49,170 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 305/1637 [27:00<1:58:12,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:45:54,505 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:45:54,505 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 306/1637 [27:06<1:57:57,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:45:59,805 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:45:59,806 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 307/1637 [27:11<1:57:52,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:46:05,124 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:46:05,124 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 308/1637 [27:16<1:57:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:46:10,426 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:46:10,426 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 309/1637 [27:22<1:57:38,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:46:15,748 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:46:15,748 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 310/1637 [27:27<1:57:25,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:46:21,041 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:46:21,041 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 311/1637 [27:32<1:57:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:46:26,356 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:46:26,357 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 312/1637 [27:38<1:57:18,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:46:31,672 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:46:31,672 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 313/1637 [27:43<1:57:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:46:36,978 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:46:36,978 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 314/1637 [27:48<1:57:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:46:42,304 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:46:42,304 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 315/1637 [27:54<1:57:12,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:46:47,636 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:46:47,637 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 316/1637 [27:59<1:57:01,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:46:52,940 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:46:52,940 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 317/1637 [28:04<1:56:53,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:46:58,247 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:46:58,247 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 318/1637 [28:09<1:56:52,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:47:03,572 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:47:03,573 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 319/1637 [28:15<1:56:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:47:08,883 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:47:08,883 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 320/1637 [28:20<1:56:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:47:14,190 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:47:14,191 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 321/1637 [28:25<1:56:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:47:19,503 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:47:19,503 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 322/1637 [28:31<1:56:23,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:47:24,809 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:47:24,809 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 323/1637 [28:36<1:56:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:47:30,111 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:47:30,111 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 324/1637 [28:41<1:56:04,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:47:35,406 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:47:35,406 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 325/1637 [28:47<1:56:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:47:40,719 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:47:40,719 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 326/1637 [28:52<1:56:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:47:46,049 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:47:46,049 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 327/1637 [28:57<1:56:00,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:47:51,361 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:47:51,361 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 328/1637 [29:03<1:55:51,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:47:56,666 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:47:56,666 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 329/1637 [29:08<1:55:46,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:48:01,975 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:48:01,975 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 330/1637 [29:13<1:55:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:48:07,286 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:48:07,287 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 331/1637 [29:18<1:55:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:48:12,601 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:48:12,601 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 332/1637 [29:24<1:55:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:48:17,912 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:48:17,912 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 333/1637 [29:29<1:55:28,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:48:23,228 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:48:23,228 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 334/1637 [29:34<1:55:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:48:28,541 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:48:28,541 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 335/1637 [29:40<1:55:18,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:48:33,857 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:48:33,857 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 336/1637 [29:45<1:55:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:48:39,173 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:48:39,173 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 337/1637 [29:50<1:55:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:48:44,480 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:48:44,480 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 338/1637 [29:56<1:55:05,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:48:49,805 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:48:49,806 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 339/1637 [30:01<1:54:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:48:55,106 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:48:55,106 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 340/1637 [30:06<1:54:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:49:00,405 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:49:00,405 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 341/1637 [30:12<1:54:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:49:05,719 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:49:05,719 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 342/1637 [30:17<1:54:42,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:49:11,047 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:49:11,048 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 343/1637 [30:22<1:54:29,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:49:16,341 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:49:16,341 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 344/1637 [30:28<1:54:23,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:49:21,651 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:49:21,651 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 345/1637 [30:33<1:54:19,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:49:26,957 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:49:26,958 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 346/1637 [30:38<1:54:08,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:49:32,252 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:49:32,253 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 347/1637 [30:43<1:54:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:49:37,567 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:49:37,567 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 348/1637 [30:49<1:54:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:49:42,878 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:49:42,879 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 349/1637 [30:54<1:53:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:49:48,191 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:49:48,192 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 350/1637 [30:59<1:53:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:49:53,505 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:49:53,505 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 351/1637 [31:05<1:53:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:49:58,812 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:49:58,812 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 352/1637 [31:10<1:53:45,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:50:04,127 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:50:04,127 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 353/1637 [31:15<1:53:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:50:09,431 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:50:09,432 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 354/1637 [31:21<1:53:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:50:14,755 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:50:14,756 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 355/1637 [31:26<1:53:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:50:20,068 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:50:20,068 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 356/1637 [31:31<1:53:21,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:50:25,367 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:50:25,367 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 357/1637 [31:37<1:53:12,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:50:30,667 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:50:30,668 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 358/1637 [31:42<1:53:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:50:35,977 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:50:35,977 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 359/1637 [31:47<1:53:04,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:50:41,288 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:50:41,288 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 360/1637 [31:52<1:52:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:50:46,598 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:50:46,598 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 361/1637 [31:58<1:53:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:50:51,926 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:50:51,927 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 362/1637 [32:03<1:52:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:50:57,230 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:50:57,231 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 363/1637 [32:08<1:52:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:51:02,543 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:51:02,543 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 364/1637 [32:14<1:52:40,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:51:07,852 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:51:07,853 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 365/1637 [32:19<1:52:44,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:51:13,186 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:51:13,187 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 366/1637 [32:24<1:52:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:51:18,489 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:51:18,489 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 367/1637 [32:30<1:52:30,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:51:23,810 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:51:23,810 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 368/1637 [32:35<1:52:19,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:51:29,108 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:51:29,108 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 369/1637 [32:40<1:52:13,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:51:34,425 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:51:34,425 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 370/1637 [32:46<1:52:12,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:51:39,740 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:51:39,741 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 371/1637 [32:51<1:52:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:51:45,052 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:51:45,053 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 372/1637 [32:56<1:52:00,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:51:50,364 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:51:50,364 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 373/1637 [33:02<1:51:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:51:55,677 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:51:55,678 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 374/1637 [33:07<1:51:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:52:00,975 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:52:00,975 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 375/1637 [33:12<1:51:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:52:06,289 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:52:06,289 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 376/1637 [33:17<1:51:38,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:52:11,604 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:52:11,604 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 377/1637 [33:23<1:51:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:52:16,912 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:52:16,912 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 378/1637 [33:28<1:51:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:52:22,215 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:52:22,215 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 379/1637 [33:33<1:51:18,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:52:27,524 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:52:27,524 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 380/1637 [33:39<1:51:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:52:32,842 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:52:32,842 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 381/1637 [33:44<1:51:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:52:38,152 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:52:38,152 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 382/1637 [33:49<1:51:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:52:43,474 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:52:43,474 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 383/1637 [33:55<1:51:00,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:52:48,778 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:52:48,779 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 384/1637 [34:00<1:50:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:52:54,077 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:52:54,078 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 385/1637 [34:05<1:50:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:52:59,378 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:52:59,379 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 386/1637 [34:11<1:50:36,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:53:04,681 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:53:04,682 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 387/1637 [34:16<1:50:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:53:10,020 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:53:10,020 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 388/1637 [34:21<1:50:38,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:53:15,335 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:53:15,336 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 389/1637 [34:27<1:50:32,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:53:20,648 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:53:20,649 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 390/1637 [34:32<1:50:30,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:53:25,971 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:53:25,972 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 391/1637 [34:37<1:50:24,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:53:31,285 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:53:31,286 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 392/1637 [34:42<1:50:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:53:36,595 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:53:36,595 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 393/1637 [34:48<1:50:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:53:41,909 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:53:41,909 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 394/1637 [34:53<1:50:10,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:53:47,237 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:53:47,238 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 395/1637 [34:58<1:49:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:53:52,537 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:53:52,537 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 396/1637 [35:04<1:49:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:53:57,837 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:53:57,837 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 397/1637 [35:09<1:49:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:54:03,147 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:54:03,148 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 398/1637 [35:14<1:49:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:54:08,475 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:54:08,476 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 399/1637 [35:20<1:49:40,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:54:13,790 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:54:13,791 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 400/1637 [35:25<1:49:32,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:54:19,099 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:54:19,099 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 401/1637 [35:30<1:49:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:54:24,409 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:54:24,410 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 402/1637 [35:36<1:49:19,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:54:29,717 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:54:29,717 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 403/1637 [35:41<1:49:20,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:54:35,048 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:54:35,049 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 404/1637 [35:46<1:49:19,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:54:40,374 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:54:40,375 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 405/1637 [35:52<1:49:07,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:54:45,675 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:54:45,675 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 406/1637 [35:57<1:48:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:54:50,972 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:54:50,972 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 407/1637 [36:02<1:48:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:54:56,301 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:54:56,301 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 408/1637 [36:08<1:48:56,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:55:01,628 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:55:01,629 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 409/1637 [36:13<1:48:48,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:55:06,940 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:55:06,941 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 410/1637 [36:18<1:48:44,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:55:12,261 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:55:12,261 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 411/1637 [36:23<1:48:41,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:55:17,584 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:55:17,584 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 412/1637 [36:29<1:48:28,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:55:22,883 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:55:22,883 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 413/1637 [36:34<1:48:23,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:55:28,195 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:55:28,195 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 414/1637 [36:39<1:48:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:55:33,505 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:55:33,505 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 415/1637 [36:45<1:48:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:55:38,818 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:55:38,818 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 416/1637 [36:50<1:48:10,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:55:44,141 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:55:44,141 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 417/1637 [36:55<1:48:07,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:55:49,463 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:55:49,464 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 418/1637 [37:01<1:48:04,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:55:54,786 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:55:54,786 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 419/1637 [37:06<1:47:55,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:56:00,097 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:56:00,097 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 420/1637 [37:11<1:47:48,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:56:05,409 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:56:05,410 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 421/1637 [37:17<1:47:40,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:56:10,715 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:56:10,716 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 422/1637 [37:22<1:47:38,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:56:16,040 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:56:16,040 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 423/1637 [37:27<1:47:39,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:56:21,372 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:56:21,372 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 424/1637 [37:33<1:47:32,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:56:26,686 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:56:26,687 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 425/1637 [37:38<1:47:19,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:56:31,987 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:56:31,987 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 426/1637 [37:43<1:47:12,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:56:37,295 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:56:37,295 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 427/1637 [37:48<1:46:58,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:56:42,584 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:56:42,584 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 428/1637 [37:54<1:46:50,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:56:47,881 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:56:47,881 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 429/1637 [37:59<1:46:46,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:56:53,189 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:56:53,189 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 430/1637 [38:04<1:46:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:56:58,497 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:56:58,498 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 431/1637 [38:10<1:46:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:57:03,810 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:57:03,811 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 432/1637 [38:15<1:46:38,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:57:09,127 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:57:09,127 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 433/1637 [38:20<1:46:36,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:57:14,444 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:57:14,444 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 434/1637 [38:26<1:46:25,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:57:19,741 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:57:19,741 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 435/1637 [38:31<1:46:19,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:57:25,050 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:57:25,050 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 436/1637 [38:36<1:46:18,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:57:30,367 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:57:30,367 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 437/1637 [38:42<1:46:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:57:35,686 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:57:35,686 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 438/1637 [38:47<1:46:12,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:57:41,005 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:57:41,005 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 439/1637 [38:52<1:46:04,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:57:46,310 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:57:46,311 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 440/1637 [38:58<1:46:00,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:57:51,629 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:57:51,629 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 441/1637 [39:03<1:45:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:57:56,946 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:57:56,946 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 442/1637 [39:08<1:45:55,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:58:02,273 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:58:02,273 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 443/1637 [39:13<1:45:51,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:58:07,595 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:58:07,596 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 444/1637 [39:19<1:45:42,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:58:12,904 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:58:12,905 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 445/1637 [39:24<1:45:38,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:58:18,223 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:58:18,224 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 446/1637 [39:29<1:45:30,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:58:23,533 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:58:23,533 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 447/1637 [39:35<1:45:21,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:58:28,839 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:58:28,839 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 448/1637 [39:40<1:45:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:58:34,151 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:58:34,151 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 449/1637 [39:45<1:45:09,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:58:39,460 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:58:39,460 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 450/1637 [39:51<1:45:09,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:58:44,786 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:58:44,786 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 451/1637 [39:56<1:45:04,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:58:50,100 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:58:50,101 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 452/1637 [40:01<1:44:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:58:55,414 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:58:55,414 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 453/1637 [40:07<1:44:45,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:59:00,708 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:59:00,708 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 454/1637 [40:12<1:44:40,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:59:06,018 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:59:06,018 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 455/1637 [40:17<1:44:35,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:59:11,327 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:59:11,328 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 456/1637 [40:23<1:44:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:59:16,649 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:59:16,649 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 457/1637 [40:28<1:44:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:59:21,957 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:59:21,958 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 458/1637 [40:33<1:44:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:59:27,267 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:59:27,267 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 459/1637 [40:38<1:44:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:59:32,578 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:59:32,578 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 460/1637 [40:44<1:44:05,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:59:37,872 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:59:37,872 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 461/1637 [40:49<1:44:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:59:43,186 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:59:43,186 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 462/1637 [40:54<1:43:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:59:48,499 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:59:48,500 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 463/1637 [41:00<1:43:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:59:53,810 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:59:53,811 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 464/1637 [41:05<1:43:49,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 19:59:59,120 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 19:59:59,121 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 465/1637 [41:10<1:43:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:00:04,425 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:00:04,425 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 466/1637 [41:16<1:43:44,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:00:09,757 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:00:09,758 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 467/1637 [41:21<1:43:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:00:15,067 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:00:15,067 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 468/1637 [41:26<1:43:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:00:20,380 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:00:20,380 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 469/1637 [41:32<1:43:24,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:00:25,690 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:00:25,690 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 470/1637 [41:37<1:43:25,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:00:31,018 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:00:31,019 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 471/1637 [41:42<1:43:22,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:00:36,343 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:00:36,343 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 472/1637 [41:48<1:43:15,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:00:41,658 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:00:41,659 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 473/1637 [41:53<1:43:07,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:00:46,969 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:00:46,969 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 474/1637 [41:58<1:42:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:00:52,276 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:00:52,276 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 475/1637 [42:03<1:42:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:00:57,588 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:00:57,588 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 476/1637 [42:09<1:42:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:01:02,907 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:01:02,908 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 477/1637 [42:14<1:42:42,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:01:08,214 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:01:08,214 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 478/1637 [42:19<1:42:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:01:13,510 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:01:13,511 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 479/1637 [42:25<1:42:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:01:18,822 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:01:18,822 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 480/1637 [42:30<1:42:21,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:01:24,130 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:01:24,130 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 481/1637 [42:35<1:42:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:01:29,434 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:01:29,434 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 482/1637 [42:41<1:42:13,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:01:34,751 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:01:34,752 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 483/1637 [42:46<1:42:03,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:01:40,049 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:01:40,050 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 484/1637 [42:51<1:41:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:01:45,357 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:01:45,358 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 485/1637 [42:57<1:41:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:01:50,682 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:01:50,683 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 486/1637 [43:02<1:41:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:01:55,998 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:01:55,998 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 487/1637 [43:07<1:41:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:02:01,308 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:02:01,308 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 488/1637 [43:13<1:41:46,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:02:06,629 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:02:06,630 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 489/1637 [43:18<1:41:42,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:02:11,948 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:02:11,948 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 490/1637 [43:23<1:41:29,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:02:17,240 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:02:17,240 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 491/1637 [43:28<1:41:18,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:02:22,533 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:02:22,534 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 492/1637 [43:34<1:41:10,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:02:27,828 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:02:27,828 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 493/1637 [43:39<1:41:12,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:02:33,152 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:02:33,152 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 494/1637 [43:44<1:41:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:02:38,466 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:02:38,466 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 495/1637 [43:50<1:41:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:02:43,768 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:02:43,768 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 496/1637 [43:55<1:40:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:02:49,074 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:02:49,075 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 497/1637 [44:00<1:40:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:02:54,394 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:02:54,394 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 498/1637 [44:06<1:40:49,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:02:59,706 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:02:59,707 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 499/1637 [44:11<1:40:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:03:05,011 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:03:05,011 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "{'loss': 0.3888, 'learning_rate': 1.3891264508246794e-05, 'epoch': 0.31}\n",
            " 31% 500/1637 [44:16<1:40:35,  5.31s/it][INFO|trainer.py:1987] 2021-10-03 20:03:10,308 >> Saving model checkpoint to /tmp/qnli/checkpoint-500\n",
            "[INFO|configuration_utils.py:413] 2021-10-03 20:03:10,309 >> Configuration saved in /tmp/qnli/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-03 20:03:12,029 >> Model weights saved in /tmp/qnli/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-03 20:03:12,030 >> tokenizer config file saved in /tmp/qnli/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-03 20:03:12,030 >> Special tokens file saved in /tmp/qnli/checkpoint-500/special_tokens_map.json\n",
            "[INFO|modeling_longformer.py:1853] 2021-10-03 20:03:16,704 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:03:16,704 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 501/1637 [44:28<2:16:48,  7.23s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:03:22,019 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:03:22,020 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 502/1637 [44:33<2:05:48,  6.65s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:03:27,324 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:03:27,325 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 503/1637 [44:39<1:58:01,  6.24s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:03:32,622 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:03:32,622 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 504/1637 [44:44<1:52:32,  5.96s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:03:37,917 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:03:37,917 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 505/1637 [44:49<1:48:42,  5.76s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:03:43,216 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:03:43,216 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 506/1637 [44:54<1:45:58,  5.62s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:03:48,511 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:03:48,511 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 507/1637 [45:00<1:44:04,  5.53s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:03:53,816 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:03:53,816 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 508/1637 [45:05<1:42:48,  5.46s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:03:59,132 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:03:59,133 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 509/1637 [45:10<1:41:45,  5.41s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:04:04,425 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:04:04,426 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 510/1637 [45:16<1:41:05,  5.38s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:04:09,739 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:04:09,740 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 511/1637 [45:21<1:40:38,  5.36s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:04:15,054 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:04:15,055 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 512/1637 [45:26<1:40:13,  5.35s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:04:20,358 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:04:20,359 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 513/1637 [45:32<1:39:53,  5.33s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:04:25,662 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:04:25,662 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 514/1637 [45:37<1:39:38,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:04:30,964 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:04:30,964 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 515/1637 [45:42<1:39:29,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:04:36,278 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:04:36,279 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 516/1637 [45:47<1:39:22,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:04:41,592 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:04:41,593 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 517/1637 [45:53<1:39:14,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:04:46,903 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:04:46,903 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 518/1637 [45:58<1:39:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:04:52,211 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:04:52,211 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 519/1637 [46:03<1:38:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:04:57,505 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:04:57,506 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 520/1637 [46:09<1:38:49,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:05:02,816 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:05:02,816 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 521/1637 [46:14<1:38:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:05:08,121 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:05:08,122 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 522/1637 [46:19<1:38:36,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:05:13,425 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:05:13,425 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 523/1637 [46:25<1:38:30,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:05:18,730 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:05:18,730 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 524/1637 [46:30<1:38:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:05:24,037 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:05:24,037 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 525/1637 [46:35<1:38:21,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:05:29,347 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:05:29,347 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 526/1637 [46:41<1:38:21,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:05:34,669 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:05:34,669 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 527/1637 [46:46<1:38:13,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:05:39,973 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:05:39,973 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 528/1637 [46:51<1:38:07,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:05:45,281 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:05:45,281 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 529/1637 [46:56<1:38:03,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:05:50,592 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:05:50,592 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 530/1637 [47:02<1:37:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:05:55,899 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:05:55,900 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 531/1637 [47:07<1:37:46,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:06:01,193 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:06:01,193 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 532/1637 [47:12<1:37:41,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:06:06,498 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:06:06,498 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 533/1637 [47:18<1:37:36,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:06:11,804 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:06:11,804 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 534/1637 [47:23<1:37:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:06:17,118 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:06:17,119 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 535/1637 [47:28<1:37:30,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:06:22,428 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:06:22,429 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 536/1637 [47:34<1:37:19,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:06:27,723 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:06:27,723 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 537/1637 [47:39<1:37:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:06:33,031 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:06:33,031 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 538/1637 [47:44<1:37:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:06:38,336 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:06:38,336 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 539/1637 [47:50<1:37:03,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:06:43,635 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:06:43,636 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 540/1637 [47:55<1:37:00,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:06:48,949 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:06:48,950 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 541/1637 [48:00<1:36:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:06:54,264 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:06:54,265 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 542/1637 [48:05<1:36:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:06:59,586 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:06:59,586 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 543/1637 [48:11<1:36:51,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:07:04,897 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:07:04,898 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 544/1637 [48:16<1:36:46,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:07:10,210 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:07:10,210 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 545/1637 [48:21<1:36:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:07:15,522 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:07:15,523 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 546/1637 [48:27<1:36:36,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:07:20,838 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:07:20,838 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 547/1637 [48:32<1:36:34,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:07:26,160 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:07:26,160 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 548/1637 [48:37<1:36:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:07:31,470 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:07:31,470 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 549/1637 [48:43<1:36:25,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:07:36,795 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:07:36,795 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 550/1637 [48:48<1:36:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:07:42,100 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:07:42,101 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 551/1637 [48:53<1:36:12,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:07:47,420 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:07:47,421 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 552/1637 [48:59<1:36:06,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:07:52,734 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:07:52,734 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 553/1637 [49:04<1:36:01,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:07:58,052 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:07:58,052 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 554/1637 [49:09<1:35:57,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:08:03,368 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:08:03,368 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 555/1637 [49:15<1:35:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:08:08,668 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:08:08,668 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 556/1637 [49:20<1:35:39,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:08:13,974 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:08:13,974 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 557/1637 [49:25<1:35:35,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:08:19,284 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:08:19,285 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 558/1637 [49:30<1:35:28,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:08:24,592 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:08:24,592 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 559/1637 [49:36<1:35:24,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:08:29,905 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:08:29,906 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 560/1637 [49:41<1:35:25,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:08:35,235 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:08:35,236 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 561/1637 [49:46<1:35:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:08:40,537 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:08:40,538 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 562/1637 [49:52<1:35:14,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:08:45,861 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:08:45,861 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 563/1637 [49:57<1:35:07,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:08:51,172 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:08:51,173 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 564/1637 [50:02<1:35:03,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:08:56,490 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:08:56,491 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 565/1637 [50:08<1:34:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:09:01,796 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:09:01,797 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 566/1637 [50:13<1:34:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:09:07,104 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:09:07,105 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 567/1637 [50:18<1:34:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:09:12,416 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:09:12,416 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 568/1637 [50:24<1:34:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:09:17,716 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:09:17,717 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 569/1637 [50:29<1:34:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:09:23,019 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:09:23,020 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 570/1637 [50:34<1:34:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:09:28,332 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:09:28,332 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 571/1637 [50:40<1:34:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:09:33,652 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:09:33,652 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 572/1637 [50:45<1:34:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:09:38,966 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:09:38,966 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 573/1637 [50:50<1:34:07,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:09:44,263 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:09:44,263 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 574/1637 [50:55<1:34:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:09:49,568 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:09:49,568 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 575/1637 [51:01<1:33:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:09:54,874 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:09:54,875 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 576/1637 [51:06<1:33:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:10:00,204 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:10:00,205 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 577/1637 [51:11<1:33:54,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:10:05,523 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:10:05,524 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 578/1637 [51:17<1:33:49,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:10:10,844 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:10:10,845 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 579/1637 [51:22<1:33:43,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:10:16,155 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:10:16,156 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 580/1637 [51:27<1:33:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:10:21,453 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:10:21,454 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 581/1637 [51:33<1:33:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:10:26,762 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:10:26,762 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 582/1637 [51:38<1:33:23,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:10:32,075 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:10:32,076 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 583/1637 [51:43<1:33:19,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:10:37,393 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:10:37,393 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 584/1637 [51:49<1:33:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:10:42,708 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:10:42,708 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 585/1637 [51:54<1:33:09,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:10:48,020 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:10:48,021 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 586/1637 [51:59<1:32:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:10:53,316 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:10:53,316 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 587/1637 [52:05<1:32:53,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:10:58,623 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:10:58,623 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 588/1637 [52:10<1:32:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:11:03,941 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:11:03,941 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 589/1637 [52:15<1:32:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:11:09,257 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:11:09,257 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 590/1637 [52:20<1:32:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:11:14,566 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:11:14,566 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 591/1637 [52:26<1:32:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:11:19,886 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:11:19,886 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 592/1637 [52:31<1:32:36,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:11:25,210 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:11:25,210 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 593/1637 [52:36<1:32:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:11:30,521 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:11:30,521 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 594/1637 [52:42<1:32:25,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:11:35,839 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:11:35,840 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 595/1637 [52:47<1:32:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:11:41,148 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:11:41,148 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 596/1637 [52:52<1:32:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:11:46,449 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:11:46,449 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 597/1637 [52:58<1:32:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:11:51,756 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:11:51,756 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 598/1637 [53:03<1:31:56,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:11:57,065 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:11:57,066 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 599/1637 [53:08<1:31:51,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:12:02,375 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:12:02,375 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 600/1637 [53:14<1:31:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:12:07,693 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:12:07,693 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 601/1637 [53:19<1:31:45,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:12:13,014 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:12:13,014 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 602/1637 [53:24<1:31:39,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:12:18,325 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:12:18,326 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 603/1637 [53:30<1:31:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:12:23,636 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:12:23,637 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 604/1637 [53:35<1:31:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:12:28,949 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:12:28,949 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 605/1637 [53:40<1:31:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:12:34,252 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:12:34,253 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 606/1637 [53:45<1:31:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:12:39,564 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:12:39,565 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 607/1637 [53:51<1:31:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:12:44,879 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:12:44,879 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 608/1637 [53:56<1:31:04,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:12:50,187 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:12:50,188 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 609/1637 [54:01<1:30:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:12:55,494 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:12:55,494 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 610/1637 [54:07<1:30:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:13:00,807 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:13:00,807 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 611/1637 [54:12<1:30:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:13:06,123 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:13:06,123 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 612/1637 [54:17<1:30:42,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:13:11,426 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:13:11,426 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 613/1637 [54:23<1:30:36,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:13:16,736 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:13:16,736 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 614/1637 [54:28<1:30:35,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:13:22,058 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:13:22,058 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 615/1637 [54:33<1:30:32,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:13:27,378 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:13:27,378 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 616/1637 [54:39<1:30:24,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:13:32,685 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:13:32,685 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 617/1637 [54:44<1:30:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:13:38,001 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:13:38,002 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 618/1637 [54:49<1:30:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:13:43,305 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:13:43,305 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 619/1637 [54:54<1:30:03,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:13:48,606 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:13:48,606 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 620/1637 [55:00<1:30:03,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:13:53,930 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:13:53,931 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 621/1637 [55:05<1:29:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:13:59,245 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:13:59,245 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 622/1637 [55:10<1:29:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:14:04,543 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:14:04,544 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 623/1637 [55:16<1:29:46,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:14:09,863 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:14:09,863 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 624/1637 [55:21<1:29:38,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:14:15,167 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:14:15,168 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 625/1637 [55:26<1:29:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:14:20,483 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:14:20,484 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 626/1637 [55:32<1:29:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:14:25,799 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:14:25,799 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 627/1637 [55:37<1:29:25,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:14:31,109 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:14:31,109 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 628/1637 [55:42<1:29:23,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:14:36,435 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:14:36,435 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 629/1637 [55:48<1:29:17,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:14:41,748 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:14:41,748 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 630/1637 [55:53<1:29:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:14:47,059 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:14:47,059 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 631/1637 [55:58<1:29:05,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:14:52,374 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:14:52,374 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 632/1637 [56:04<1:29:05,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:14:57,706 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:14:57,707 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 633/1637 [56:09<1:28:59,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:15:03,019 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:15:03,020 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 634/1637 [56:14<1:28:53,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:15:08,336 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:15:08,336 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 635/1637 [56:20<1:28:45,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:15:13,645 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:15:13,645 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 636/1637 [56:25<1:28:38,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:15:18,954 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:15:18,954 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 637/1637 [56:30<1:28:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:15:24,272 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:15:24,272 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 638/1637 [56:35<1:28:29,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:15:29,586 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:15:29,587 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 639/1637 [56:41<1:28:23,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:15:34,902 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:15:34,903 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 640/1637 [56:46<1:28:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:15:40,205 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:15:40,205 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 641/1637 [56:51<1:28:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:15:45,512 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:15:45,512 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 642/1637 [56:57<1:28:05,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:15:50,828 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:15:50,828 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 643/1637 [57:02<1:28:05,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:15:56,160 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:15:56,160 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 644/1637 [57:07<1:28:01,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:16:01,479 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:16:01,479 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 645/1637 [57:13<1:27:54,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:16:06,793 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:16:06,793 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 646/1637 [57:18<1:27:48,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:16:12,108 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:16:12,108 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 647/1637 [57:23<1:27:44,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:16:17,428 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:16:17,428 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 648/1637 [57:29<1:27:35,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:16:22,735 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:16:22,735 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 649/1637 [57:34<1:27:32,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:16:28,058 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:16:28,058 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 650/1637 [57:39<1:27:30,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:16:33,383 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:16:33,383 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 651/1637 [57:45<1:27:21,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:16:38,692 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:16:38,693 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 652/1637 [57:50<1:27:20,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:16:44,020 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:16:44,021 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 653/1637 [57:55<1:27:13,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:16:49,335 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:16:49,335 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 654/1637 [58:01<1:27:10,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:16:54,664 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:16:54,665 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 655/1637 [58:06<1:27:02,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:16:59,973 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:16:59,974 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 656/1637 [58:11<1:26:55,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:17:05,285 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:17:05,286 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 657/1637 [58:16<1:26:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:17:10,596 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:17:10,596 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 658/1637 [58:22<1:26:44,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:17:15,917 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:17:15,918 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 659/1637 [58:27<1:26:43,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:17:21,247 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:17:21,248 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 660/1637 [58:32<1:26:35,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:17:26,557 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:17:26,557 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 661/1637 [58:38<1:26:25,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:17:31,859 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:17:31,860 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 662/1637 [58:43<1:26:21,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:17:37,178 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:17:37,179 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 663/1637 [58:48<1:26:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:17:42,494 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:17:42,494 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 664/1637 [58:54<1:26:09,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:17:47,802 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:17:47,803 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 665/1637 [58:59<1:26:05,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:17:53,119 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:17:53,120 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 666/1637 [59:04<1:25:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:17:58,430 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:17:58,431 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 667/1637 [59:10<1:25:58,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:18:03,759 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:18:03,759 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 668/1637 [59:15<1:25:51,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:18:09,071 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:18:09,072 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 669/1637 [59:20<1:25:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:18:14,378 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:18:14,379 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 670/1637 [59:26<1:25:42,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:18:19,707 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:18:19,707 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 671/1637 [59:31<1:25:39,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:18:25,034 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:18:25,034 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 672/1637 [59:36<1:25:33,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:18:30,350 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:18:30,350 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 673/1637 [59:42<1:25:26,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:18:35,664 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:18:35,665 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 674/1637 [59:47<1:25:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:18:40,971 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:18:40,971 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 675/1637 [59:52<1:25:12,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:18:46,284 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:18:46,285 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 676/1637 [59:57<1:25:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:18:51,582 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:18:51,582 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 677/1637 [1:00:03<1:24:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:18:56,893 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:18:56,893 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 678/1637 [1:00:08<1:24:49,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:19:02,195 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:19:02,195 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 679/1637 [1:00:13<1:24:45,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:19:07,505 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:19:07,505 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 680/1637 [1:00:19<1:24:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:19:12,817 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:19:12,817 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 681/1637 [1:00:24<1:24:36,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:19:18,129 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:19:18,130 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 682/1637 [1:00:29<1:24:28,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:19:23,430 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:19:23,430 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 683/1637 [1:00:35<1:24:24,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:19:28,740 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:19:28,740 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 684/1637 [1:00:40<1:24:14,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:19:34,032 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:19:34,032 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 685/1637 [1:00:45<1:24:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:19:39,356 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:19:39,357 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 686/1637 [1:00:51<1:24:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:19:44,673 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:19:44,673 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 687/1637 [1:00:56<1:24:09,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:19:49,995 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:19:49,996 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 688/1637 [1:01:01<1:24:03,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:19:55,309 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:19:55,310 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 689/1637 [1:01:07<1:23:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:20:00,621 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:20:00,621 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 690/1637 [1:01:12<1:23:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:20:05,918 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:20:05,918 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 691/1637 [1:01:17<1:23:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:20:11,230 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:20:11,231 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 692/1637 [1:01:22<1:23:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:20:16,545 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:20:16,546 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 693/1637 [1:01:28<1:23:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:20:21,857 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:20:21,857 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 694/1637 [1:01:33<1:23:28,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:20:27,165 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:20:27,165 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 695/1637 [1:01:38<1:23:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:20:32,475 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:20:32,476 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 696/1637 [1:01:44<1:23:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:20:37,788 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:20:37,788 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 697/1637 [1:01:49<1:23:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:20:43,105 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:20:43,106 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 698/1637 [1:01:54<1:23:09,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:20:48,421 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:20:48,421 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 699/1637 [1:02:00<1:23:06,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:20:53,742 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:20:53,742 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 700/1637 [1:02:05<1:22:56,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:20:59,042 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:20:59,042 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 701/1637 [1:02:10<1:22:51,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:21:04,355 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:21:04,355 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 702/1637 [1:02:16<1:22:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:21:09,658 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:21:09,659 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 703/1637 [1:02:21<1:22:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:21:14,977 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:21:14,977 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 704/1637 [1:02:26<1:22:35,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:21:20,286 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:21:20,286 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 705/1637 [1:02:31<1:22:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:21:25,600 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:21:25,601 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 706/1637 [1:02:37<1:22:25,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:21:30,913 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:21:30,913 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 707/1637 [1:02:42<1:22:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:21:36,227 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:21:36,227 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 708/1637 [1:02:47<1:22:13,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:21:41,533 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:21:41,533 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 709/1637 [1:02:53<1:22:12,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:21:46,859 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:21:46,860 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 710/1637 [1:02:58<1:22:07,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:21:52,175 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:21:52,176 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 711/1637 [1:03:03<1:21:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:21:57,476 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:21:57,477 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 712/1637 [1:03:09<1:21:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:22:02,784 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:22:02,784 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 713/1637 [1:03:14<1:21:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:22:08,082 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:22:08,082 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 714/1637 [1:03:19<1:21:39,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:22:13,392 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:22:13,392 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 715/1637 [1:03:25<1:21:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:22:18,691 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:22:18,691 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 716/1637 [1:03:30<1:21:24,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:22:23,990 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:22:23,990 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 717/1637 [1:03:35<1:21:19,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:22:29,296 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:22:29,297 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 718/1637 [1:03:40<1:21:15,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:22:34,603 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:22:34,603 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 719/1637 [1:03:46<1:21:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:22:39,925 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:22:39,926 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 720/1637 [1:03:51<1:21:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:22:45,238 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:22:45,238 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 721/1637 [1:03:56<1:21:04,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:22:50,549 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:22:50,549 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 722/1637 [1:04:02<1:21:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:22:55,869 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:22:55,870 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 723/1637 [1:04:07<1:20:56,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:23:01,184 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:23:01,184 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 724/1637 [1:04:12<1:20:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:23:06,499 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:23:06,500 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 725/1637 [1:04:18<1:20:42,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:23:11,798 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:23:11,798 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 726/1637 [1:04:23<1:20:32,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:23:17,089 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:23:17,090 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 727/1637 [1:04:28<1:20:32,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:23:22,415 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:23:22,416 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 728/1637 [1:04:34<1:20:32,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:23:27,744 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:23:27,745 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 729/1637 [1:04:39<1:20:28,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:23:33,064 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:23:33,065 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 730/1637 [1:04:44<1:20:22,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:23:38,379 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:23:38,379 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 731/1637 [1:04:50<1:20:17,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:23:43,696 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:23:43,697 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 732/1637 [1:04:55<1:20:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:23:48,993 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:23:48,993 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 733/1637 [1:05:00<1:19:56,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:23:54,287 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:23:54,287 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 734/1637 [1:05:05<1:19:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:23:59,619 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:23:59,619 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 735/1637 [1:05:11<1:19:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:24:04,921 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:24:04,921 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 736/1637 [1:05:16<1:19:49,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:24:10,249 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:24:10,249 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 737/1637 [1:05:21<1:19:46,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:24:15,575 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:24:15,575 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 738/1637 [1:05:27<1:19:40,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:24:20,890 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:24:20,891 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 739/1637 [1:05:32<1:19:37,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:24:26,220 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:24:26,220 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 740/1637 [1:05:37<1:19:30,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:24:31,529 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:24:31,529 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 741/1637 [1:05:43<1:19:25,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:24:36,849 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:24:36,849 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 742/1637 [1:05:48<1:19:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:24:42,150 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:24:42,150 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 743/1637 [1:05:53<1:19:16,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:24:47,487 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:24:47,487 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 744/1637 [1:05:59<1:19:08,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:24:52,795 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:24:52,796 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 745/1637 [1:06:04<1:19:00,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:24:58,104 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:24:58,104 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 746/1637 [1:06:09<1:18:56,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:25:03,423 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:25:03,423 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 747/1637 [1:06:15<1:18:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:25:08,735 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:25:08,735 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 748/1637 [1:06:20<1:18:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:25:14,045 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:25:14,045 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 749/1637 [1:06:25<1:18:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:25:19,354 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:25:19,354 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 750/1637 [1:06:31<1:18:38,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:25:24,693 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:25:24,693 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 751/1637 [1:06:36<1:18:32,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:25:30,011 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:25:30,011 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 752/1637 [1:06:41<1:18:29,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:25:35,337 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:25:35,337 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 753/1637 [1:06:47<1:18:19,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:25:40,641 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:25:40,641 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 754/1637 [1:06:52<1:18:15,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:25:45,965 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:25:45,965 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 755/1637 [1:06:57<1:18:09,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:25:51,275 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:25:51,276 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 756/1637 [1:07:02<1:17:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:25:56,572 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:25:56,573 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 757/1637 [1:07:08<1:17:58,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:26:01,903 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:26:01,904 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 758/1637 [1:07:13<1:17:52,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:26:07,214 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:26:07,215 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 759/1637 [1:07:18<1:17:42,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:26:12,512 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:26:12,512 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 760/1637 [1:07:24<1:17:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:26:17,815 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:26:17,815 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 761/1637 [1:07:29<1:17:26,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:26:23,112 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:26:23,112 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 762/1637 [1:07:34<1:17:24,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:26:28,429 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:26:28,430 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 763/1637 [1:07:40<1:17:16,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:26:33,726 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:26:33,726 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 764/1637 [1:07:45<1:17:09,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:26:39,026 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:26:39,027 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 765/1637 [1:07:50<1:17:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:26:44,336 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:26:44,336 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 766/1637 [1:07:56<1:17:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:26:49,642 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:26:49,642 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 767/1637 [1:08:01<1:16:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:26:54,953 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:26:54,954 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 768/1637 [1:08:06<1:16:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:27:00,260 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:27:00,261 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 769/1637 [1:08:11<1:16:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:27:05,582 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:27:05,582 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 770/1637 [1:08:17<1:16:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:27:10,890 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:27:10,890 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 771/1637 [1:08:22<1:16:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:27:16,193 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:27:16,193 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 772/1637 [1:08:27<1:16:29,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:27:21,493 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:27:21,494 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 773/1637 [1:08:33<1:16:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:27:26,811 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:27:26,811 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 774/1637 [1:08:38<1:16:21,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:27:32,120 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:27:32,120 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 775/1637 [1:08:43<1:16:19,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:27:37,443 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:27:37,443 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 776/1637 [1:08:49<1:16:09,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:27:42,735 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:27:42,735 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 777/1637 [1:08:54<1:16:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:27:48,058 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:27:48,058 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 778/1637 [1:08:59<1:15:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:27:53,357 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:27:53,357 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 779/1637 [1:09:05<1:15:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:27:58,666 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:27:58,666 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 780/1637 [1:09:10<1:15:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:28:03,970 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:28:03,971 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 781/1637 [1:09:15<1:15:45,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:28:09,287 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:28:09,288 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 782/1637 [1:09:20<1:15:38,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:28:14,589 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:28:14,590 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 783/1637 [1:09:26<1:15:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:28:19,899 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:28:19,899 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 784/1637 [1:09:31<1:15:28,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:28:25,208 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:28:25,208 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 785/1637 [1:09:36<1:15:23,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:28:30,518 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:28:30,518 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 786/1637 [1:09:42<1:15:18,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:28:35,833 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:28:35,833 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 787/1637 [1:09:47<1:15:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:28:41,154 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:28:41,154 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 788/1637 [1:09:52<1:15:16,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:28:46,488 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:28:46,489 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 789/1637 [1:09:58<1:15:08,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:28:51,797 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:28:51,797 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 790/1637 [1:10:03<1:15:00,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:28:57,106 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:28:57,107 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 791/1637 [1:10:08<1:14:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:29:02,415 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:29:02,415 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 792/1637 [1:10:14<1:14:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:29:07,721 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:29:07,721 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 793/1637 [1:10:19<1:14:42,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:29:13,031 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:29:13,031 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 794/1637 [1:10:24<1:14:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:29:18,343 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:29:18,343 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 795/1637 [1:10:30<1:14:32,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:29:23,658 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:29:23,659 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 796/1637 [1:10:35<1:14:28,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:29:28,975 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:29:28,976 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 797/1637 [1:10:40<1:14:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:29:34,286 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:29:34,286 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 798/1637 [1:10:46<1:14:22,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:29:39,620 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:29:39,620 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 799/1637 [1:10:51<1:14:14,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:29:44,927 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:29:44,928 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 800/1637 [1:10:56<1:14:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:29:50,238 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:29:50,238 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 801/1637 [1:11:01<1:14:07,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:29:55,572 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:29:55,572 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 802/1637 [1:11:07<1:13:58,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:30:00,878 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:30:00,879 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 803/1637 [1:11:12<1:13:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:30:06,190 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:30:06,190 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 804/1637 [1:11:17<1:13:49,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:30:11,515 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:30:11,516 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 805/1637 [1:11:23<1:13:45,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:30:16,838 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:30:16,838 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 806/1637 [1:11:28<1:13:40,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:30:22,161 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:30:22,161 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 807/1637 [1:11:33<1:13:29,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:30:27,456 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:30:27,456 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 808/1637 [1:11:39<1:13:26,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:30:32,785 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:30:32,786 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 809/1637 [1:11:44<1:13:24,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:30:38,107 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:30:38,107 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 810/1637 [1:11:49<1:13:21,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:30:43,437 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:30:43,437 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 811/1637 [1:11:55<1:13:17,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:30:48,764 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:30:48,764 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 812/1637 [1:12:00<1:13:12,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:30:54,087 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:30:54,087 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 813/1637 [1:12:05<1:13:00,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:30:59,385 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:30:59,386 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 814/1637 [1:12:11<1:12:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:31:04,673 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:31:04,674 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 815/1637 [1:12:16<1:12:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:31:09,976 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:31:09,976 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 816/1637 [1:12:21<1:12:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:31:15,285 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:31:15,285 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 817/1637 [1:12:26<1:12:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:31:20,589 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:31:20,589 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 818/1637 [1:12:32<1:12:24,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:31:25,892 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:31:25,892 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 819/1637 [1:12:37<1:12:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:31:31,200 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:31:31,201 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 820/1637 [1:12:42<1:12:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:31:36,504 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:31:36,505 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 821/1637 [1:12:48<1:12:13,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:31:41,825 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:31:41,825 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 822/1637 [1:12:53<1:12:12,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:31:47,159 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:31:47,159 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 823/1637 [1:12:58<1:12:05,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:31:52,469 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:31:52,470 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 824/1637 [1:13:04<1:11:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:31:57,768 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:31:57,768 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 825/1637 [1:13:09<1:11:53,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:32:03,086 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:32:03,086 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 826/1637 [1:13:14<1:11:51,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:32:08,410 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:32:08,410 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 827/1637 [1:13:20<1:11:45,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:32:13,722 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:32:13,722 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 828/1637 [1:13:25<1:11:39,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:32:19,034 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:32:19,034 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 829/1637 [1:13:30<1:11:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:32:24,346 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:32:24,347 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 830/1637 [1:13:36<1:11:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:32:29,657 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:32:29,657 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 831/1637 [1:13:41<1:11:23,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:32:34,977 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:32:34,978 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 832/1637 [1:13:46<1:11:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:32:40,288 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:32:40,288 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 833/1637 [1:13:51<1:11:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:32:45,596 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:32:45,596 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 834/1637 [1:13:57<1:11:04,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:32:50,905 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:32:50,905 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 835/1637 [1:14:02<1:10:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:32:56,215 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:32:56,215 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 836/1637 [1:14:07<1:10:51,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:33:01,514 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:33:01,514 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 837/1637 [1:14:13<1:10:43,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:33:06,812 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:33:06,813 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 838/1637 [1:14:18<1:10:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:33:12,144 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:33:12,145 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 839/1637 [1:14:23<1:10:35,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:33:17,441 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:33:17,441 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 840/1637 [1:14:29<1:10:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:33:22,755 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:33:22,755 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 841/1637 [1:14:34<1:10:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:33:28,071 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:33:28,071 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 842/1637 [1:14:39<1:10:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:33:33,381 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:33:33,381 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 843/1637 [1:14:45<1:10:19,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:33:38,702 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:33:38,703 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 844/1637 [1:14:50<1:10:09,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:33:43,997 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:33:43,998 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 845/1637 [1:14:55<1:10:04,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:33:49,304 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:33:49,305 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 846/1637 [1:15:01<1:10:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:33:54,622 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:33:54,623 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 847/1637 [1:15:06<1:09:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:33:59,928 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:33:59,928 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 848/1637 [1:15:11<1:09:49,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:34:05,239 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:34:05,239 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 849/1637 [1:15:16<1:09:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:34:10,549 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:34:10,550 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 850/1637 [1:15:22<1:09:39,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:34:15,863 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:34:15,863 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 851/1637 [1:15:27<1:09:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:34:21,171 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:34:21,171 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 852/1637 [1:15:32<1:09:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:34:26,478 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:34:26,478 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 853/1637 [1:15:38<1:09:21,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:34:31,782 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:34:31,782 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 854/1637 [1:15:43<1:09:18,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:34:37,098 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:34:37,099 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 855/1637 [1:15:48<1:09:12,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:34:42,408 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:34:42,409 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 856/1637 [1:15:54<1:09:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:34:47,722 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:34:47,722 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 857/1637 [1:15:59<1:09:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:34:53,034 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:34:53,034 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 858/1637 [1:16:04<1:08:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:34:58,346 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:34:58,346 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 859/1637 [1:16:10<1:08:56,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:35:03,673 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:35:03,674 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 860/1637 [1:16:15<1:08:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:35:08,975 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:35:08,975 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 861/1637 [1:16:20<1:08:40,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:35:14,278 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:35:14,278 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 862/1637 [1:16:25<1:08:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:35:19,581 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:35:19,582 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 863/1637 [1:16:31<1:08:28,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:35:24,891 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:35:24,891 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 864/1637 [1:16:36<1:08:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:35:30,196 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:35:30,197 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 865/1637 [1:16:41<1:08:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:35:35,520 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:35:35,520 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 866/1637 [1:16:47<1:08:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:35:40,827 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:35:40,827 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 867/1637 [1:16:52<1:08:07,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:35:46,132 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:35:46,132 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 868/1637 [1:16:57<1:08:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:35:51,437 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:35:51,437 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 869/1637 [1:17:03<1:07:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:35:56,740 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:35:56,740 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 870/1637 [1:17:08<1:07:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:36:02,050 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:36:02,051 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 871/1637 [1:17:13<1:07:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:36:07,358 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:36:07,358 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 872/1637 [1:17:19<1:07:39,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:36:12,659 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:36:12,659 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 873/1637 [1:17:24<1:07:32,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:36:17,957 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:36:17,958 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 874/1637 [1:17:29<1:07:30,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:36:23,277 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:36:23,277 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 875/1637 [1:17:34<1:07:23,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:36:28,578 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:36:28,579 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 876/1637 [1:17:40<1:07:23,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:36:33,911 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:36:33,911 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 877/1637 [1:17:45<1:07:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:36:39,216 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:36:39,216 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 878/1637 [1:17:50<1:07:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:36:44,522 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:36:44,523 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 879/1637 [1:17:56<1:07:04,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:36:49,830 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:36:49,831 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 880/1637 [1:18:01<1:06:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:36:55,140 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:36:55,140 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 881/1637 [1:18:06<1:06:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:37:00,452 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:37:00,453 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 882/1637 [1:18:12<1:06:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:37:05,765 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:37:05,765 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 883/1637 [1:18:17<1:06:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:37:11,073 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:37:11,073 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 884/1637 [1:18:22<1:06:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:37:16,379 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:37:16,379 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 885/1637 [1:18:28<1:06:37,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:37:21,712 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:37:21,712 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 886/1637 [1:18:33<1:06:35,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:37:27,042 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:37:27,043 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 887/1637 [1:18:38<1:06:27,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:37:32,347 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:37:32,348 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 888/1637 [1:18:44<1:06:22,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:37:37,667 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:37:37,667 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 889/1637 [1:18:49<1:06:16,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:37:42,981 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:37:42,982 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 890/1637 [1:18:54<1:06:11,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:37:48,299 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:37:48,300 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 891/1637 [1:18:59<1:06:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:37:53,596 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:37:53,596 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 892/1637 [1:19:05<1:05:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:37:58,901 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:37:58,901 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 893/1637 [1:19:10<1:05:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:38:04,226 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:38:04,226 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 894/1637 [1:19:15<1:05:50,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:38:09,548 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:38:09,548 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 895/1637 [1:19:21<1:05:46,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:38:14,873 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:38:14,873 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 896/1637 [1:19:26<1:05:44,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:38:20,207 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:38:20,208 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 897/1637 [1:19:31<1:05:38,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:38:25,525 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:38:25,525 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 898/1637 [1:19:37<1:05:30,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:38:30,834 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:38:30,835 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 899/1637 [1:19:42<1:05:21,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:38:36,141 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:38:36,142 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 900/1637 [1:19:47<1:05:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:38:41,445 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:38:41,446 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 901/1637 [1:19:53<1:05:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:38:46,763 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:38:46,763 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 902/1637 [1:19:58<1:05:07,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:38:52,087 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:38:52,087 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 903/1637 [1:20:03<1:05:02,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:38:57,403 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:38:57,404 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 904/1637 [1:20:09<1:04:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:39:02,713 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:39:02,713 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 905/1637 [1:20:14<1:04:52,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:39:08,037 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:39:08,038 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 906/1637 [1:20:19<1:04:46,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:39:13,352 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:39:13,353 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 907/1637 [1:20:25<1:04:40,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:39:18,666 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:39:18,666 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 908/1637 [1:20:30<1:04:34,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:39:23,980 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:39:23,980 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 909/1637 [1:20:35<1:04:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:39:29,282 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:39:29,282 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 910/1637 [1:20:40<1:04:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:39:34,587 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:39:34,588 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 911/1637 [1:20:46<1:04:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:39:39,897 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:39:39,897 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 912/1637 [1:20:51<1:04:09,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:39:45,208 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:39:45,208 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 913/1637 [1:20:56<1:04:03,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:39:50,512 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:39:50,512 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 914/1637 [1:21:02<1:03:54,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:39:55,806 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:39:55,807 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 915/1637 [1:21:07<1:03:48,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:40:01,109 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:40:01,109 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 916/1637 [1:21:12<1:03:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:40:06,432 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:40:06,432 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 917/1637 [1:21:18<1:03:47,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:40:11,764 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:40:11,764 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 918/1637 [1:21:23<1:03:40,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:40:17,069 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:40:17,070 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 919/1637 [1:21:28<1:03:32,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:40:22,371 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:40:22,371 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 920/1637 [1:21:34<1:03:28,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:40:27,688 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:40:27,688 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 921/1637 [1:21:39<1:03:23,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:40:33,003 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:40:33,003 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 922/1637 [1:21:44<1:03:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:40:38,294 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:40:38,295 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 923/1637 [1:21:50<1:03:13,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:40:43,625 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:40:43,625 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 924/1637 [1:21:55<1:03:10,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:40:48,946 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:40:48,946 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 925/1637 [1:22:00<1:03:08,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:40:54,280 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:40:54,281 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 926/1637 [1:22:05<1:03:03,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:40:59,601 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:40:59,602 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 927/1637 [1:22:11<1:02:54,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:41:04,905 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:41:04,906 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 928/1637 [1:22:16<1:02:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:41:10,213 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:41:10,214 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 929/1637 [1:22:21<1:02:40,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:41:15,519 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:41:15,519 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 930/1637 [1:22:27<1:02:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:41:20,823 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:41:20,823 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 931/1637 [1:22:32<1:02:29,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:41:26,143 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:41:26,143 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 932/1637 [1:22:37<1:02:25,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:41:31,453 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:41:31,454 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 933/1637 [1:22:43<1:02:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:41:36,752 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:41:36,753 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 934/1637 [1:22:48<1:02:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:41:42,054 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:41:42,055 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 935/1637 [1:22:53<1:02:05,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:41:47,364 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:41:47,364 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 936/1637 [1:22:59<1:01:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:41:52,667 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:41:52,667 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 937/1637 [1:23:04<1:01:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:41:57,993 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:41:57,994 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 938/1637 [1:23:09<1:01:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:42:03,311 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:42:03,312 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 939/1637 [1:23:15<1:01:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:42:08,620 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:42:08,621 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 940/1637 [1:23:20<1:01:42,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:42:13,931 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:42:13,931 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 941/1637 [1:23:25<1:01:39,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:42:19,255 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:42:19,256 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 942/1637 [1:23:30<1:01:34,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:42:24,573 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:42:24,574 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 943/1637 [1:23:36<1:01:29,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:42:29,888 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:42:29,888 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 944/1637 [1:23:41<1:01:23,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:42:35,204 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:42:35,204 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 945/1637 [1:23:46<1:01:18,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:42:40,520 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:42:40,521 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 946/1637 [1:23:52<1:01:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:42:45,828 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:42:45,829 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 947/1637 [1:23:57<1:01:04,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:42:51,133 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:42:51,133 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 948/1637 [1:24:02<1:00:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:42:56,440 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:42:56,441 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 949/1637 [1:24:08<1:00:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:43:01,737 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:43:01,738 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 950/1637 [1:24:13<1:00:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:43:07,070 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:43:07,071 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 951/1637 [1:24:18<1:00:45,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:43:12,383 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:43:12,384 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 952/1637 [1:24:24<1:00:39,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:43:17,693 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:43:17,694 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 953/1637 [1:24:29<1:00:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:43:23,004 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:43:23,005 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 954/1637 [1:24:34<1:00:30,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:43:28,327 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:43:28,327 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 955/1637 [1:24:40<1:00:23,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:43:33,634 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:43:33,635 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 956/1637 [1:24:45<1:00:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:43:38,942 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:43:38,942 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 957/1637 [1:24:50<1:00:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:43:44,239 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:43:44,240 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 958/1637 [1:24:55<1:00:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:43:49,540 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:43:49,540 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 959/1637 [1:25:01<59:58,  5.31s/it]  [INFO|modeling_longformer.py:1853] 2021-10-03 20:43:54,851 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:43:54,852 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 960/1637 [1:25:06<59:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:44:00,165 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:44:00,165 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 961/1637 [1:25:11<59:49,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:44:05,477 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:44:05,477 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 962/1637 [1:25:17<59:45,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:44:10,794 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:44:10,794 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 963/1637 [1:25:22<59:39,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:44:16,100 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:44:16,101 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 964/1637 [1:25:27<59:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:44:21,412 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:44:21,412 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 965/1637 [1:25:33<59:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:44:26,717 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:44:26,717 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 966/1637 [1:25:38<59:21,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:44:32,023 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:44:32,023 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 967/1637 [1:25:43<59:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:44:37,335 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:44:37,336 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 968/1637 [1:25:49<59:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:44:42,643 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:44:42,643 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 969/1637 [1:25:54<59:11,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:44:47,979 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:44:47,980 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 970/1637 [1:25:59<59:07,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:44:53,301 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:44:53,301 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 971/1637 [1:26:04<59:00,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:44:58,612 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:44:58,612 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 972/1637 [1:26:10<58:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:45:03,914 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:45:03,914 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 973/1637 [1:26:15<58:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:45:09,227 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:45:09,227 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 974/1637 [1:26:20<58:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:45:14,534 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:45:14,534 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 975/1637 [1:26:26<58:35,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:45:19,841 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:45:19,842 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 976/1637 [1:26:31<58:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:45:25,160 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:45:25,160 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 977/1637 [1:26:36<58:25,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:45:30,471 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:45:30,471 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 978/1637 [1:26:42<58:21,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:45:35,787 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:45:35,787 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 979/1637 [1:26:47<58:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:45:41,100 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:45:41,100 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 980/1637 [1:26:52<58:12,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:45:46,422 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:45:46,423 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 981/1637 [1:26:58<58:04,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:45:51,723 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:45:51,723 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 982/1637 [1:27:03<57:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:45:57,032 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:45:57,032 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 983/1637 [1:27:08<57:51,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:46:02,334 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:46:02,334 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 984/1637 [1:27:14<57:46,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:46:07,643 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:46:07,643 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 985/1637 [1:27:19<57:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:46:12,964 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:46:12,964 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 986/1637 [1:27:24<57:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:46:18,257 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:46:18,258 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 987/1637 [1:27:29<57:32,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:46:23,580 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:46:23,581 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 988/1637 [1:27:35<57:30,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:46:28,911 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:46:28,912 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 989/1637 [1:27:40<57:24,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:46:34,222 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:46:34,222 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 990/1637 [1:27:45<57:18,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:46:39,535 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:46:39,535 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 991/1637 [1:27:51<57:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:46:44,838 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:46:44,839 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 992/1637 [1:27:56<57:04,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:46:50,143 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:46:50,144 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 993/1637 [1:28:01<56:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:46:55,452 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:46:55,452 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 994/1637 [1:28:07<56:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:47:00,754 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:47:00,754 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 995/1637 [1:28:12<56:49,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:47:06,072 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:47:06,073 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 996/1637 [1:28:17<56:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:47:11,382 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:47:11,383 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 997/1637 [1:28:23<56:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:47:16,708 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:47:16,708 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 998/1637 [1:28:28<56:35,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:47:22,021 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:47:22,022 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 999/1637 [1:28:33<56:29,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:47:27,333 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:47:27,333 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "{'loss': 0.3008, 'learning_rate': 7.782529016493586e-06, 'epoch': 0.61}\n",
            " 61% 1000/1637 [1:28:39<56:22,  5.31s/it][INFO|trainer.py:1987] 2021-10-03 20:47:32,627 >> Saving model checkpoint to /tmp/qnli/checkpoint-1000\n",
            "[INFO|configuration_utils.py:413] 2021-10-03 20:47:32,628 >> Configuration saved in /tmp/qnli/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-03 20:47:34,399 >> Model weights saved in /tmp/qnli/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-03 20:47:34,399 >> tokenizer config file saved in /tmp/qnli/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-03 20:47:34,400 >> Special tokens file saved in /tmp/qnli/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|modeling_longformer.py:1853] 2021-10-03 20:47:38,957 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:47:38,957 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 1001/1637 [1:28:50<1:16:24,  7.21s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:47:44,273 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:47:44,274 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 1002/1637 [1:28:55<1:10:14,  6.64s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:47:49,577 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:47:49,577 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 1003/1637 [1:29:01<1:05:56,  6.24s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:47:54,890 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:47:54,890 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 1004/1637 [1:29:06<1:02:50,  5.96s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:48:00,188 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:48:00,188 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 1005/1637 [1:29:11<1:00:42,  5.76s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:48:05,502 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:48:05,502 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 1006/1637 [1:29:17<59:11,  5.63s/it]  [INFO|modeling_longformer.py:1853] 2021-10-03 20:48:10,814 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:48:10,815 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 1007/1637 [1:29:22<58:04,  5.53s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:48:16,116 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:48:16,117 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 1008/1637 [1:29:27<57:16,  5.46s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:48:21,423 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:48:21,423 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 1009/1637 [1:29:33<56:41,  5.42s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:48:26,730 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:48:26,730 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 1010/1637 [1:29:38<56:12,  5.38s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:48:32,020 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:48:32,021 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 1011/1637 [1:29:43<55:56,  5.36s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:48:37,345 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:48:37,345 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 1012/1637 [1:29:49<55:43,  5.35s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:48:42,662 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:48:42,663 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 1013/1637 [1:29:54<55:30,  5.34s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:48:47,972 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:48:47,972 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 1014/1637 [1:29:59<55:18,  5.33s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:48:53,273 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:48:53,273 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 1015/1637 [1:30:04<55:09,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:48:58,580 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:48:58,580 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 1016/1637 [1:30:10<55:03,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:49:03,895 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:49:03,895 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 1017/1637 [1:30:15<54:59,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:49:09,223 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:49:09,223 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 1018/1637 [1:30:20<54:52,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:49:14,538 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:49:14,538 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 1019/1637 [1:30:26<54:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:49:19,839 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:49:19,839 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 1020/1637 [1:30:31<54:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:49:25,145 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:49:25,146 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 1021/1637 [1:30:36<54:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:49:30,453 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:49:30,454 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 1022/1637 [1:30:42<54:28,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:49:35,781 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:49:35,781 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 1023/1637 [1:30:47<54:24,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:49:41,099 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:49:41,099 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 1024/1637 [1:30:52<54:20,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:49:46,421 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:49:46,421 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 1025/1637 [1:30:58<54:16,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:49:51,749 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:49:51,749 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 1026/1637 [1:31:03<54:10,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:49:57,066 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:49:57,066 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 1027/1637 [1:31:08<54:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:50:02,365 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:50:02,365 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 1028/1637 [1:31:14<53:56,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:50:07,680 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:50:07,681 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 1029/1637 [1:31:19<53:49,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:50:12,989 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:50:12,989 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 1030/1637 [1:31:24<53:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:50:18,298 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:50:18,298 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 1031/1637 [1:31:29<53:38,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:50:23,608 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:50:23,609 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 1032/1637 [1:31:35<53:29,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:50:28,900 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:50:28,901 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 1033/1637 [1:31:40<53:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:50:34,215 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:50:34,215 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 1034/1637 [1:31:45<53:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:50:39,523 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:50:39,523 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 1035/1637 [1:31:51<53:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:50:44,841 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:50:44,841 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 1036/1637 [1:31:56<53:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:50:50,150 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:50:50,150 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 1037/1637 [1:32:01<53:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:50:55,474 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:50:55,474 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 1038/1637 [1:32:07<53:03,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:51:00,793 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:51:00,793 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 1039/1637 [1:32:12<52:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:51:06,100 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:51:06,101 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 1040/1637 [1:32:17<52:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:51:11,406 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:51:11,407 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 1041/1637 [1:32:23<52:46,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:51:16,721 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:51:16,722 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 1042/1637 [1:32:28<52:40,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:51:22,033 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:51:22,033 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 1043/1637 [1:32:33<52:35,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:51:27,346 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:51:27,346 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 1044/1637 [1:32:39<52:29,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:51:32,655 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:51:32,656 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 1045/1637 [1:32:44<52:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:51:37,957 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:51:37,957 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 1046/1637 [1:32:49<52:18,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:51:43,270 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:51:43,271 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 1047/1637 [1:32:54<52:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:51:48,574 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:51:48,574 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 1048/1637 [1:33:00<52:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:51:53,885 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:51:53,886 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 1049/1637 [1:33:05<52:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:51:59,201 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:51:59,202 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 1050/1637 [1:33:10<51:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:52:04,516 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:52:04,517 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 1051/1637 [1:33:16<51:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:52:09,826 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:52:09,826 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 1052/1637 [1:33:21<51:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:52:15,146 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:52:15,147 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 1053/1637 [1:33:26<51:46,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:52:20,477 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:52:20,478 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 1054/1637 [1:33:32<51:40,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:52:25,794 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:52:25,794 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 1055/1637 [1:33:37<51:34,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:52:31,109 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:52:31,110 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 1056/1637 [1:33:42<51:30,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:52:36,433 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:52:36,434 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 1057/1637 [1:33:48<51:25,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:52:41,755 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:52:41,755 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 1058/1637 [1:33:53<51:20,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:52:47,078 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:52:47,079 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 1059/1637 [1:33:58<51:12,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:52:52,383 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:52:52,383 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 1060/1637 [1:34:04<51:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:52:57,693 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:52:57,693 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 1061/1637 [1:34:09<51:01,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:53:03,013 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:53:03,013 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 1062/1637 [1:34:14<50:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:53:08,322 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:53:08,322 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 1063/1637 [1:34:20<50:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:53:13,628 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:53:13,628 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 1064/1637 [1:34:25<50:40,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:53:18,920 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:53:18,920 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 1065/1637 [1:34:30<50:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:53:24,224 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:53:24,225 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 1066/1637 [1:34:35<50:28,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:53:29,526 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:53:29,526 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 1067/1637 [1:34:41<50:24,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:53:34,833 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:53:34,834 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 1068/1637 [1:34:46<50:18,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:53:40,140 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:53:40,140 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 1069/1637 [1:34:51<50:13,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:53:45,446 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:53:45,446 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 1070/1637 [1:34:57<50:05,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:53:50,732 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:53:50,732 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 1071/1637 [1:35:02<49:59,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:53:56,029 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:53:56,030 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 1072/1637 [1:35:07<49:54,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:54:01,335 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:54:01,336 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 1073/1637 [1:35:13<49:51,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:54:06,643 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:54:06,644 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 1074/1637 [1:35:18<49:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:54:11,966 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:54:11,966 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 1075/1637 [1:35:23<49:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:54:17,277 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:54:17,277 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 1076/1637 [1:35:28<49:36,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:54:22,572 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:54:22,573 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 1077/1637 [1:35:34<49:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:54:27,881 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:54:27,881 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 1078/1637 [1:35:39<49:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:54:33,191 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:54:33,191 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 1079/1637 [1:35:44<49:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:54:38,507 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:54:38,508 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 1080/1637 [1:35:50<49:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:54:43,818 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:54:43,819 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 1081/1637 [1:35:55<49:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:54:49,142 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:54:49,143 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 1082/1637 [1:36:00<49:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:54:54,441 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:54:54,441 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 1083/1637 [1:36:06<49:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:54:59,755 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:54:59,756 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 1084/1637 [1:36:11<48:56,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:55:05,064 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:55:05,064 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 1085/1637 [1:36:16<48:53,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:55:10,386 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:55:10,386 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 1086/1637 [1:36:22<48:48,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:55:15,704 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:55:15,705 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 1087/1637 [1:36:27<48:42,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:55:21,015 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:55:21,015 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 1088/1637 [1:36:32<48:36,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:55:26,322 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:55:26,322 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 1089/1637 [1:36:38<48:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:55:31,636 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:55:31,637 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 1090/1637 [1:36:43<48:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:55:36,956 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:55:36,956 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 1091/1637 [1:36:48<48:19,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:55:42,253 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:55:42,254 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 1092/1637 [1:36:53<48:13,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:55:47,561 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:55:47,561 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 1093/1637 [1:36:59<48:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:55:52,874 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:55:52,875 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 1094/1637 [1:37:04<48:06,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:55:58,203 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:55:58,204 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 1095/1637 [1:37:09<47:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:56:03,503 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:56:03,503 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 1096/1637 [1:37:15<47:51,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:56:08,805 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:56:08,806 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 1097/1637 [1:37:20<47:45,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:56:14,108 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:56:14,108 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 1098/1637 [1:37:25<47:40,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:56:19,420 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:56:19,420 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 1099/1637 [1:37:31<47:36,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:56:24,729 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:56:24,729 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 1100/1637 [1:37:36<47:32,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:56:30,048 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:56:30,048 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 1101/1637 [1:37:41<47:28,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:56:35,367 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:56:35,367 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 1102/1637 [1:37:47<47:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:56:40,680 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:56:40,680 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 1103/1637 [1:37:52<47:13,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:56:45,968 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:56:45,968 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 1104/1637 [1:37:57<47:09,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:56:51,295 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:56:51,296 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 1105/1637 [1:38:02<47:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:56:56,607 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:56:56,608 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 1106/1637 [1:38:08<46:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:57:01,895 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:57:01,895 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 1107/1637 [1:38:13<46:51,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:57:07,196 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:57:07,196 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 1108/1637 [1:38:18<46:46,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:57:12,505 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:57:12,506 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 1109/1637 [1:38:24<46:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:57:17,826 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:57:17,826 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 1110/1637 [1:38:29<46:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:57:23,129 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:57:23,130 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 1111/1637 [1:38:34<46:29,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:57:28,422 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:57:28,422 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 1112/1637 [1:38:40<46:23,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:57:33,722 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:57:33,722 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 1113/1637 [1:38:45<46:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:57:39,034 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:57:39,035 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 1114/1637 [1:38:50<46:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:57:44,344 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:57:44,344 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 1115/1637 [1:38:56<46:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:57:49,654 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:57:49,654 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 1116/1637 [1:39:01<46:07,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:57:54,976 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:57:54,976 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 1117/1637 [1:39:06<46:04,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:58:00,300 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:58:00,301 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 1118/1637 [1:39:11<45:58,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:58:05,614 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:58:05,615 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 1119/1637 [1:39:17<45:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:58:10,913 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:58:10,913 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 1120/1637 [1:39:22<45:45,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:58:16,223 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:58:16,224 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 1121/1637 [1:39:27<45:40,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:58:21,534 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:58:21,535 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 1122/1637 [1:39:33<45:36,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:58:26,854 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:58:26,854 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 1123/1637 [1:39:38<45:30,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:58:32,165 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:58:32,166 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 1124/1637 [1:39:43<45:27,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:58:37,495 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:58:37,495 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 1125/1637 [1:39:49<45:19,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:58:42,789 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:58:42,789 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 1126/1637 [1:39:54<45:13,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:58:48,097 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:58:48,098 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 1127/1637 [1:39:59<45:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:58:53,407 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:58:53,408 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 1128/1637 [1:40:05<45:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:58:58,708 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:58:58,708 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 1129/1637 [1:40:10<44:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:59:04,032 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:59:04,032 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 1130/1637 [1:40:15<44:51,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:59:09,333 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:59:09,333 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 1131/1637 [1:40:21<44:44,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:59:14,627 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:59:14,627 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 1132/1637 [1:40:26<44:38,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:59:19,932 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:59:19,932 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 1133/1637 [1:40:31<44:31,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:59:25,221 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:59:25,222 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 1134/1637 [1:40:36<44:28,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:59:30,542 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:59:30,542 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 1135/1637 [1:40:42<44:23,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:59:35,844 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:59:35,844 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 1136/1637 [1:40:47<44:19,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:59:41,159 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:59:41,159 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 1137/1637 [1:40:52<44:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:59:46,476 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:59:46,477 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 1138/1637 [1:40:58<44:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:59:51,791 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:59:51,791 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 1139/1637 [1:41:03<44:04,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 20:59:57,098 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 20:59:57,098 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 1140/1637 [1:41:08<43:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:00:02,401 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:00:02,401 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 1141/1637 [1:41:14<43:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:00:07,705 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:00:07,706 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 1142/1637 [1:41:19<43:49,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:00:13,028 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:00:13,029 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 1143/1637 [1:41:24<43:42,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:00:18,333 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:00:18,333 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 1144/1637 [1:41:30<43:38,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:00:23,648 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:00:23,648 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 1145/1637 [1:41:35<43:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:00:28,966 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:00:28,966 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 1146/1637 [1:41:40<43:29,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:00:34,282 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:00:34,282 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 1147/1637 [1:41:45<43:25,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:00:39,607 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:00:39,608 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 1148/1637 [1:41:51<43:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:00:44,903 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:00:44,904 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 1149/1637 [1:41:56<43:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:00:50,213 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:00:50,213 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 1150/1637 [1:42:01<43:07,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:00:55,532 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:00:55,532 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 1151/1637 [1:42:07<43:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:01:00,845 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:01:00,845 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 1152/1637 [1:42:12<42:56,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:01:06,156 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:01:06,157 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 1153/1637 [1:42:17<42:52,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:01:11,480 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:01:11,481 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 1154/1637 [1:42:23<42:47,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:01:16,795 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:01:16,795 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 1155/1637 [1:42:28<42:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:01:22,109 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:01:22,109 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 1156/1637 [1:42:33<42:36,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:01:27,422 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:01:27,422 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 1157/1637 [1:42:39<42:29,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:01:32,724 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:01:32,724 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 1158/1637 [1:42:44<42:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:01:38,022 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:01:38,022 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 1159/1637 [1:42:49<42:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:01:43,336 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:01:43,337 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 1160/1637 [1:42:55<42:15,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:01:48,668 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:01:48,668 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 1161/1637 [1:43:00<42:10,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:01:53,985 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:01:53,986 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 1162/1637 [1:43:05<42:05,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:01:59,300 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:01:59,300 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 1163/1637 [1:43:10<41:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:02:04,609 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:02:04,610 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 1164/1637 [1:43:16<41:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:02:09,919 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:02:09,919 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 1165/1637 [1:43:21<41:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:02:15,227 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:02:15,228 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 1166/1637 [1:43:26<41:42,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:02:20,548 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:02:20,548 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 1167/1637 [1:43:32<41:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:02:25,865 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:02:25,865 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 1168/1637 [1:43:37<41:32,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:02:31,175 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:02:31,176 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 1169/1637 [1:43:42<41:25,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:02:36,480 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:02:36,481 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 1170/1637 [1:43:48<41:19,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:02:41,784 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:02:41,784 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 1171/1637 [1:43:53<41:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:02:47,113 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:02:47,114 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 1172/1637 [1:43:58<41:11,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:02:52,429 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:02:52,429 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 1173/1637 [1:44:04<41:07,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:02:57,752 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:02:57,752 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 1174/1637 [1:44:09<40:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:03:03,049 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:03:03,050 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 1175/1637 [1:44:14<40:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:03:08,351 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:03:08,352 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 1176/1637 [1:44:20<40:49,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:03:13,678 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:03:13,678 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 1177/1637 [1:44:25<40:45,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:03:19,000 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:03:19,001 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 1178/1637 [1:44:30<40:39,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:03:24,308 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:03:24,308 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 1179/1637 [1:44:36<40:36,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:03:29,638 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:03:29,639 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 1180/1637 [1:44:41<40:32,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:03:34,970 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:03:34,971 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 1181/1637 [1:44:46<40:24,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:03:40,273 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:03:40,273 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 1182/1637 [1:44:51<40:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:03:45,578 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:03:45,578 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 1183/1637 [1:44:57<40:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:03:50,888 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:03:50,889 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 1184/1637 [1:45:02<40:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:03:56,198 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:03:56,199 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 1185/1637 [1:45:07<40:03,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:04:01,540 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:04:01,541 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 1186/1637 [1:45:13<39:59,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:04:06,856 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:04:06,856 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 1187/1637 [1:45:18<39:54,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:04:12,181 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:04:12,181 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 1188/1637 [1:45:23<39:50,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:04:17,512 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:04:17,513 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 1189/1637 [1:45:29<39:45,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:04:22,842 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:04:22,842 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 1190/1637 [1:45:34<39:39,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:04:28,153 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:04:28,153 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 1191/1637 [1:45:39<39:34,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:04:33,485 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:04:33,485 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 1192/1637 [1:45:45<39:27,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:04:38,794 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:04:38,794 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 1193/1637 [1:45:50<39:20,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:04:44,103 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:04:44,104 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 1194/1637 [1:45:55<39:13,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:04:49,407 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:04:49,408 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 1195/1637 [1:46:01<39:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:04:54,721 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:04:54,721 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 1196/1637 [1:46:06<39:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:05:00,028 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:05:00,028 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 1197/1637 [1:46:11<38:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:05:05,349 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:05:05,350 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 1198/1637 [1:46:17<38:53,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:05:10,666 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:05:10,667 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 1199/1637 [1:46:22<38:45,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:05:15,959 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:05:15,959 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 1200/1637 [1:46:27<38:39,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:05:21,266 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:05:21,267 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 1201/1637 [1:46:32<38:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:05:26,567 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:05:26,567 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 1202/1637 [1:46:38<38:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:05:31,870 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:05:31,870 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 1203/1637 [1:46:43<38:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:05:37,179 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:05:37,180 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 1204/1637 [1:46:48<38:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:05:42,486 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:05:42,487 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 1205/1637 [1:46:54<38:10,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:05:47,776 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:05:47,776 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 1206/1637 [1:46:59<38:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:05:53,092 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:05:53,092 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 1207/1637 [1:47:04<38:03,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:05:58,417 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:05:58,417 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 1208/1637 [1:47:10<37:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:06:03,723 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:06:03,724 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 1209/1637 [1:47:15<37:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:06:09,046 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:06:09,047 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 1210/1637 [1:47:20<37:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:06:14,358 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:06:14,358 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 1211/1637 [1:47:26<37:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:06:19,668 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:06:19,669 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 1212/1637 [1:47:31<37:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:06:24,975 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:06:24,975 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 1213/1637 [1:47:36<37:34,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:06:30,308 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:06:30,309 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 1214/1637 [1:47:42<37:29,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:06:35,625 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:06:35,626 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 1215/1637 [1:47:47<37:24,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:06:40,949 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:06:40,949 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 1216/1637 [1:47:52<37:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:06:46,251 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:06:46,252 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 1217/1637 [1:47:57<37:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:06:51,563 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:06:51,564 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 1218/1637 [1:48:03<37:05,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:06:56,873 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:06:56,874 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 1219/1637 [1:48:08<37:03,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:07:02,207 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:07:02,208 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 1220/1637 [1:48:13<36:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:07:07,509 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:07:07,509 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 1221/1637 [1:48:19<36:51,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:07:12,835 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:07:12,835 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 1222/1637 [1:48:24<36:47,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:07:18,156 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:07:18,156 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 1223/1637 [1:48:29<36:42,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:07:23,477 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:07:23,477 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 1224/1637 [1:48:35<36:36,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:07:28,790 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:07:28,791 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 1225/1637 [1:48:40<36:29,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:07:34,097 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:07:34,097 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 1226/1637 [1:48:45<36:24,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:07:39,416 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:07:39,416 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 1227/1637 [1:48:51<36:20,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:07:44,738 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:07:44,738 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 1228/1637 [1:48:56<36:13,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:07:50,041 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:07:50,041 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 1229/1637 [1:49:01<36:04,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:07:55,327 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:07:55,327 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 1230/1637 [1:49:07<35:58,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:08:00,626 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:08:00,627 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 1231/1637 [1:49:12<35:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:08:05,944 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:08:05,944 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 1232/1637 [1:49:17<35:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:08:11,244 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:08:11,244 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 1233/1637 [1:49:22<35:42,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:08:16,542 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:08:16,542 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 1234/1637 [1:49:28<35:37,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:08:21,843 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:08:21,844 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 1235/1637 [1:49:33<35:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:08:27,166 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:08:27,167 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 1236/1637 [1:49:38<35:29,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:08:32,481 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:08:32,481 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 1237/1637 [1:49:44<35:24,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:08:37,797 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:08:37,797 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 1238/1637 [1:49:49<35:19,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:08:43,108 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:08:43,108 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 1239/1637 [1:49:54<35:15,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:08:48,432 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:08:48,433 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 1240/1637 [1:50:00<35:10,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:08:53,750 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:08:53,750 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 1241/1637 [1:50:05<35:06,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:08:59,077 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:08:59,077 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 1242/1637 [1:50:10<35:00,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:09:04,391 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:09:04,391 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 1243/1637 [1:50:16<34:54,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:09:09,703 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:09:09,703 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 1244/1637 [1:50:21<34:50,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:09:15,029 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:09:15,029 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 1245/1637 [1:50:26<34:44,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:09:20,344 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:09:20,344 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 1246/1637 [1:50:32<34:40,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:09:25,670 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:09:25,671 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 1247/1637 [1:50:37<34:33,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:09:30,979 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:09:30,979 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 1248/1637 [1:50:42<34:29,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:09:36,307 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:09:36,307 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 1249/1637 [1:50:48<34:24,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:09:41,631 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:09:41,631 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 1250/1637 [1:50:53<34:19,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:09:46,951 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:09:46,952 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 1251/1637 [1:50:58<34:12,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:09:52,261 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:09:52,261 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 1252/1637 [1:51:03<34:06,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:09:57,572 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:09:57,573 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 1253/1637 [1:51:09<34:00,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:10:02,882 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:10:02,882 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 1254/1637 [1:51:14<33:53,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:10:08,183 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:10:08,183 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 1255/1637 [1:51:19<33:46,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:10:13,475 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:10:13,475 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 1256/1637 [1:51:25<33:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:10:18,787 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:10:18,787 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 1257/1637 [1:51:30<33:38,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:10:24,112 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:10:24,112 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 1258/1637 [1:51:35<33:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:10:29,424 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:10:29,425 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 1259/1637 [1:51:41<33:28,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:10:34,743 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:10:34,744 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 1260/1637 [1:51:46<33:23,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:10:40,059 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:10:40,059 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 1261/1637 [1:51:51<33:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:10:45,368 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:10:45,368 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 1262/1637 [1:51:57<33:13,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:10:50,692 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:10:50,693 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 1263/1637 [1:52:02<33:07,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:10:56,001 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:10:56,002 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 1264/1637 [1:52:07<33:03,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:11:01,325 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:11:01,326 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 1265/1637 [1:52:13<32:57,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:11:06,641 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:11:06,641 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 1266/1637 [1:52:18<32:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:11:11,944 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:11:11,944 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 1267/1637 [1:52:23<32:45,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:11:17,251 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:11:17,252 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 1268/1637 [1:52:28<32:39,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:11:22,563 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:11:22,564 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 1269/1637 [1:52:34<32:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:11:27,867 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:11:27,867 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 1270/1637 [1:52:39<32:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:11:33,168 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:11:33,169 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 1271/1637 [1:52:44<32:21,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:11:38,474 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:11:38,475 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 1272/1637 [1:52:50<32:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:11:43,785 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:11:43,785 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 1273/1637 [1:52:55<32:13,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:11:49,105 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:11:49,105 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 1274/1637 [1:53:00<32:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:11:54,404 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:11:54,404 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 1275/1637 [1:53:06<32:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:11:59,721 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:11:59,722 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 1276/1637 [1:53:11<31:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:12:05,045 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:12:05,045 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 1277/1637 [1:53:16<31:53,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:12:10,360 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:12:10,361 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 1278/1637 [1:53:22<31:46,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:12:15,662 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:12:15,663 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 1279/1637 [1:53:27<31:40,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:12:20,964 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:12:20,965 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 1280/1637 [1:53:32<31:35,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:12:26,276 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:12:26,276 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 1281/1637 [1:53:37<31:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:12:31,599 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:12:31,599 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 1282/1637 [1:53:43<31:27,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:12:36,923 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:12:36,924 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 1283/1637 [1:53:48<31:22,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:12:42,239 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:12:42,240 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 1284/1637 [1:53:53<31:16,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:12:47,554 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:12:47,554 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 1285/1637 [1:53:59<31:11,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:12:52,868 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:12:52,868 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 1286/1637 [1:54:04<31:05,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:12:58,185 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:12:58,185 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 1287/1637 [1:54:09<30:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:13:03,492 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:13:03,492 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 1288/1637 [1:54:15<30:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:13:08,791 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:13:08,792 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 1289/1637 [1:54:20<30:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:13:14,095 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:13:14,096 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 1290/1637 [1:54:25<30:42,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:13:19,408 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:13:19,408 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 1291/1637 [1:54:31<30:38,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:13:24,734 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:13:24,734 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 1292/1637 [1:54:36<30:34,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:13:30,054 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:13:30,055 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 1293/1637 [1:54:41<30:28,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:13:35,367 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:13:35,367 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 1294/1637 [1:54:47<30:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:13:40,673 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:13:40,674 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 1295/1637 [1:54:52<30:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:13:45,975 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:13:45,976 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 1296/1637 [1:54:57<30:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:13:51,283 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:13:51,283 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 1297/1637 [1:55:02<30:05,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:13:56,596 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:13:56,597 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 1298/1637 [1:55:08<29:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:14:01,903 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:14:01,903 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 1299/1637 [1:55:13<29:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:14:07,221 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:14:07,222 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 1300/1637 [1:55:18<29:49,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:14:12,528 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:14:12,529 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 1301/1637 [1:55:24<29:42,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:14:17,820 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:14:17,820 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 1302/1637 [1:55:29<29:36,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:14:23,119 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:14:23,120 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 1303/1637 [1:55:34<29:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:14:28,429 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:14:28,429 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 1304/1637 [1:55:40<29:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:14:33,738 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:14:33,738 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 1305/1637 [1:55:45<29:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:14:39,052 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:14:39,053 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 1306/1637 [1:55:50<29:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:14:44,369 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:14:44,370 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 1307/1637 [1:55:56<29:14,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:14:49,695 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:14:49,695 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 1308/1637 [1:56:01<29:07,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:14:54,997 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:14:54,997 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 1309/1637 [1:56:06<29:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:15:00,306 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:15:00,306 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 1310/1637 [1:56:11<28:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:15:05,608 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:15:05,608 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 1311/1637 [1:56:17<28:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:15:10,911 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:15:10,912 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 1312/1637 [1:56:22<28:46,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:15:16,237 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:15:16,237 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 1313/1637 [1:56:27<28:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:15:21,549 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:15:21,549 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 1314/1637 [1:56:33<28:35,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:15:26,862 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:15:26,862 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 1315/1637 [1:56:38<28:30,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:15:32,175 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:15:32,175 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 1316/1637 [1:56:43<28:24,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:15:37,478 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:15:37,479 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 1317/1637 [1:56:49<28:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:15:42,803 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:15:42,803 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 1318/1637 [1:56:54<28:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:15:48,111 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:15:48,111 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 1319/1637 [1:56:59<28:09,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:15:53,427 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:15:53,427 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 1320/1637 [1:57:05<28:03,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:15:58,730 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:15:58,730 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 1321/1637 [1:57:10<27:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:16:04,043 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:16:04,043 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 1322/1637 [1:57:15<27:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:16:09,352 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:16:09,352 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 1323/1637 [1:57:21<27:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:16:14,677 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:16:14,677 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 1324/1637 [1:57:26<27:42,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:16:19,984 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:16:19,984 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 1325/1637 [1:57:31<27:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:16:25,294 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:16:25,295 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 1326/1637 [1:57:36<27:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:16:30,599 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:16:30,599 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 1327/1637 [1:57:42<27:25,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:16:35,902 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:16:35,902 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 1328/1637 [1:57:47<27:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:16:41,214 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:16:41,214 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 1329/1637 [1:57:52<27:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:16:46,519 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:16:46,519 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 1330/1637 [1:57:58<27:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:16:51,844 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:16:51,845 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 1331/1637 [1:58:03<27:05,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:16:57,151 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:16:57,152 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 1332/1637 [1:58:08<26:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:17:02,443 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:17:02,443 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 1333/1637 [1:58:14<26:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:17:07,770 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:17:07,770 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 1334/1637 [1:58:19<26:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:17:13,090 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:17:13,090 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 1335/1637 [1:58:24<26:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:17:18,394 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:17:18,394 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 1336/1637 [1:58:30<26:38,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:17:23,698 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:17:23,698 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 1337/1637 [1:58:35<26:32,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:17:29,004 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:17:29,004 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 1338/1637 [1:58:40<26:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:17:34,312 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:17:34,312 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 1339/1637 [1:58:45<26:21,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:17:39,617 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:17:39,618 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 1340/1637 [1:58:51<26:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:17:44,925 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:17:44,925 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 1341/1637 [1:58:56<26:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:17:50,236 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:17:50,237 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 1342/1637 [1:59:01<26:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:17:55,548 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:17:55,548 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 1343/1637 [1:59:07<25:59,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:18:00,840 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:18:00,841 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 1344/1637 [1:59:12<25:54,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:18:06,143 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:18:06,144 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 1345/1637 [1:59:17<25:47,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:18:11,431 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:18:11,431 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 1346/1637 [1:59:23<25:42,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:18:16,731 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:18:16,732 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 1347/1637 [1:59:28<25:37,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:18:22,035 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:18:22,036 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 1348/1637 [1:59:33<25:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:18:27,359 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:18:27,359 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 1349/1637 [1:59:39<25:28,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:18:32,667 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:18:32,667 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 1350/1637 [1:59:44<25:23,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:18:37,981 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:18:37,981 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 1351/1637 [1:59:49<25:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:18:43,275 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:18:43,276 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 1352/1637 [1:59:54<25:12,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:18:48,588 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:18:48,589 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 1353/1637 [2:00:00<25:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:18:53,909 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:18:53,910 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 1354/1637 [2:00:05<25:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:18:59,210 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:18:59,210 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 1355/1637 [2:00:10<24:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:19:04,527 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:19:04,527 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 1356/1637 [2:00:16<24:53,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:19:09,850 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:19:09,850 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 1357/1637 [2:00:21<24:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:19:15,163 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:19:15,164 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 1358/1637 [2:00:26<24:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:19:20,470 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:19:20,471 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 1359/1637 [2:00:32<24:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:19:25,789 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:19:25,789 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 1360/1637 [2:00:37<24:32,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:19:31,108 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:19:31,108 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 1361/1637 [2:00:42<24:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:19:36,418 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:19:36,418 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 1362/1637 [2:00:48<24:22,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:19:41,746 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:19:41,746 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 1363/1637 [2:00:53<24:16,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:19:47,059 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:19:47,060 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 1364/1637 [2:00:58<24:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:19:52,368 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:19:52,368 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 1365/1637 [2:01:04<24:04,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:19:57,673 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:19:57,674 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 1366/1637 [2:01:09<24:00,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:20:03,001 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:20:03,002 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 1367/1637 [2:01:14<23:55,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:20:08,316 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:20:08,317 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 1368/1637 [2:01:20<23:50,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:20:13,634 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:20:13,635 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 1369/1637 [2:01:25<23:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:20:18,948 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:20:18,949 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 1370/1637 [2:01:30<23:40,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:20:24,280 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:20:24,280 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 1371/1637 [2:01:35<23:34,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:20:29,595 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:20:29,596 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 1372/1637 [2:01:41<23:29,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:20:34,912 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:20:34,912 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 1373/1637 [2:01:46<23:24,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:20:40,235 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:20:40,236 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 1374/1637 [2:01:51<23:18,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:20:45,548 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:20:45,549 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 1375/1637 [2:01:57<23:13,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:20:50,866 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:20:50,866 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 1376/1637 [2:02:02<23:07,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:20:56,172 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:20:56,172 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 1377/1637 [2:02:07<23:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:21:01,486 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:21:01,487 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 1378/1637 [2:02:13<22:57,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:21:06,815 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:21:06,816 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 1379/1637 [2:02:18<22:52,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:21:12,144 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:21:12,144 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 1380/1637 [2:02:23<22:46,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:21:17,452 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:21:17,452 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 1381/1637 [2:02:29<22:40,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:21:22,762 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:21:22,762 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 1382/1637 [2:02:34<22:35,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:21:28,080 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:21:28,081 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 1383/1637 [2:02:39<22:30,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:21:33,393 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:21:33,394 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 1384/1637 [2:02:45<22:24,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:21:38,700 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:21:38,701 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 1385/1637 [2:02:50<22:20,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:21:44,033 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:21:44,034 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 1386/1637 [2:02:55<22:15,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:21:49,354 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:21:49,355 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 1387/1637 [2:03:01<22:09,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:21:54,667 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:21:54,667 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 1388/1637 [2:03:06<22:03,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:21:59,977 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:21:59,977 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 1389/1637 [2:03:11<21:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:22:05,284 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:22:05,284 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 1390/1637 [2:03:16<21:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:22:10,597 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:22:10,597 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 1391/1637 [2:03:22<21:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:22:15,916 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:22:15,917 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 1392/1637 [2:03:27<21:42,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:22:21,243 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:22:21,243 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 1393/1637 [2:03:32<21:37,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:22:26,560 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:22:26,560 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 1394/1637 [2:03:38<21:31,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:22:31,869 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:22:31,869 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 1395/1637 [2:03:43<21:26,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:22:37,188 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:22:37,189 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 1396/1637 [2:03:48<21:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:22:42,496 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:22:42,497 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 1397/1637 [2:03:54<21:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:22:47,803 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:22:47,804 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 1398/1637 [2:03:59<21:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:22:53,125 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:22:53,125 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 1399/1637 [2:04:04<21:04,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:22:58,427 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:22:58,428 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 1400/1637 [2:04:10<20:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:23:03,746 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:23:03,747 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 1401/1637 [2:04:15<20:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:23:09,043 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:23:09,044 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 1402/1637 [2:04:20<20:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:23:14,348 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:23:14,348 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 1403/1637 [2:04:26<20:42,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:23:19,659 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:23:19,659 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 1404/1637 [2:04:31<20:36,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:23:24,968 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:23:24,968 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 1405/1637 [2:04:36<20:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:23:30,280 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:23:30,280 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 1406/1637 [2:04:41<20:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:23:35,590 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:23:35,591 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 1407/1637 [2:04:47<20:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:23:40,884 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:23:40,884 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 1408/1637 [2:04:52<20:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:23:46,196 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:23:46,196 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 1409/1637 [2:04:57<20:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:23:51,507 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:23:51,507 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 1410/1637 [2:05:03<20:05,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:23:56,820 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:23:56,820 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 1411/1637 [2:05:08<20:02,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:24:02,161 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:24:02,161 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 1412/1637 [2:05:13<19:56,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:24:07,471 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:24:07,471 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 1413/1637 [2:05:19<19:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:24:12,779 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:24:12,779 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 1414/1637 [2:05:24<19:45,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:24:18,097 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:24:18,098 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 1415/1637 [2:05:29<19:40,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:24:23,413 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:24:23,413 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 1416/1637 [2:05:35<19:34,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:24:28,729 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:24:28,729 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 1417/1637 [2:05:40<19:30,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:24:34,053 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:24:34,053 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 1418/1637 [2:05:45<19:24,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:24:39,375 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:24:39,376 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 1419/1637 [2:05:51<19:19,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:24:44,697 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:24:44,697 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 1420/1637 [2:05:56<19:15,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:24:50,029 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:24:50,029 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 1421/1637 [2:06:01<19:09,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:24:55,346 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:24:55,346 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 1422/1637 [2:06:07<19:03,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:25:00,655 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:25:00,655 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 1423/1637 [2:06:12<18:57,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:25:05,963 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:25:05,964 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 1424/1637 [2:06:17<18:52,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:25:11,282 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:25:11,283 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 1425/1637 [2:06:22<18:47,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:25:16,611 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:25:16,612 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 1426/1637 [2:06:28<18:41,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:25:21,918 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:25:21,918 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 1427/1637 [2:06:33<18:37,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:25:27,248 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:25:27,249 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 1428/1637 [2:06:38<18:32,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:25:32,571 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:25:32,571 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 1429/1637 [2:06:44<18:26,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:25:37,895 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:25:37,895 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 1430/1637 [2:06:49<18:19,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:25:43,188 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:25:43,188 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 1431/1637 [2:06:54<18:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:25:48,495 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:25:48,496 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 1432/1637 [2:07:00<18:09,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:25:53,812 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:25:53,813 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 1433/1637 [2:07:05<18:03,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:25:59,119 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:25:59,120 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 1434/1637 [2:07:10<17:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:26:04,421 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:26:04,421 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 1435/1637 [2:07:16<17:52,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:26:09,731 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:26:09,732 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 1436/1637 [2:07:21<17:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:26:15,059 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:26:15,059 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 1437/1637 [2:07:26<17:42,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:26:20,360 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:26:20,361 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 1438/1637 [2:07:32<17:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:26:25,678 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:26:25,678 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 1439/1637 [2:07:37<17:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:26:30,990 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:26:30,991 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 1440/1637 [2:07:42<17:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:26:36,293 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:26:36,294 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 1441/1637 [2:07:47<17:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:26:41,597 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:26:41,597 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 1442/1637 [2:07:53<17:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:26:46,914 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:26:46,914 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 1443/1637 [2:07:58<17:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:26:52,223 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:26:52,223 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 1444/1637 [2:08:03<17:04,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:26:57,536 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:26:57,537 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 1445/1637 [2:08:09<17:00,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:27:02,863 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:27:02,863 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 1446/1637 [2:08:14<16:55,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:27:08,190 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:27:08,190 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 1447/1637 [2:08:19<16:50,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:27:13,502 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:27:13,503 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 1448/1637 [2:08:25<16:45,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:27:18,829 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:27:18,829 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 1449/1637 [2:08:30<16:39,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:27:24,134 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:27:24,134 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 1450/1637 [2:08:35<16:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:27:29,432 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:27:29,433 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 1451/1637 [2:08:41<16:28,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:27:34,753 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:27:34,753 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 1452/1637 [2:08:46<16:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:27:40,066 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:27:40,066 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 1453/1637 [2:08:51<16:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:27:45,381 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:27:45,381 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 1454/1637 [2:08:57<16:12,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:27:50,701 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:27:50,701 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 1455/1637 [2:09:02<16:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:27:56,007 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:27:56,007 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 1456/1637 [2:09:07<16:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:28:01,318 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:28:01,319 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 1457/1637 [2:09:12<15:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:28:06,617 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:28:06,617 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 1458/1637 [2:09:18<15:49,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:28:11,919 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:28:11,920 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 1459/1637 [2:09:23<15:45,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:28:17,245 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:28:17,246 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 1460/1637 [2:09:28<15:40,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:28:22,562 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:28:22,563 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 1461/1637 [2:09:34<15:35,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:28:27,875 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:28:27,875 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 1462/1637 [2:09:39<15:30,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:28:33,197 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:28:33,198 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 1463/1637 [2:09:44<15:24,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:28:38,512 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:28:38,512 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 1464/1637 [2:09:50<15:19,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:28:43,828 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:28:43,828 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 1465/1637 [2:09:55<15:14,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:28:49,146 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:28:49,146 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 1466/1637 [2:10:00<15:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:28:54,444 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:28:54,445 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 1467/1637 [2:10:06<15:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:28:59,756 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:28:59,756 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 1468/1637 [2:10:11<14:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:29:05,056 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:29:05,057 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 1469/1637 [2:10:16<14:51,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:29:10,368 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:29:10,369 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 1470/1637 [2:10:22<14:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:29:15,686 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:29:15,687 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 1471/1637 [2:10:27<14:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:29:20,999 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:29:21,000 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 1472/1637 [2:10:32<14:36,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:29:26,303 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:29:26,304 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 1473/1637 [2:10:37<14:30,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:29:31,612 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:29:31,612 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 1474/1637 [2:10:43<14:25,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:29:36,915 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:29:36,915 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 1475/1637 [2:10:48<14:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:29:42,226 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:29:42,226 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 1476/1637 [2:10:53<14:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:29:47,533 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:29:47,533 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 1477/1637 [2:10:59<14:09,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:29:52,846 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:29:52,846 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 1478/1637 [2:11:04<14:04,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:29:58,156 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:29:58,156 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 1479/1637 [2:11:09<13:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:30:03,477 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:30:03,477 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 1480/1637 [2:11:15<13:53,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:30:08,777 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:30:08,777 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 1481/1637 [2:11:20<13:47,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:30:14,064 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:30:14,064 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 1482/1637 [2:11:25<13:41,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:30:19,348 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:30:19,348 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 1483/1637 [2:11:31<13:36,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:30:24,659 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:30:24,659 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 1484/1637 [2:11:36<13:31,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:30:29,969 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:30:29,969 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 1485/1637 [2:11:41<13:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:30:35,293 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:30:35,293 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 1486/1637 [2:11:46<13:22,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:30:40,606 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:30:40,607 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 1487/1637 [2:11:52<13:16,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:30:45,916 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:30:45,917 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 1488/1637 [2:11:57<13:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:30:51,224 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:30:51,225 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 1489/1637 [2:12:02<13:05,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:30:56,526 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:30:56,527 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 1490/1637 [2:12:08<13:00,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:31:01,841 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:31:01,841 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 1491/1637 [2:12:13<12:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:31:07,163 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:31:07,163 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 1492/1637 [2:12:18<12:51,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:31:12,503 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:31:12,503 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 1493/1637 [2:12:24<12:46,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:31:17,818 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:31:17,818 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 1494/1637 [2:12:29<12:39,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:31:23,111 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:31:23,111 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 1495/1637 [2:12:34<12:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:31:28,419 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:31:28,419 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 1496/1637 [2:12:40<12:28,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:31:33,717 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:31:33,717 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 1497/1637 [2:12:45<12:22,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:31:39,011 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:31:39,011 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 1498/1637 [2:12:50<12:17,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:31:44,316 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:31:44,316 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 1499/1637 [2:12:56<12:12,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:31:49,629 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:31:49,630 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "{'loss': 0.2703, 'learning_rate': 1.6737935247403788e-06, 'epoch': 0.92}\n",
            " 92% 1500/1637 [2:13:01<12:06,  5.31s/it][INFO|trainer.py:1987] 2021-10-03 21:31:54,924 >> Saving model checkpoint to /tmp/qnli/checkpoint-1500\n",
            "[INFO|configuration_utils.py:413] 2021-10-03 21:31:54,926 >> Configuration saved in /tmp/qnli/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-03 21:31:56,840 >> Model weights saved in /tmp/qnli/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-03 21:31:56,841 >> tokenizer config file saved in /tmp/qnli/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-03 21:31:56,841 >> Special tokens file saved in /tmp/qnli/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|modeling_longformer.py:1853] 2021-10-03 21:32:01,467 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:32:01,467 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 1501/1637 [2:13:13<16:28,  7.27s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:32:06,786 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:32:06,787 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 1502/1637 [2:13:18<15:01,  6.68s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:32:12,095 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:32:12,095 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 1503/1637 [2:13:23<13:59,  6.27s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:32:17,392 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:32:17,392 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 1504/1637 [2:13:29<13:15,  5.98s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:32:22,704 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:32:22,704 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 1505/1637 [2:13:34<12:42,  5.77s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:32:28,000 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:32:28,000 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 1506/1637 [2:13:39<12:17,  5.63s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:32:33,300 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:32:33,300 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 1507/1637 [2:13:44<11:59,  5.54s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:32:38,611 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:32:38,611 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 1508/1637 [2:13:50<11:44,  5.47s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:32:43,910 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:32:43,911 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 1509/1637 [2:13:55<11:33,  5.41s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:32:49,205 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:32:49,206 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 1510/1637 [2:14:00<11:22,  5.38s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:32:54,495 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:32:54,495 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 1511/1637 [2:14:06<11:14,  5.35s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:32:59,796 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:32:59,796 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 1512/1637 [2:14:11<11:07,  5.34s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:33:05,112 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:33:05,112 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 1513/1637 [2:14:16<11:00,  5.33s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:33:10,409 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:33:10,409 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 1514/1637 [2:14:22<10:54,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:33:15,712 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:33:15,712 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 1515/1637 [2:14:27<10:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:33:21,009 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:33:21,009 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 1516/1637 [2:14:32<10:43,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:33:26,326 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:33:26,327 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 1517/1637 [2:14:38<10:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:33:31,635 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:33:31,635 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 1518/1637 [2:14:43<10:32,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:33:36,952 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:33:36,952 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 1519/1637 [2:14:48<10:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:33:42,251 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:33:42,251 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 1520/1637 [2:14:53<10:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:33:47,553 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:33:47,553 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 1521/1637 [2:14:59<10:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:33:52,860 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:33:52,860 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 1522/1637 [2:15:04<10:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:33:58,166 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:33:58,167 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 1523/1637 [2:15:09<10:05,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:34:03,494 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:34:03,495 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 1524/1637 [2:15:15<10:00,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:34:08,797 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:34:08,798 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 1525/1637 [2:15:20<09:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:34:14,108 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:34:14,108 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 1526/1637 [2:15:25<09:49,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:34:19,424 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:34:19,425 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 1527/1637 [2:15:31<09:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:34:24,736 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:34:24,737 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 1528/1637 [2:15:36<09:38,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:34:30,037 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:34:30,038 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 1529/1637 [2:15:41<09:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:34:35,336 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:34:35,336 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 1530/1637 [2:15:47<09:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:34:40,643 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:34:40,644 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 1531/1637 [2:15:52<09:22,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:34:45,944 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:34:45,944 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 1532/1637 [2:15:57<09:16,  5.30s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:34:51,234 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:34:51,235 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 1533/1637 [2:16:02<09:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:34:56,552 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:34:56,552 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 1534/1637 [2:16:08<09:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:35:01,868 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:35:01,868 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 1535/1637 [2:16:13<09:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:35:07,175 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:35:07,175 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 1536/1637 [2:16:18<08:56,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:35:12,482 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:35:12,482 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 1537/1637 [2:16:24<08:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:35:17,787 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:35:17,788 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 1538/1637 [2:16:29<08:45,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:35:23,107 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:35:23,107 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 1539/1637 [2:16:34<08:40,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:35:28,420 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:35:28,420 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 1540/1637 [2:16:40<08:35,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:35:33,740 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:35:33,740 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 1541/1637 [2:16:45<08:30,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:35:39,057 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:35:39,057 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 1542/1637 [2:16:50<08:24,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:35:44,357 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:35:44,357 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 1543/1637 [2:16:56<08:19,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:35:49,664 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:35:49,665 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 1544/1637 [2:17:01<08:13,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:35:54,971 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:35:54,971 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 1545/1637 [2:17:06<08:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:36:00,283 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:36:00,283 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 1546/1637 [2:17:11<08:03,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:36:05,595 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:36:05,595 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 1547/1637 [2:17:17<07:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:36:10,914 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:36:10,914 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 1548/1637 [2:17:22<07:53,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:36:16,234 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:36:16,235 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 1549/1637 [2:17:27<07:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:36:21,544 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:36:21,545 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 1550/1637 [2:17:33<07:42,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:36:26,872 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:36:26,872 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 1551/1637 [2:17:38<07:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:36:32,179 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:36:32,180 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 1552/1637 [2:17:43<07:32,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:36:37,508 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:36:37,509 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 1553/1637 [2:17:49<07:26,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:36:42,821 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:36:42,822 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 1554/1637 [2:17:54<07:21,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:36:48,147 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:36:48,147 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 1555/1637 [2:17:59<07:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:36:53,450 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:36:53,450 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 1556/1637 [2:18:05<07:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:36:58,761 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:36:58,761 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 1557/1637 [2:18:10<07:05,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:37:04,078 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:37:04,078 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 1558/1637 [2:18:15<06:59,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:37:09,389 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:37:09,389 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 1559/1637 [2:18:21<06:54,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:37:14,696 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:37:14,697 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 1560/1637 [2:18:26<06:49,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:37:20,023 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:37:20,023 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 1561/1637 [2:18:31<06:44,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:37:25,338 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:37:25,338 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 1562/1637 [2:18:37<06:38,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:37:30,640 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:37:30,640 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 1563/1637 [2:18:42<06:33,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:37:35,953 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:37:35,953 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 1564/1637 [2:18:47<06:28,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:37:41,277 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:37:41,278 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 1565/1637 [2:18:52<06:23,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:37:46,609 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:37:46,609 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 1566/1637 [2:18:58<06:17,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:37:51,904 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:37:51,904 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 1567/1637 [2:19:03<06:11,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:37:57,216 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:37:57,216 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 1568/1637 [2:19:08<06:06,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:38:02,529 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:38:02,530 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 1569/1637 [2:19:14<06:01,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:38:07,834 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:38:07,835 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 1570/1637 [2:19:19<05:55,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:38:13,142 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:38:13,142 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 1571/1637 [2:19:24<05:50,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:38:18,451 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:38:18,451 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 1572/1637 [2:19:30<05:45,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:38:23,763 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:38:23,763 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 1573/1637 [2:19:35<05:40,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:38:29,086 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:38:29,087 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 1574/1637 [2:19:40<05:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:38:34,402 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:38:34,402 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 1575/1637 [2:19:46<05:29,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:38:39,712 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:38:39,712 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 1576/1637 [2:19:51<05:24,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:38:45,039 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:38:45,040 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 1577/1637 [2:19:56<05:18,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:38:50,349 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:38:50,350 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 1578/1637 [2:20:02<05:13,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:38:55,675 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:38:55,675 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 1579/1637 [2:20:07<05:08,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:39:00,981 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:39:00,981 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 1580/1637 [2:20:12<05:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:39:06,286 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:39:06,286 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 1581/1637 [2:20:17<04:57,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:39:11,608 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:39:11,609 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 1582/1637 [2:20:23<04:52,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:39:16,925 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:39:16,925 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 1583/1637 [2:20:28<04:47,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:39:22,240 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:39:22,240 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 1584/1637 [2:20:33<04:41,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:39:27,547 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:39:27,547 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 1585/1637 [2:20:39<04:36,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:39:32,877 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:39:32,877 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 1586/1637 [2:20:44<04:31,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:39:38,188 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:39:38,189 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 1587/1637 [2:20:49<04:25,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:39:43,488 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:39:43,489 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 1588/1637 [2:20:55<04:20,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:39:48,802 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:39:48,803 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 1589/1637 [2:21:00<04:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:39:54,098 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:39:54,098 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 1590/1637 [2:21:05<04:09,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:39:59,408 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:39:59,409 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 1591/1637 [2:21:11<04:04,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:40:04,708 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:40:04,708 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 1592/1637 [2:21:16<03:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:40:10,025 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:40:10,026 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 1593/1637 [2:21:21<03:53,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:40:15,347 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:40:15,347 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 1594/1637 [2:21:27<03:48,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:40:20,657 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:40:20,657 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 1595/1637 [2:21:32<03:43,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:40:25,975 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:40:25,975 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 1596/1637 [2:21:37<03:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:40:31,275 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:40:31,275 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 1597/1637 [2:21:42<03:32,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:40:36,600 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:40:36,601 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 1598/1637 [2:21:48<03:27,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:40:41,909 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:40:41,909 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 1599/1637 [2:21:53<03:21,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:40:47,231 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:40:47,231 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 1600/1637 [2:21:58<03:16,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:40:52,552 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:40:52,552 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 1601/1637 [2:22:04<03:11,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:40:57,870 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:40:57,870 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 1602/1637 [2:22:09<03:06,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:41:03,184 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:41:03,185 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 1603/1637 [2:22:14<03:00,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:41:08,498 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:41:08,498 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 1604/1637 [2:22:20<02:55,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:41:13,814 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:41:13,814 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 1605/1637 [2:22:25<02:49,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:41:19,115 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:41:19,116 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 1606/1637 [2:22:30<02:44,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:41:24,426 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:41:24,426 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 1607/1637 [2:22:36<02:39,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:41:29,739 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:41:29,740 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 1608/1637 [2:22:41<02:34,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:41:35,055 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:41:35,056 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 1609/1637 [2:22:46<02:28,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:41:40,378 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:41:40,378 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 1610/1637 [2:22:52<02:23,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:41:45,681 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:41:45,681 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 1611/1637 [2:22:57<02:18,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:41:50,993 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:41:50,994 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 1612/1637 [2:23:02<02:12,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:41:56,290 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:41:56,291 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 1613/1637 [2:23:07<02:07,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:42:01,594 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:42:01,595 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 1614/1637 [2:23:13<02:02,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:42:06,905 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:42:06,905 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 1615/1637 [2:23:18<01:56,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:42:12,226 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:42:12,226 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 1616/1637 [2:23:23<01:51,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:42:17,537 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:42:17,537 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 1617/1637 [2:23:29<01:46,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:42:22,838 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:42:22,838 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 1618/1637 [2:23:34<01:40,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:42:28,138 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:42:28,139 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 1619/1637 [2:23:39<01:35,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:42:33,455 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:42:33,456 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 1620/1637 [2:23:45<01:30,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:42:38,783 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:42:38,783 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 1621/1637 [2:23:50<01:24,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:42:44,084 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:42:44,084 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 1622/1637 [2:23:55<01:19,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:42:49,393 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:42:49,394 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 1623/1637 [2:24:01<01:14,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:42:54,707 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:42:54,707 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 1624/1637 [2:24:06<01:09,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:43:00,014 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:43:00,014 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 1625/1637 [2:24:11<01:03,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:43:05,326 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:43:05,326 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 1626/1637 [2:24:17<00:58,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:43:10,635 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:43:10,636 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 1627/1637 [2:24:22<00:53,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:43:15,939 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:43:15,939 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 1628/1637 [2:24:27<00:47,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:43:21,256 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:43:21,257 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "100% 1629/1637 [2:24:32<00:42,  5.32s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:43:26,588 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:43:26,588 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "100% 1630/1637 [2:24:38<00:37,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:43:31,893 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:43:31,894 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "100% 1631/1637 [2:24:43<00:31,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:43:37,190 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:43:37,190 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "100% 1632/1637 [2:24:48<00:26,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:43:42,501 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:43:42,501 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "100% 1633/1637 [2:24:54<00:21,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:43:47,816 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:43:47,816 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "100% 1634/1637 [2:24:59<00:15,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:43:53,139 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:43:53,139 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "100% 1635/1637 [2:25:04<00:10,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:43:58,438 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:43:58,438 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "100% 1636/1637 [2:25:10<00:05,  5.31s/it][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:03,730 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:03,730 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "100% 1637/1637 [2:25:13<00:00,  4.68s/it][INFO|trainer.py:1401] 2021-10-03 21:44:06,945 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 8713.3474, 'train_samples_per_second': 12.021, 'train_steps_per_second': 0.188, 'train_loss': 0.31511726216273694, 'epoch': 1.0}\n",
            "100% 1637/1637 [2:25:13<00:00,  5.32s/it]\n",
            "[INFO|trainer.py:1987] 2021-10-03 21:44:06,947 >> Saving model checkpoint to /tmp/qnli/\n",
            "[INFO|configuration_utils.py:413] 2021-10-03 21:44:06,948 >> Configuration saved in /tmp/qnli/config.json\n",
            "[INFO|modeling_utils.py:1041] 2021-10-03 21:44:08,675 >> Model weights saved in /tmp/qnli/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2033] 2021-10-03 21:44:08,676 >> tokenizer config file saved in /tmp/qnli/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2039] 2021-10-03 21:44:08,677 >> Special tokens file saved in /tmp/qnli/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  train_loss               =     0.3151\n",
            "  train_runtime            = 2:25:13.34\n",
            "  train_samples            =     104743\n",
            "  train_samples_per_second =     12.021\n",
            "  train_steps_per_second   =      0.188\n",
            "10/03/2021 21:44:08 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:541] 2021-10-03 21:44:08,796 >> The following columns in the evaluation set  don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: question, sentence, idx.\n",
            "[INFO|trainer.py:2235] 2021-10-03 21:44:08,799 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2237] 2021-10-03 21:44:08,799 >>   Num examples = 5463\n",
            "[INFO|trainer.py:2240] 2021-10-03 21:44:08,799 >>   Batch size = 8\n",
            "[INFO|modeling_longformer.py:1853] 2021-10-03 21:44:08,804 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:08,805 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  0% 0/683 [00:00<?, ?it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:08,973 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:08,973 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  0% 2/683 [00:00<00:53, 12.63it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:09,132 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:09,133 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "[INFO|modeling_longformer.py:1853] 2021-10-03 21:44:09,291 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:09,292 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 4/683 [00:00<01:25,  7.92it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:09,451 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:09,451 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 5/683 [00:00<01:32,  7.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:09,610 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:09,611 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 6/683 [00:00<01:36,  7.01it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:09,768 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:09,769 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 7/683 [00:00<01:39,  6.77it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:09,930 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:09,931 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 8/683 [00:01<01:42,  6.58it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:10,090 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:10,090 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 9/683 [00:01<01:42,  6.56it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:10,243 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:10,243 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  1% 10/683 [00:01<01:43,  6.50it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:10,400 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:10,400 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 11/683 [00:01<01:43,  6.47it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:10,556 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:10,556 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 12/683 [00:01<01:44,  6.43it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:10,714 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:10,714 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 13/683 [00:01<01:44,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:10,873 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:10,874 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 14/683 [00:02<01:44,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:11,028 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:11,029 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 15/683 [00:02<01:44,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:11,186 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:11,187 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 16/683 [00:02<01:44,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:11,341 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:11,341 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  2% 17/683 [00:02<01:44,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:11,499 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:11,499 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 18/683 [00:02<01:44,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:11,659 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:11,659 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 19/683 [00:02<01:44,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:11,817 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:11,817 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 20/683 [00:02<01:43,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:11,971 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:11,971 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 21/683 [00:03<01:44,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:12,131 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:12,132 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 22/683 [00:03<01:44,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:12,287 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:12,288 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  3% 23/683 [00:03<01:43,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:12,444 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:12,445 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 24/683 [00:03<01:44,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:12,605 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:12,605 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 25/683 [00:03<01:44,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:12,763 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:12,764 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 26/683 [00:03<01:43,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:12,920 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:12,920 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 27/683 [00:04<01:43,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:13,078 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:13,078 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 28/683 [00:04<01:43,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:13,236 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:13,237 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 29/683 [00:04<01:44,  6.24it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:13,402 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:13,402 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  4% 30/683 [00:04<01:45,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:13,567 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:13,568 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 31/683 [00:04<01:46,  6.13it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:13,733 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:13,733 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 32/683 [00:04<01:45,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:13,892 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:13,892 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 33/683 [00:05<01:44,  6.19it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:14,052 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:14,053 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 34/683 [00:05<01:45,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:14,215 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:14,215 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 35/683 [00:05<01:43,  6.28it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:14,368 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:14,369 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 36/683 [00:05<01:43,  6.28it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:14,528 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:14,528 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  5% 37/683 [00:05<01:42,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:14,683 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:14,684 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 38/683 [00:05<01:41,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:14,841 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:14,841 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 39/683 [00:06<01:41,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:14,999 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:15,000 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 40/683 [00:06<01:42,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:15,159 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:15,160 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 41/683 [00:06<01:40,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:15,314 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:15,314 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 42/683 [00:06<01:41,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:15,472 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:15,473 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 43/683 [00:06<01:41,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:15,630 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:15,630 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  6% 44/683 [00:06<01:41,  6.28it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:15,794 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:15,794 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 45/683 [00:06<01:41,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:15,950 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:15,950 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 46/683 [00:07<01:42,  6.24it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:16,115 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:16,115 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 47/683 [00:07<01:42,  6.20it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:16,278 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:16,278 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 48/683 [00:07<01:42,  6.17it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:16,442 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:16,442 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 49/683 [00:07<01:41,  6.25it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:16,598 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:16,598 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 50/683 [00:07<01:41,  6.26it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:16,756 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:16,756 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  7% 51/683 [00:07<01:39,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:16,910 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:16,911 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 52/683 [00:08<01:39,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:17,066 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:17,066 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 53/683 [00:08<01:39,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:17,224 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:17,224 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 54/683 [00:08<01:38,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:17,381 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:17,381 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 55/683 [00:08<01:38,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:17,539 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:17,539 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 56/683 [00:08<01:38,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:17,698 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:17,698 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 57/683 [00:08<01:39,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:17,857 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:17,857 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  8% 58/683 [00:09<01:39,  6.27it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:18,022 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:18,022 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 59/683 [00:09<01:39,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:18,177 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:18,177 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 60/683 [00:09<01:38,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:18,335 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:18,336 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 61/683 [00:09<01:38,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:18,490 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:18,490 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 62/683 [00:09<01:38,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:18,651 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:18,651 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 63/683 [00:09<01:37,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:18,808 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:18,809 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "  9% 64/683 [00:09<01:37,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:18,965 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:18,965 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 65/683 [00:10<01:37,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:19,123 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:19,123 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 66/683 [00:10<01:37,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:19,279 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:19,279 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 67/683 [00:10<01:37,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:19,438 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:19,439 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 68/683 [00:10<01:36,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:19,593 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:19,593 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 69/683 [00:10<01:36,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:19,750 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:19,751 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 70/683 [00:10<01:35,  6.41it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:19,903 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:19,904 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 10% 71/683 [00:11<01:35,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:20,061 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:20,061 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 72/683 [00:11<01:35,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:20,217 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:20,217 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 73/683 [00:11<01:35,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:20,376 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:20,376 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 74/683 [00:11<01:35,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:20,532 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:20,532 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 75/683 [00:11<01:36,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:20,694 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:20,694 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 76/683 [00:11<01:36,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:20,852 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:20,852 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 77/683 [00:12<01:36,  6.27it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:21,015 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:21,015 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 11% 78/683 [00:12<01:36,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:21,172 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:21,172 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 79/683 [00:12<01:35,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:21,327 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:21,328 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 80/683 [00:12<01:35,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:21,484 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:21,485 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 81/683 [00:12<01:34,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:21,642 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:21,643 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 82/683 [00:12<01:35,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:21,801 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:21,802 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 83/683 [00:12<01:35,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:21,962 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:21,963 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 84/683 [00:13<01:34,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:22,120 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:22,120 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 12% 85/683 [00:13<01:34,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:22,278 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:22,278 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 86/683 [00:13<01:33,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:22,432 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:22,432 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 87/683 [00:13<01:33,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:22,590 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:22,591 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 88/683 [00:13<01:33,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:22,748 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:22,748 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 89/683 [00:13<01:32,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:22,903 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:22,903 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 90/683 [00:14<01:32,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:23,059 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:23,060 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 91/683 [00:14<01:32,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:23,216 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:23,216 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 13% 92/683 [00:14<01:32,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:23,373 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:23,374 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 93/683 [00:14<01:32,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:23,532 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:23,533 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 94/683 [00:14<01:32,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:23,689 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:23,689 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 95/683 [00:14<01:32,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:23,847 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:23,848 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 96/683 [00:15<01:32,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:24,006 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:24,006 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 97/683 [00:15<01:32,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:24,162 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:24,163 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 98/683 [00:15<01:32,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:24,320 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:24,320 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 14% 99/683 [00:15<01:31,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:24,476 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:24,477 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 100/683 [00:15<01:31,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:24,633 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:24,634 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 101/683 [00:15<01:31,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:24,790 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:24,790 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 102/683 [00:15<01:32,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:24,953 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:24,953 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 103/683 [00:16<01:31,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:25,109 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:25,110 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 104/683 [00:16<01:31,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:25,266 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:25,266 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 15% 105/683 [00:16<01:30,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:25,422 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:25,422 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 106/683 [00:16<01:31,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:25,582 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:25,582 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 107/683 [00:16<01:30,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:25,737 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:25,737 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 108/683 [00:16<01:30,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:25,897 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:25,897 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 109/683 [00:17<01:30,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:26,054 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:26,054 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 110/683 [00:17<01:30,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:26,212 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:26,212 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 111/683 [00:17<01:29,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:26,366 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:26,366 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 16% 112/683 [00:17<01:29,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:26,522 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:26,523 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 113/683 [00:17<01:29,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:26,678 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:26,678 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 114/683 [00:17<01:28,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:26,835 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:26,835 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 115/683 [00:18<01:28,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:26,990 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:26,990 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 116/683 [00:18<01:28,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:27,148 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:27,148 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 117/683 [00:18<01:28,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:27,303 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:27,303 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 118/683 [00:18<01:28,  6.41it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:27,459 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:27,460 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 17% 119/683 [00:18<01:28,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:27,616 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:27,616 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 120/683 [00:18<01:28,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:27,778 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:27,779 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 121/683 [00:18<01:28,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:27,933 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:27,933 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 122/683 [00:19<01:28,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:28,090 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:28,091 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 123/683 [00:19<01:27,  6.41it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:28,244 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:28,244 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 124/683 [00:19<01:27,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:28,401 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:28,401 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 125/683 [00:19<01:27,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:28,557 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:28,557 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 18% 126/683 [00:19<01:27,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:28,715 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:28,715 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 127/683 [00:19<01:26,  6.41it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:28,869 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:28,870 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 128/683 [00:20<01:27,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:29,030 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:29,030 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 129/683 [00:20<01:27,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:29,188 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:29,189 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 130/683 [00:20<01:27,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:29,347 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:29,347 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 131/683 [00:20<01:26,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:29,503 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:29,503 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 132/683 [00:20<01:26,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:29,659 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:29,659 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 19% 133/683 [00:20<01:26,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:29,817 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:29,817 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 134/683 [00:21<01:26,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:29,977 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:29,977 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 135/683 [00:21<01:27,  6.27it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:30,138 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:30,139 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 136/683 [00:21<01:28,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:30,305 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:30,306 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 137/683 [00:21<01:28,  6.16it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:30,470 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:30,470 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 138/683 [00:21<01:29,  6.12it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:30,635 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:30,636 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 139/683 [00:21<01:29,  6.09it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:30,802 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:30,802 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 20% 140/683 [00:21<01:28,  6.16it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:30,959 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:30,959 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 141/683 [00:22<01:27,  6.19it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:31,120 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:31,120 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 142/683 [00:22<01:26,  6.26it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:31,274 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:31,275 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 143/683 [00:22<01:26,  6.28it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:31,433 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:31,433 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 144/683 [00:22<01:25,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:31,589 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:31,589 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 145/683 [00:22<01:24,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:31,744 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:31,744 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 21% 146/683 [00:22<01:24,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:31,904 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:31,904 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 147/683 [00:23<01:24,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:32,062 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:32,063 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 148/683 [00:23<01:24,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:32,220 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:32,220 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 149/683 [00:23<01:24,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:32,378 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:32,379 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 150/683 [00:23<01:23,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:32,534 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:32,535 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 151/683 [00:23<01:23,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:32,693 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:32,693 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 152/683 [00:23<01:23,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:32,849 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:32,849 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 22% 153/683 [00:24<01:23,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:33,007 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:33,008 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 154/683 [00:24<01:24,  6.28it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:33,170 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:33,170 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 155/683 [00:24<01:25,  6.20it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:33,336 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:33,337 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 156/683 [00:24<01:25,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:33,499 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:33,499 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 157/683 [00:24<01:24,  6.23it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:33,656 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:33,656 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 158/683 [00:24<01:23,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:33,810 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:33,810 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 159/683 [00:24<01:22,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:33,967 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:33,967 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 23% 160/683 [00:25<01:22,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:34,125 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:34,126 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 161/683 [00:25<01:22,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:34,283 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:34,283 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 162/683 [00:25<01:21,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:34,438 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:34,438 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 163/683 [00:25<01:21,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:34,597 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:34,597 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 164/683 [00:25<01:21,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:34,752 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:34,752 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 165/683 [00:25<01:21,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:34,909 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:34,909 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 166/683 [00:26<01:20,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:35,064 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:35,064 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 24% 167/683 [00:26<01:21,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:35,229 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:35,230 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 168/683 [00:26<01:22,  6.27it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:35,388 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:35,389 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 169/683 [00:26<01:22,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:35,554 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:35,555 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 170/683 [00:26<01:22,  6.22it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:35,713 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:35,714 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 171/683 [00:26<01:22,  6.24it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:35,874 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:35,874 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 172/683 [00:27<01:21,  6.27it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:36,030 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:36,031 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 173/683 [00:27<01:20,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:36,188 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:36,188 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 25% 174/683 [00:27<01:20,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:36,344 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:36,344 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 175/683 [00:27<01:19,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:36,500 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:36,501 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 176/683 [00:27<01:19,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:36,657 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:36,657 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 177/683 [00:27<01:19,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:36,816 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:36,816 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 178/683 [00:28<01:19,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:36,974 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:36,974 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 179/683 [00:28<01:20,  6.27it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:37,139 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:37,140 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 26% 180/683 [00:28<01:20,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:37,302 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:37,302 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 181/683 [00:28<01:21,  6.15it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:37,469 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:37,469 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 182/683 [00:28<01:21,  6.14it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:37,632 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:37,632 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 183/683 [00:28<01:21,  6.14it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:37,795 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:37,796 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 184/683 [00:28<01:20,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:37,953 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:37,954 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 185/683 [00:29<01:20,  6.16it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:38,117 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:38,118 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 186/683 [00:29<01:19,  6.24it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:38,273 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:38,273 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 27% 187/683 [00:29<01:19,  6.27it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:38,431 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:38,431 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 188/683 [00:29<01:18,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:38,586 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:38,586 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 189/683 [00:29<01:18,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:38,743 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:38,744 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 190/683 [00:29<01:18,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:38,903 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:38,904 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 191/683 [00:30<01:18,  6.28it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:39,064 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:39,064 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 192/683 [00:30<01:17,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:39,218 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:39,219 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 193/683 [00:30<01:17,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:39,376 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:39,377 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 28% 194/683 [00:30<01:17,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:39,535 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:39,535 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 195/683 [00:30<01:17,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:39,693 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:39,693 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 196/683 [00:30<01:17,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:39,851 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:39,851 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 197/683 [00:31<01:16,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:40,011 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:40,011 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 198/683 [00:31<01:17,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:40,170 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:40,170 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 199/683 [00:31<01:16,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:40,327 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:40,327 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 200/683 [00:31<01:16,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:40,485 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:40,485 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 29% 201/683 [00:31<01:16,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:40,643 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:40,643 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 202/683 [00:31<01:15,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:40,800 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:40,800 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 203/683 [00:31<01:15,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:40,959 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:40,960 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 204/683 [00:32<01:15,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:41,117 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:41,118 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 205/683 [00:32<01:15,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:41,277 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:41,278 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 206/683 [00:32<01:15,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:41,431 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:41,431 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 207/683 [00:32<01:14,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:41,589 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:41,589 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 30% 208/683 [00:32<01:14,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:41,744 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:41,744 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 209/683 [00:32<01:14,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:41,900 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:41,901 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 210/683 [00:33<01:14,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:42,060 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:42,060 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 211/683 [00:33<01:14,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:42,221 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:42,221 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 212/683 [00:33<01:15,  6.27it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:42,382 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:42,383 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 213/683 [00:33<01:15,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:42,548 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:42,548 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 214/683 [00:33<01:16,  6.16it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:42,712 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:42,713 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 31% 215/683 [00:33<01:16,  6.12it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:42,878 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:42,878 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 216/683 [00:34<01:15,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:43,036 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:43,036 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 217/683 [00:34<01:15,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:43,195 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:43,195 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 218/683 [00:34<01:14,  6.27it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:43,351 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:43,352 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 219/683 [00:34<01:13,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:43,509 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:43,509 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 220/683 [00:34<01:13,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:43,664 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:43,665 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 32% 221/683 [00:34<01:12,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:43,823 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:43,823 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 222/683 [00:35<01:13,  6.26it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:43,986 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:43,986 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 223/683 [00:35<01:13,  6.26it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:44,147 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:44,147 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 224/683 [00:35<01:13,  6.24it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:44,307 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:44,308 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 225/683 [00:35<01:13,  6.20it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:44,472 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:44,473 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 226/683 [00:35<01:13,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:44,634 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:44,634 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 227/683 [00:35<01:14,  6.13it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:44,800 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:44,800 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 33% 228/683 [00:35<01:13,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:44,956 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:44,957 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 229/683 [00:36<01:13,  6.20it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:45,120 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:45,120 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 230/683 [00:36<01:13,  6.20it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:45,280 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:45,280 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 231/683 [00:36<01:12,  6.24it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:45,438 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:45,438 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 232/683 [00:36<01:12,  6.26it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:45,595 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:45,596 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 233/683 [00:36<01:11,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:45,754 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:45,754 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 234/683 [00:36<01:11,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:45,911 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:45,911 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 34% 235/683 [00:37<01:10,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:46,070 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:46,070 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 236/683 [00:37<01:11,  6.24it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:46,233 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:46,233 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 237/683 [00:37<01:11,  6.20it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:46,397 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:46,397 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 238/683 [00:37<01:11,  6.23it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:46,556 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:46,556 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 239/683 [00:37<01:11,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:46,720 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:46,721 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 240/683 [00:37<01:11,  6.19it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:46,881 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:46,882 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 241/683 [00:38<01:11,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:47,045 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:47,045 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 35% 242/683 [00:38<01:10,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:47,203 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:47,203 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 243/683 [00:38<01:10,  6.26it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:47,360 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:47,361 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 244/683 [00:38<01:09,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:47,516 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:47,516 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 245/683 [00:38<01:09,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:47,674 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:47,674 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 246/683 [00:38<01:09,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:47,832 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:47,832 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 247/683 [00:39<01:09,  6.28it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:47,994 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:47,994 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 248/683 [00:39<01:09,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:48,151 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:48,151 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 36% 249/683 [00:39<01:08,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:48,310 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:48,310 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 250/683 [00:39<01:08,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:48,463 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:48,463 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 251/683 [00:39<01:07,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:48,619 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:48,619 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 252/683 [00:39<01:07,  6.42it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:48,773 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:48,773 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 253/683 [00:39<01:07,  6.41it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:48,929 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:48,929 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 254/683 [00:40<01:06,  6.43it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:49,084 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:49,085 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 255/683 [00:40<01:06,  6.42it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:49,240 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:49,241 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 37% 256/683 [00:40<01:06,  6.42it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:49,395 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:49,396 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 257/683 [00:40<01:06,  6.44it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:49,551 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:49,551 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 258/683 [00:40<01:06,  6.41it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:49,708 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:49,708 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 259/683 [00:40<01:06,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:49,866 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:49,867 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 260/683 [00:41<01:06,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:50,024 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:50,024 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 261/683 [00:41<01:06,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:50,183 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:50,183 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 38% 262/683 [00:41<01:06,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:50,342 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:50,342 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 263/683 [00:41<01:06,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:50,499 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:50,499 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 264/683 [00:41<01:06,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:50,655 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:50,655 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 265/683 [00:41<01:05,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:50,812 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:50,813 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 266/683 [00:41<01:05,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:50,966 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:50,966 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 267/683 [00:42<01:05,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:51,125 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:51,125 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 268/683 [00:42<01:05,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:51,282 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:51,282 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 39% 269/683 [00:42<01:05,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:51,440 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:51,440 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 270/683 [00:42<01:04,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:51,595 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:51,595 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 271/683 [00:42<01:04,  6.41it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:51,750 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:51,750 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 272/683 [00:42<01:04,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:51,907 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:51,907 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 273/683 [00:43<01:04,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:52,067 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:52,067 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 274/683 [00:43<01:05,  6.24it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:52,234 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:52,234 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 275/683 [00:43<01:05,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:52,397 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:52,397 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 40% 276/683 [00:43<01:05,  6.19it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:52,559 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:52,559 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 277/683 [00:43<01:05,  6.17it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:52,722 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:52,722 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 278/683 [00:43<01:05,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:52,883 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:52,883 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 279/683 [00:44<01:05,  6.14it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:53,049 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:53,049 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 280/683 [00:44<01:04,  6.22it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:53,205 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:53,205 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 281/683 [00:44<01:04,  6.28it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:53,361 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:53,361 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 282/683 [00:44<01:03,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:53,517 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:53,517 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 41% 283/683 [00:44<01:03,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:53,675 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:53,675 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 284/683 [00:44<01:02,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:53,830 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:53,830 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 285/683 [00:45<01:02,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:53,988 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:53,988 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 286/683 [00:45<01:02,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:54,144 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:54,145 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 287/683 [00:45<01:02,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:54,302 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:54,302 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 288/683 [00:45<01:02,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:54,458 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:54,459 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 289/683 [00:45<01:02,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:54,616 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:54,616 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 42% 290/683 [00:45<01:01,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:54,773 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:54,773 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 291/683 [00:45<01:01,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:54,930 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:54,930 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 292/683 [00:46<01:01,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:55,091 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:55,091 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 293/683 [00:46<01:02,  6.23it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:55,256 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:55,256 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 294/683 [00:46<01:02,  6.20it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:55,420 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:55,421 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 295/683 [00:46<01:02,  6.17it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:55,584 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:55,584 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 296/683 [00:46<01:02,  6.16it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:55,746 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:55,747 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 43% 297/683 [00:46<01:02,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:55,904 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:55,904 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 298/683 [00:47<01:01,  6.25it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:56,062 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:56,062 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 299/683 [00:47<01:01,  6.28it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:56,219 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:56,219 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 300/683 [00:47<01:00,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:56,376 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:56,376 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 301/683 [00:47<01:00,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:56,532 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:56,532 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 302/683 [00:47<00:59,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:56,687 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:56,688 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 44% 303/683 [00:47<00:59,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:56,846 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:56,847 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 304/683 [00:48<00:59,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:57,005 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:57,005 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 305/683 [00:48<00:59,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:57,162 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:57,163 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 306/683 [00:48<00:59,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:57,322 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:57,322 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 307/683 [00:48<00:59,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:57,480 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:57,480 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 308/683 [00:48<00:59,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:57,638 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:57,638 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 309/683 [00:48<00:58,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:57,795 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:57,795 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 45% 310/683 [00:48<00:58,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:57,952 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:57,952 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 311/683 [00:49<00:58,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:58,111 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:58,111 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 312/683 [00:49<00:58,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:58,268 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:58,268 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 313/683 [00:49<00:58,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:58,424 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:58,424 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 314/683 [00:49<00:57,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:58,579 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:58,579 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 315/683 [00:49<00:57,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:58,735 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:58,736 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 316/683 [00:49<00:57,  6.41it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:58,889 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:58,890 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 46% 317/683 [00:50<00:57,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:59,053 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:59,053 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 318/683 [00:50<00:57,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:59,213 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:59,213 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 319/683 [00:50<00:58,  6.24it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:59,377 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:59,377 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 320/683 [00:50<00:58,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:59,541 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:59,541 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 321/683 [00:50<00:58,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:59,703 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:59,704 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 322/683 [00:50<00:58,  6.19it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:44:59,864 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:44:59,865 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 323/683 [00:51<00:57,  6.28it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:00,018 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:00,019 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 47% 324/683 [00:51<00:56,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:00,175 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:00,176 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 325/683 [00:51<00:56,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:00,330 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:00,330 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 326/683 [00:51<00:56,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:00,489 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:00,489 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 327/683 [00:51<00:55,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:00,644 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:00,644 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 328/683 [00:51<00:55,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:00,802 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:00,802 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 329/683 [00:51<00:55,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:00,958 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:00,958 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 330/683 [00:52<00:55,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:01,113 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:01,113 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 48% 331/683 [00:52<00:55,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:01,271 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:01,271 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 332/683 [00:52<00:55,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:01,429 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:01,429 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 333/683 [00:52<00:54,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:01,584 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:01,584 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 334/683 [00:52<00:54,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:01,740 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:01,740 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 335/683 [00:52<00:54,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:01,897 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:01,897 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 336/683 [00:53<00:54,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:02,054 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:02,054 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 337/683 [00:53<00:54,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:02,208 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:02,209 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 49% 338/683 [00:53<00:53,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:02,366 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:02,366 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 339/683 [00:53<00:53,  6.41it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:02,520 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:02,521 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 340/683 [00:53<00:53,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:02,677 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:02,678 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 341/683 [00:53<00:53,  6.44it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:02,831 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:02,831 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 342/683 [00:54<00:53,  6.41it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:02,988 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:02,988 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 343/683 [00:54<00:52,  6.43it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:03,143 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:03,143 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 50% 344/683 [00:54<00:52,  6.42it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:03,300 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:03,300 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 345/683 [00:54<00:52,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:03,458 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:03,458 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 346/683 [00:54<00:52,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:03,615 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:03,615 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 347/683 [00:54<00:52,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:03,770 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:03,770 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 348/683 [00:54<00:52,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:03,929 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:03,930 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 349/683 [00:55<00:53,  6.28it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:04,093 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:04,093 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 350/683 [00:55<00:53,  6.22it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:04,259 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:04,259 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 51% 351/683 [00:55<00:53,  6.19it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:04,420 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:04,421 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 352/683 [00:55<00:53,  6.15it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:04,585 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:04,586 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 353/683 [00:55<00:53,  6.20it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:04,744 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:04,744 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 354/683 [00:55<00:53,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:04,907 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:04,907 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 355/683 [00:56<00:52,  6.24it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:05,064 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:05,065 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 356/683 [00:56<00:52,  6.26it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:05,223 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:05,223 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 357/683 [00:56<00:51,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:05,379 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:05,379 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 52% 358/683 [00:56<00:51,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:05,536 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:05,536 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 359/683 [00:56<00:51,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:05,693 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:05,693 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 360/683 [00:56<00:51,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:05,853 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:05,854 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 361/683 [00:57<00:51,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:06,012 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:06,013 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 362/683 [00:57<00:50,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:06,171 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:06,171 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 363/683 [00:57<00:50,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:06,326 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:06,327 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 364/683 [00:57<00:50,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:06,484 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:06,484 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 53% 365/683 [00:57<00:50,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:06,642 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:06,642 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 366/683 [00:57<00:49,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:06,798 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:06,798 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 367/683 [00:57<00:49,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:06,957 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:06,958 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 368/683 [00:58<00:49,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:07,116 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:07,116 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 369/683 [00:58<00:49,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:07,274 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:07,274 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 370/683 [00:58<00:49,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:07,432 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:07,432 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 371/683 [00:58<00:49,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:07,589 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:07,590 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 54% 372/683 [00:58<00:49,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:07,746 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:07,746 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 373/683 [00:58<00:49,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:07,906 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:07,906 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 374/683 [00:59<00:48,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:08,060 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:08,060 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 375/683 [00:59<00:48,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:08,218 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:08,218 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 376/683 [00:59<00:48,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:08,375 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:08,375 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 377/683 [00:59<00:48,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:08,534 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:08,534 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 378/683 [00:59<00:47,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:08,689 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:08,689 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 55% 379/683 [00:59<00:47,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:08,848 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:08,849 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 380/683 [01:00<00:48,  6.27it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:09,011 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:09,012 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 381/683 [01:00<00:47,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:09,169 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:09,170 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 382/683 [01:00<00:47,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:09,325 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:09,326 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 383/683 [01:00<00:47,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:09,483 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:09,483 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 384/683 [01:00<00:46,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:09,637 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:09,638 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 56% 385/683 [01:00<00:46,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:09,794 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:09,794 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 386/683 [01:00<00:46,  6.40it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:09,951 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:09,951 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 387/683 [01:01<00:46,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:10,109 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:10,109 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 388/683 [01:01<00:46,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:10,265 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:10,265 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 389/683 [01:01<00:46,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:10,426 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:10,426 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 390/683 [01:01<00:46,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:10,582 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:10,583 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 391/683 [01:01<00:46,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:10,740 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:10,740 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 57% 392/683 [01:01<00:45,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:10,894 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:10,894 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 393/683 [01:02<00:45,  6.41it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:11,050 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:11,050 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 394/683 [01:02<00:45,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:11,208 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:11,208 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 395/683 [01:02<00:45,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:11,368 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:11,368 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 396/683 [01:02<00:45,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:11,526 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:11,526 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 397/683 [01:02<00:44,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:11,682 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:11,682 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 398/683 [01:02<00:44,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:11,839 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:11,839 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 58% 399/683 [01:03<00:44,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:11,997 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:11,997 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 400/683 [01:03<00:44,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:12,154 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:12,154 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 401/683 [01:03<00:44,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:12,311 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:12,311 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 402/683 [01:03<00:44,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:12,469 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:12,469 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 403/683 [01:03<00:43,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:12,624 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:12,625 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 404/683 [01:03<00:43,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:12,780 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:12,781 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 405/683 [01:03<00:43,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:12,936 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:12,936 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 59% 406/683 [01:04<00:43,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:13,094 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:13,094 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 407/683 [01:04<00:43,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:13,252 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:13,252 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 408/683 [01:04<00:43,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:13,412 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:13,412 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 409/683 [01:04<00:43,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:13,568 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:13,568 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 410/683 [01:04<00:42,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:13,726 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:13,727 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 411/683 [01:04<00:42,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:13,882 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:13,882 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 412/683 [01:05<00:42,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:14,040 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:14,041 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 60% 413/683 [01:05<00:42,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:14,196 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:14,196 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 414/683 [01:05<00:42,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:14,354 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:14,354 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 415/683 [01:05<00:42,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:14,510 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:14,511 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 416/683 [01:05<00:41,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:14,668 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:14,668 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 417/683 [01:05<00:41,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:14,824 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:14,824 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 418/683 [01:06<00:41,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:14,984 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:14,985 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 419/683 [01:06<00:42,  6.26it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:15,148 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:15,148 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 61% 420/683 [01:06<00:42,  6.22it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:15,312 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:15,312 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 421/683 [01:06<00:42,  6.16it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:15,476 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:15,477 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 422/683 [01:06<00:42,  6.13it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:15,642 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:15,643 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 423/683 [01:06<00:42,  6.14it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:15,804 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:15,804 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 424/683 [01:06<00:41,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:15,963 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:15,963 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 425/683 [01:07<00:41,  6.25it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:16,120 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:16,120 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 62% 426/683 [01:07<00:40,  6.28it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:16,277 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:16,277 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 427/683 [01:07<00:40,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:16,433 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:16,433 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 428/683 [01:07<00:40,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:16,590 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:16,590 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 429/683 [01:07<00:40,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:16,746 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:16,746 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 430/683 [01:07<00:40,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:16,909 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:16,909 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 431/683 [01:08<00:40,  6.27it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:17,070 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:17,070 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 432/683 [01:08<00:40,  6.20it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:17,235 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:17,236 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 63% 433/683 [01:08<00:40,  6.17it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:17,398 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:17,399 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 434/683 [01:08<00:40,  6.12it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:17,566 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:17,566 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 435/683 [01:08<00:40,  6.11it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:17,729 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:17,729 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 436/683 [01:08<00:40,  6.13it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:17,891 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:17,891 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 437/683 [01:09<00:39,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:18,049 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:18,050 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 438/683 [01:09<00:39,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:18,209 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:18,210 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 439/683 [01:09<00:39,  6.25it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:18,366 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:18,366 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 64% 440/683 [01:09<00:38,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:18,523 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:18,523 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 441/683 [01:09<00:38,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:18,680 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:18,680 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 442/683 [01:09<00:38,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:18,838 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:18,838 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 443/683 [01:10<00:37,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:18,995 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:18,995 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 444/683 [01:10<00:37,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:19,154 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:19,154 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 445/683 [01:10<00:37,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:19,312 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:19,312 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 446/683 [01:10<00:37,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:19,471 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:19,472 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 65% 447/683 [01:10<00:37,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:19,627 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:19,627 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 448/683 [01:10<00:37,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:19,784 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:19,784 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 449/683 [01:10<00:36,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:19,941 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:19,941 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 450/683 [01:11<00:37,  6.28it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:20,106 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:20,106 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 451/683 [01:11<00:37,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:20,270 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:20,271 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 452/683 [01:11<00:37,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:20,435 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:20,436 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 453/683 [01:11<00:37,  6.16it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:20,597 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:20,597 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 66% 454/683 [01:11<00:37,  6.15it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:20,762 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:20,762 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 455/683 [01:11<00:36,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:20,920 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:20,921 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 456/683 [01:12<00:36,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:21,080 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:21,080 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 457/683 [01:12<00:36,  6.27it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:21,235 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:21,236 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 458/683 [01:12<00:35,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:21,392 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:21,392 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 459/683 [01:12<00:35,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:21,552 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:21,552 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 460/683 [01:12<00:35,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:21,711 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:21,711 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 67% 461/683 [01:12<00:35,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:21,869 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:21,870 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 462/683 [01:13<00:35,  6.24it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:22,034 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:22,034 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 463/683 [01:13<00:35,  6.16it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:22,200 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:22,200 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 464/683 [01:13<00:35,  6.15it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:22,364 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:22,365 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 465/683 [01:13<00:35,  6.16it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:22,524 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:22,525 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 466/683 [01:13<00:35,  6.13it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:22,689 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:22,690 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 68% 467/683 [01:13<00:34,  6.19it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:22,848 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:22,848 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 468/683 [01:14<00:34,  6.20it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:23,009 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:23,009 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 469/683 [01:14<00:34,  6.17it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:23,172 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:23,173 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 470/683 [01:14<00:34,  6.16it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:23,336 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:23,336 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 471/683 [01:14<00:34,  6.13it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:23,500 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:23,500 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 472/683 [01:14<00:34,  6.09it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:23,668 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:23,668 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 473/683 [01:14<00:34,  6.13it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:23,827 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:23,827 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 69% 474/683 [01:15<00:33,  6.20it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:23,985 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:23,985 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 475/683 [01:15<00:33,  6.25it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:24,142 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:24,142 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 476/683 [01:15<00:33,  6.25it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:24,301 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:24,302 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 477/683 [01:15<00:32,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:24,457 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:24,457 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 478/683 [01:15<00:32,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:24,614 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:24,614 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 479/683 [01:15<00:32,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:24,771 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:24,771 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 480/683 [01:15<00:31,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:24,927 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:24,927 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 70% 481/683 [01:16<00:31,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:25,082 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:25,083 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 482/683 [01:16<00:31,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:25,240 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:25,240 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 483/683 [01:16<00:31,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:25,397 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:25,398 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 484/683 [01:16<00:31,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:25,557 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:25,557 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 485/683 [01:16<00:31,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:25,713 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:25,714 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 486/683 [01:16<00:30,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:25,870 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:25,871 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 487/683 [01:17<00:30,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:26,028 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:26,028 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 71% 488/683 [01:17<00:30,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:26,186 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:26,186 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 489/683 [01:17<00:30,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:26,345 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:26,345 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 490/683 [01:17<00:30,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:26,502 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:26,503 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 491/683 [01:17<00:30,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:26,662 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:26,662 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 492/683 [01:17<00:30,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:26,817 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:26,818 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 493/683 [01:18<00:29,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:26,975 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:26,975 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 494/683 [01:18<00:29,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:27,134 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:27,134 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 72% 495/683 [01:18<00:29,  6.28it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:27,297 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:27,297 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 496/683 [01:18<00:29,  6.25it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:27,457 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:27,457 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 497/683 [01:18<00:29,  6.24it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:27,620 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:27,620 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 498/683 [01:18<00:29,  6.26it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:27,777 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:27,777 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 499/683 [01:18<00:29,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:27,935 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:27,935 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 500/683 [01:19<00:29,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:28,093 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:28,093 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 501/683 [01:19<00:28,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:28,249 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:28,250 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 73% 502/683 [01:19<00:28,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:28,405 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:28,405 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 503/683 [01:19<00:28,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:28,561 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:28,561 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 504/683 [01:19<00:28,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:28,718 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:28,719 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 505/683 [01:19<00:28,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:28,879 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:28,879 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 506/683 [01:20<00:27,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:29,038 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:29,038 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 507/683 [01:20<00:27,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:29,198 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:29,199 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 74% 508/683 [01:20<00:27,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:29,354 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:29,354 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 509/683 [01:20<00:27,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:29,513 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:29,513 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 510/683 [01:20<00:27,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:29,670 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:29,670 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 511/683 [01:20<00:27,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:29,829 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:29,829 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 512/683 [01:21<00:26,  6.39it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:29,981 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:29,981 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 513/683 [01:21<00:26,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:30,141 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:30,142 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 514/683 [01:21<00:26,  6.27it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:30,305 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:30,305 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 75% 515/683 [01:21<00:26,  6.25it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:30,468 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:30,468 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 516/683 [01:21<00:26,  6.19it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:30,631 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:30,631 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 517/683 [01:21<00:26,  6.15it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:30,797 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:30,797 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 518/683 [01:21<00:26,  6.15it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:30,958 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:30,959 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 519/683 [01:22<00:26,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:31,117 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:31,117 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 520/683 [01:22<00:26,  6.26it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:31,273 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:31,273 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 521/683 [01:22<00:25,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:31,430 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:31,430 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 76% 522/683 [01:22<00:25,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:31,583 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:31,584 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 523/683 [01:22<00:25,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:31,739 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:31,740 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 524/683 [01:22<00:25,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:31,897 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:31,898 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 525/683 [01:23<00:24,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:32,055 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:32,056 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 526/683 [01:23<00:24,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:32,212 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:32,212 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 527/683 [01:23<00:24,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:32,368 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:32,368 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 528/683 [01:23<00:24,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:32,530 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:32,530 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 77% 529/683 [01:23<00:24,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:32,687 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:32,687 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 530/683 [01:23<00:24,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:32,846 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:32,846 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 531/683 [01:24<00:24,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:33,004 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:33,004 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 532/683 [01:24<00:23,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:33,162 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:33,163 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 533/683 [01:24<00:23,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:33,321 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:33,321 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 534/683 [01:24<00:23,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:33,475 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:33,475 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 535/683 [01:24<00:23,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:33,634 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:33,634 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 78% 536/683 [01:24<00:23,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:33,791 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:33,791 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 537/683 [01:24<00:23,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:33,952 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:33,952 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 538/683 [01:25<00:23,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:34,112 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:34,113 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 539/683 [01:25<00:23,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:34,279 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:34,279 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 540/683 [01:25<00:23,  6.16it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:34,443 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:34,443 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 541/683 [01:25<00:23,  6.13it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:34,609 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:34,609 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 79% 542/683 [01:25<00:23,  6.12it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:34,772 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:34,772 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 543/683 [01:25<00:22,  6.14it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:34,935 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:34,935 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 544/683 [01:26<00:22,  6.22it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:35,089 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:35,089 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 545/683 [01:26<00:22,  6.26it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:35,247 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:35,247 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 546/683 [01:26<00:21,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:35,402 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:35,402 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 547/683 [01:26<00:21,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:35,559 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:35,559 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 548/683 [01:26<00:21,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:35,715 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:35,715 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 80% 549/683 [01:26<00:21,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:35,874 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:35,874 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 550/683 [01:27<00:21,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:36,035 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:36,036 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 551/683 [01:27<00:20,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:36,192 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:36,192 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 552/683 [01:27<00:20,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:36,347 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:36,348 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 553/683 [01:27<00:20,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:36,505 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:36,505 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 554/683 [01:27<00:20,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:36,664 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:36,664 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 555/683 [01:27<00:20,  6.26it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:36,828 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:36,829 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 81% 556/683 [01:28<00:20,  6.24it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:36,989 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:36,989 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 557/683 [01:28<00:20,  6.17it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:37,157 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:37,157 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 558/683 [01:28<00:20,  6.14it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:37,320 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:37,321 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 559/683 [01:28<00:20,  6.11it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:37,486 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:37,486 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 560/683 [01:28<00:20,  6.12it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:37,650 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:37,650 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 561/683 [01:28<00:19,  6.13it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:37,810 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:37,811 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 562/683 [01:28<00:19,  6.17it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:37,970 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:37,971 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 82% 563/683 [01:29<00:19,  6.20it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:38,130 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:38,130 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 564/683 [01:29<00:19,  6.19it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:38,293 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:38,294 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 565/683 [01:29<00:19,  6.15it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:38,457 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:38,458 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 566/683 [01:29<00:19,  6.13it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:38,622 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:38,623 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 567/683 [01:29<00:18,  6.15it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:38,784 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:38,785 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 568/683 [01:29<00:18,  6.13it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:38,948 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:38,948 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 569/683 [01:30<00:18,  6.15it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:39,109 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:39,109 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 83% 570/683 [01:30<00:18,  6.19it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:39,268 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:39,269 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 571/683 [01:30<00:17,  6.28it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:39,422 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:39,422 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 572/683 [01:30<00:17,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:39,579 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:39,579 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 573/683 [01:30<00:17,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:39,736 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:39,736 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 574/683 [01:30<00:17,  6.28it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:39,898 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:39,899 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 575/683 [01:31<00:17,  6.22it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:40,062 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:40,062 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 576/683 [01:31<00:17,  6.19it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:40,226 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:40,227 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 84% 577/683 [01:31<00:17,  6.15it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:40,390 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:40,391 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 578/683 [01:31<00:17,  6.14it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:40,555 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:40,555 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 579/683 [01:31<00:16,  6.16it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:40,715 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:40,715 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 580/683 [01:31<00:16,  6.17it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:40,877 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:40,878 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 581/683 [01:32<00:16,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:41,035 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:41,035 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 582/683 [01:32<00:16,  6.16it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:41,200 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:41,201 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 85% 583/683 [01:32<00:16,  6.18it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:41,361 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:41,362 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 584/683 [01:32<00:16,  6.15it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:41,527 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:41,527 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 585/683 [01:32<00:15,  6.15it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:41,688 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:41,689 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 586/683 [01:32<00:15,  6.14it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:41,853 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:41,854 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 587/683 [01:33<00:15,  6.15it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:42,013 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:42,014 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 588/683 [01:33<00:15,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:42,173 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:42,174 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 589/683 [01:33<00:15,  6.25it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:42,329 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:42,329 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 86% 590/683 [01:33<00:14,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:42,486 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:42,486 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 591/683 [01:33<00:14,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:42,639 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:42,639 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 592/683 [01:33<00:14,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:42,800 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:42,800 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 593/683 [01:33<00:14,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:42,958 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:42,958 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 594/683 [01:34<00:14,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:43,116 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:43,116 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 595/683 [01:34<00:13,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:43,275 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:43,275 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 596/683 [01:34<00:13,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:43,431 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:43,432 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 87% 597/683 [01:34<00:13,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:43,588 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:43,589 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 598/683 [01:34<00:13,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:43,745 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:43,746 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 599/683 [01:34<00:13,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:43,904 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:43,904 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 600/683 [01:35<00:13,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:44,063 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:44,063 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 601/683 [01:35<00:13,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:44,224 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:44,224 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 602/683 [01:35<00:13,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:44,389 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:44,389 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 603/683 [01:35<00:12,  6.23it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:44,549 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:44,549 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 88% 604/683 [01:35<00:12,  6.17it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:44,714 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:44,715 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 605/683 [01:35<00:12,  6.22it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:44,872 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:44,872 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 606/683 [01:36<00:12,  6.27it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:45,029 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:45,030 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 607/683 [01:36<00:12,  6.29it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:45,186 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:45,187 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 608/683 [01:36<00:11,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:45,343 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:45,343 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 609/683 [01:36<00:11,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:45,502 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:45,502 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 610/683 [01:36<00:11,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:45,660 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:45,660 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 89% 611/683 [01:36<00:11,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:45,816 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:45,817 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 612/683 [01:37<00:11,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:45,978 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:45,978 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 613/683 [01:37<00:11,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:46,133 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:46,133 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 614/683 [01:37<00:10,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:46,294 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:46,294 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 615/683 [01:37<00:10,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:46,450 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:46,450 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 616/683 [01:37<00:10,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:46,609 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:46,610 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 617/683 [01:37<00:10,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:46,762 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:46,763 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 90% 618/683 [01:37<00:10,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:46,924 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:46,924 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 619/683 [01:38<00:10,  6.27it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:47,086 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:47,086 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 620/683 [01:38<00:09,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:47,242 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:47,243 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 621/683 [01:38<00:09,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:47,398 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:47,398 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 622/683 [01:38<00:09,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:47,556 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:47,557 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 623/683 [01:38<00:09,  6.28it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:47,718 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:47,718 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 91% 624/683 [01:38<00:09,  6.26it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:47,880 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:47,880 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 625/683 [01:39<00:09,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:48,043 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:48,044 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 626/683 [01:39<00:09,  6.22it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:48,204 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:48,204 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 627/683 [01:39<00:08,  6.26it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:48,360 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:48,360 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 628/683 [01:39<00:08,  6.27it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:48,520 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:48,520 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 629/683 [01:39<00:08,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:48,675 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:48,676 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 630/683 [01:39<00:08,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:48,835 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:48,835 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 92% 631/683 [01:40<00:08,  6.21it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:49,000 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:49,001 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 632/683 [01:40<00:08,  6.22it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:49,161 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:49,162 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 633/683 [01:40<00:08,  6.24it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:49,320 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:49,320 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 634/683 [01:40<00:07,  6.27it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:49,478 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:49,478 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 635/683 [01:40<00:07,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:49,633 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:49,633 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 636/683 [01:40<00:07,  6.35it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:49,789 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:49,790 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 637/683 [01:40<00:07,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:49,947 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:49,948 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 93% 638/683 [01:41<00:07,  6.23it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:50,114 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:50,114 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 639/683 [01:41<00:07,  6.25it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:50,272 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:50,272 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 640/683 [01:41<00:06,  6.20it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:50,436 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:50,436 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 641/683 [01:41<00:06,  6.22it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:50,595 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:50,596 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 642/683 [01:41<00:06,  6.22it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:50,758 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:50,758 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 643/683 [01:41<00:06,  6.19it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:50,920 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:50,921 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 644/683 [01:42<00:06,  6.19it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:51,083 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:51,084 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 94% 645/683 [01:42<00:06,  6.16it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:51,246 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:51,246 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 646/683 [01:42<00:06,  6.13it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:51,412 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:51,413 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 647/683 [01:42<00:05,  6.12it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:51,575 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:51,575 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 648/683 [01:42<00:05,  6.11it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:51,739 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:51,740 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 649/683 [01:42<00:05,  6.19it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:51,896 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:51,896 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 650/683 [01:43<00:05,  6.23it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:52,055 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:52,055 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 651/683 [01:43<00:05,  6.26it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:52,212 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:52,212 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 95% 652/683 [01:43<00:04,  6.31it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:52,368 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:52,368 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 653/683 [01:43<00:04,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:52,524 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:52,524 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 654/683 [01:43<00:04,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:52,681 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:52,681 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 655/683 [01:43<00:04,  6.32it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:52,840 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:52,840 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 656/683 [01:44<00:04,  6.23it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:53,007 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:53,008 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 657/683 [01:44<00:04,  6.17it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:53,172 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:53,172 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 658/683 [01:44<00:04,  6.11it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:53,340 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:53,340 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 96% 659/683 [01:44<00:03,  6.15it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:53,500 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:53,500 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 660/683 [01:44<00:03,  6.16it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:53,662 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:53,663 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 661/683 [01:44<00:03,  6.15it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:53,824 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:53,825 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 662/683 [01:45<00:03,  6.16it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:53,987 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:53,988 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 663/683 [01:45<00:03,  6.13it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:54,151 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:54,152 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 664/683 [01:45<00:03,  6.12it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:54,316 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:54,317 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 97% 665/683 [01:45<00:02,  6.13it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:54,477 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:54,478 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 666/683 [01:45<00:02,  6.10it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:54,644 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:54,645 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 667/683 [01:45<00:02,  6.19it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:54,799 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:54,800 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 668/683 [01:45<00:02,  6.26it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:54,956 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:54,956 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 669/683 [01:46<00:02,  6.26it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:55,115 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:55,115 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 670/683 [01:46<00:02,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:55,272 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:55,272 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 671/683 [01:46<00:01,  6.30it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:55,430 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:55,430 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 98% 672/683 [01:46<00:01,  6.33it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:55,586 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:55,587 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 673/683 [01:46<00:01,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:55,741 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:55,742 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 674/683 [01:46<00:01,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:55,898 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:55,898 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 675/683 [01:47<00:01,  6.36it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:56,056 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:56,057 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 676/683 [01:47<00:01,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:56,215 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:56,215 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 677/683 [01:47<00:00,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:56,369 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:56,370 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 678/683 [01:47<00:00,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:56,526 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:56,526 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            " 99% 679/683 [01:47<00:00,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:56,683 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:56,684 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "100% 680/683 [01:47<00:00,  6.37it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:56,840 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:56,840 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "100% 681/683 [01:48<00:00,  6.38it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:56,997 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:56,997 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "100% 682/683 [01:48<00:00,  6.34it/s][INFO|modeling_longformer.py:1853] 2021-10-03 21:45:57,156 >> Initializing global attention on CLS token...\n",
            "[INFO|modeling_longformer.py:1554] 2021-10-03 21:45:57,157 >> Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
            "100% 683/683 [01:48<00:00,  6.51it/s]10/03/2021 21:45:57 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow\n",
            "100% 683/683 [01:48<00:00,  6.30it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_accuracy           =     0.9162\n",
            "  eval_loss               =      0.213\n",
            "  eval_runtime            = 0:01:48.54\n",
            "  eval_samples            =       5463\n",
            "  eval_samples_per_second =     50.328\n",
            "  eval_steps_per_second   =      6.292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgfoUfN8af6I"
      },
      "source": [
        "!cp -r /tmp/qnli /tmp/qnli-longformer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0wvmuKYJamv"
      },
      "source": [
        "!rm -r /tmp/qnli-longformer/checkpoint*\n",
        "!rm -r /tmp/qnli-longformer/runs\n",
        "!rm /tmp/qnli-longformer/pytorch_model.bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z5qUir-LPMe",
        "outputId": "52c50d7e-032d-46c7-cc85-560098d3897e"
      },
      "source": [
        "!cp /tmp/qnli/pytorch_model.bin /tmp/qnli-longformer/pytorch_model.bin\n",
        "!rm /tmp/qnli-longformer.zip\n",
        "!zip -r /tmp/qnli-longformer.zip /tmp/qnli-longformer\n",
        "from google.colab import files; files.download('/tmp/qnli-longformer.zip')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: tmp/qnli-longformer/ (stored 0%)\n",
            "  adding: tmp/qnli-longformer/train_results.json (deflated 41%)\n",
            "  adding: tmp/qnli-longformer/README.md (deflated 49%)\n",
            "  adding: tmp/qnli-longformer/special_tokens_map.json (deflated 50%)\n",
            "  adding: tmp/qnli-longformer/config.json (deflated 57%)\n",
            "  adding: tmp/qnli-longformer/trainer_state.json (deflated 60%)\n",
            "  adding: tmp/qnli-longformer/eval_results.json (deflated 41%)\n",
            "  adding: tmp/qnli-longformer/pytorch_model.bin (deflated 9%)\n",
            "  adding: tmp/qnli-longformer/tokenizer.json (deflated 59%)\n",
            "  adding: tmp/qnli-longformer/merges.txt (deflated 53%)\n",
            "  adding: tmp/qnli-longformer/tokenizer_config.json (deflated 47%)\n",
            "  adding: tmp/qnli-longformer/vocab.json (deflated 59%)\n",
            "  adding: tmp/qnli-longformer/all_results.json (deflated 56%)\n",
            "  adding: tmp/qnli-longformer/training_args.bin (deflated 48%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKIS5xg5PACx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}